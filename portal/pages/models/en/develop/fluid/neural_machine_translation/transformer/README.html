{% verbatim %}
<p>The minimum PaddlePaddle version needed for the code sample in this directory is the lastest develop branch. If you are on a version of PaddlePaddle earlier than this, <a href="http://www.paddlepaddle.org/docs/develop/documentation/en/build_and_install/pip_install_en.html">please update your installation</a>.</p>
<hr/>
<h1>Attention is All You Need: A Paddle Fluid implementation</h1>
<p>This is a Paddle Fluid implementation of the Transformer model in <a href="">Attention is All You Need</a> (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, arxiv, 2017).</p>
<p>If you use the dataset/code in your research, please cite the paper:</p>
<div class="highlight"><pre><span></span>@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6000--6010},
  year={2017}
}
</pre></div>
<h3>TODO</h3>
<p>This project is still under active development.</p>
{% endverbatim %}