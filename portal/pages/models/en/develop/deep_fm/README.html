{% verbatim %}
<p>The minimum PaddlePaddle version needed for the code sample in this directory is v0.11.0. If you are on a version of PaddlePaddle earlier than v0.11.0, <a href="http://www.paddlepaddle.org/docs/develop/documentation/en/build_and_install/pip_install_en.html">please update your installation</a>.</p>
<hr/>
<h1>Deep Factorization Machine for Click-Through Rate prediction</h1>
<h2>Introduction</h2>
<p>This model implements the DeepFM proposed in the following paper:</p>
<div class="highlight"><pre><span></span>@inproceedings{guo2017deepfm,
  title={DeepFM: A Factorization-Machine based Neural Network for CTR Prediction},
  author={Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li and Xiuqiang He},
  booktitle={the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI)},
  pages={1725--1731},
  year={2017}
}
</pre></div>
<p>The DeepFm combines factorization machine and deep neural networks to model
both low order and high order feature interactions. For details of the
factorization machines, please refer to the paper <a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf">factorization
machines</a></p>
<h2>Dataset</h2>
<p>This example uses Criteo dataset which was used for the <a href="https://www.kaggle.com/c/criteo-display-ad-challenge/">Display Advertising
Challenge</a>
hosted by Kaggle.</p>
<p>Each row is the features for an ad display and the first column is a label
indicating whether this ad has been clicked or not. There are 39 features in
total. 13 features take integer values and the other 26 features are
categorical features. For the test dataset, the labels are omitted.</p>
<p>Download dataset:
</p><div class="highlight"><pre><span></span><span class="nb">cd</span> data <span class="o">&amp;&amp;</span> ./download.sh <span class="o">&amp;&amp;</span> <span class="nb">cd</span> ..
</pre></div>
<h2>Model</h2>
<p>The DeepFM model is composed of the factorization machine layer (FM) and deep
neural networks (DNN). All the input features are feeded to both FM and DNN.
The output from FM and DNN are combined to form the final output. The embedding
layer for sparse features in the DNN shares the parameters with the latent
vectors (factors) of the FM layer.</p>
<p>The factorization machine layer in PaddlePaddle computes the second order
interactions. The following code example combines the factorization machine
layer and fully connected layer to form the full version of factorization
machine:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fm_layer</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">factor_size</span><span class="p">):</span>
    <span class="n">first_order</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Linear</span><span class="p">())</span>
    <span class="n">second_order</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">factorization_machine</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">factor_size</span><span class="o">=</span><span class="n">factor_size</span><span class="p">)</span>
    <span class="n">fm</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">addto</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">first_order</span><span class="p">,</span> <span class="n">second_order</span><span class="p">],</span>
                            <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Linear</span><span class="p">(),</span>
                            <span class="n">bias_attr</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fm</span>
</pre></div>
<h2>Data preparation</h2>
<p>To preprocess the raw dataset, the integer features are clipped then min-max
normalized to [0, 1] and the categorical features are one-hot encoded. The raw
training dataset are splited such that 90% are used for training and the other
10% are used for validation during training.</p>
<div class="highlight"><pre><span></span>python preprocess.py --datadir ./data/raw --outdir ./data
</pre></div>
<h2>Train</h2>
<p>The command line options for training can be listed by <code>python train.py -h</code>.</p>
<p>To train the model:
</p><div class="highlight"><pre><span></span>python train.py <span class="se">\</span>
        --train_data_path data/train.txt <span class="se">\</span>
        --test_data_path data/valid.txt <span class="se">\</span>
        <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span> <span class="p">|</span> tee train.log
</pre></div>
<p>After training pass 9 batch 40000, the testing AUC is <code>0.807178</code> and the testing
cost is <code>0.445196</code>.</p>
<h2>Infer</h2>
<p>The command line options for infering can be listed by <code>python infer.py -h</code>.</p>
<p>To make inference for the test dataset:
</p><div class="highlight"><pre><span></span>python infer.py <span class="se">\</span>
        --model_gz_path models/model-pass-9-batch-10000.tar.gz <span class="se">\</span>
        --data_path data/test.txt <span class="se">\</span>
        --prediction_output_path ./predict.txt
</pre></div>
{% endverbatim %}