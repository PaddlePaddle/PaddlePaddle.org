{% verbatim %}
<p>The minimum PaddlePaddle version needed for the code sample in this directory is v0.10.0. If you are on a version of PaddlePaddle earlier than v0.10.0, <a href="http://www.paddlepaddle.org/docs/develop/documentation/en/build_and_install/pip_install_en.html">please update your installation</a>.</p>
<hr/>
<h1>Deep Structured Semantic Models (DSSM)</h1>
<p>Deep Structured Semantic Models (DSSM) is simple but powerful DNN based model for matching web search queries and the URL based documents. This example demonstrates how to use PaddlePaddle to implement a generic DSSM model for modeling the semantic similarity between two strings.</p>
<h2>Background Introduction</h2>
<p>DSSM [<a href="##References">1</a>]is a classic semantic model proposed by the Institute of Physics. It is used to study the semantic distance between two texts. The general implementation of DSSM is as follows.</p>
<ol>
<li>The CTR predictor measures the degree of association between a user search query and a candidate web page.</li>
<li>Text relevance, which measures the degree of semantic correlation between two strings.</li>
<li>Automatically recommend, measure the degree of association between User and the recommended Item.</li>
</ol>
<h2>Model Architecture</h2>
<p>In the original paper [<a href="#References">1</a>] the DSSM model uses the implicit semantic relation between the user search query and the document as metric. The model structure is as follows</p>
<p align="center">
<img src="./images/dssm.png"/><br/><br/>
Figure 1. DSSM In the original paper
</p>
<p>With the subsequent optimization of the DSSM model to simplify the structure [<a href="#References">3</a>]，the model becomes：</p>
<p align="center">
<img src="./images/dssm2.png" width="600"/><br/><br/>
Figure 2. DSSM generic structure
</p>
<p>The blank box in the figure can be replaced by any model, such as fully connected FC, convoluted CNN, RNN, etc. The structure is designed to measure the semantic distance between two elements (such as strings).</p>
<p>In practice，DSSM model serves as a basic building block, with different loss functions to achieve specific functions, such as</p>
<ul>
<li>In ranking system, the pairwise rank loss function.</li>
<li>In the CTR estimate, instead of the binary classification on the click, use cross-entropy loss for a classification model</li>
<li>In regression model,  the cosine similarity is used to calculate the similarity</li>
</ul>
<h2>Model Implementation</h2>
<p>At a high level, DSSM model is composed of three components: the left and right DNN, and loss function on top of them. In complex tasks, the structure of the left DNN and the light DNN can be different. In this example, we keep these two DNN structures the same. And we choose any of FC, CNN, and RNN for the DNN architecture.</p>
<p>In PaddlePaddle, the loss functions are supported for any of classification, regression, and ranking. Among them, the distance between the left and right DNN is calculated by the cosine similarity. In the classification task, the predicted distribution is calculated by softmax.</p>
<p>Here we demonstrate:</p>
<ul>
<li>How CNN, FC do text information extraction can refer to <a href="https://github.com/PaddlePaddle/models/blob/develop/text_classification/README.md#模型详解">text classification</a></li>
<li>The contents of the RNN / GRU can be found in  <a href="https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/README.md#gated-recurrent-unit-gru">Machine Translation</a></li>
<li>For Pairwise Rank learning, please refer to <a href="https://github.com/PaddlePaddle/models/blob/develop/ltr/README.md">learn to rank</a></li>
</ul>
<p>Figure 3 shows the general architecture for both regression and classification models.</p>
<p align="center">
<img src="./images/dssm3.jpg"/><br/><br/>
Figure 3. DSSM for REGRESSION or CLASSIFICATION
</p>
<p>The structure of the Pairwise Rank is more complex, as shown in Figure 4.</p>
<p align="center">
<img src="./images/dssm2.jpg"/><br/><br/>
图 4. DSSM for Pairwise Rank
</p>
<p>In below, we describe how to train DSSM model in PaddlePaddle. All the codes are included in  <code>./network_conf.py</code>.</p>
<h3>Create a word vector table for the text</h3>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">''</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Create word embedding. The `prefix` is added in front of the name of</span>
<span class="sd">    embedding"s learnable parameter.</span>
<span class="sd">    """</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Create embedding table [</span><span class="si">%s</span><span class="s2">] whose dimention is </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span>
                <span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'</span><span class="si">%s</span><span class="s1">_emb.w'</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">emb</span>
</pre></div>
<p>Since the input (embedding table) is a list of the IDs of the words corresponding to a sentence, the word vector table outputs the sequence of word vectors.</p>
<h3>CNN implementation</h3>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_cnn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">''</span><span class="p">):</span>

    <span class="sd">"""</span>
<span class="sd">    A multi-layer CNN.</span>
<span class="sd">    :param emb: The word embedding.</span>
<span class="sd">    :type emb: paddle.layer</span>
<span class="sd">    :param prefix: The prefix will be added to of layers' names.</span>
<span class="sd">    :type prefix: str</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="nf">create_conv</span><span class="p">(</span><span class="n">context_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">prefix</span><span class="p">):</span>
        <span class="n">key</span> <span class="o">=</span> <span class="s2">"</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">_</span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">context_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">conv</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">sequence_conv_pool</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
            <span class="n">context_len</span><span class="o">=</span><span class="n">context_len</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="c1"># set parameter attr for parameter sharing</span>
            <span class="n">context_proj_param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">key</span> <span class="o">+</span> <span class="s2">"contex_proj.w"</span><span class="p">),</span>
            <span class="n">fc_param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">key</span> <span class="o">+</span> <span class="s2">"_fc.w"</span><span class="p">),</span>
            <span class="n">fc_bias_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">key</span> <span class="o">+</span> <span class="s2">"_fc.b"</span><span class="p">),</span>
            <span class="n">pool_bias_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">key</span> <span class="o">+</span> <span class="s2">"_pool.b"</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">conv</span>

    <span class="n">conv_3</span> <span class="o">=</span> <span class="n">create_conv</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">"cnn"</span><span class="p">)</span>
    <span class="n">conv_4</span> <span class="o">=</span> <span class="n">create_conv</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">"cnn"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">conv_3</span><span class="p">,</span> <span class="n">conv_4</span><span class="p">])</span>
</pre></div>
<p>CNN accepts the word sequence of the embedding table, then process the data by convolution and pooling, and finally outputs a semantic vector.</p>
<h3>RNN implementation</h3>
<p>RNN is suitable for learning variable length of the information</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_rnn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">''</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    A GRU sentence vector learner.</span>
<span class="sd">    """</span>
    <span class="n">gru</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">simple_gru</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">mixed_param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'</span><span class="si">%s</span><span class="s1">_gru_mixed.w'</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">),</span>
        <span class="n">mixed_bias_param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"</span><span class="si">%s</span><span class="s2">_gru_mixed.b"</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">),</span>
        <span class="n">gru_param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'</span><span class="si">%s</span><span class="s1">_gru.w'</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">),</span>
        <span class="n">gru_bias_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"</span><span class="si">%s</span><span class="s2">_gru.b"</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">))</span>
    <span class="n">sent_vec</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">last_seq</span><span class="p">(</span><span class="n">gru</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sent_vec</span>
</pre></div>
<h3>FC implementation</h3>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_fc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">''</span><span class="p">):</span>

    <span class="sd">"""</span>
<span class="sd">    A multi-layer fully connected neural networks.</span>
<span class="sd">    :param emb: The output of the embedding layer</span>
<span class="sd">    :type emb: paddle.layer</span>
<span class="sd">    :param prefix: A prefix will be added to the layers' names.</span>
<span class="sd">    :type prefix: str</span>
<span class="sd">    """</span>

    <span class="n">_input_layer</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">pooling</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span> <span class="n">pooling_type</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">pooling</span><span class="o">.</span><span class="n">Max</span><span class="p">())</span>
    <span class="n">fc</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">_input_layer</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'</span><span class="si">%s</span><span class="s1">_fc.w'</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">),</span>
        <span class="n">bias_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"</span><span class="si">%s</span><span class="s2">_fc.b"</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">fc</span>
</pre></div>
<p>In the construction of FC, we use <code>paddle.layer.pooling</code> for the maximum pooling operation on the word vector sequence. Then we transform the sequence into a fixed dimensional vector.</p>
<h3>Multi-layer DNN implementation</h3>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_dnn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sent_vec</span><span class="p">,</span> <span class="n">prefix</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_input_layer</span> <span class="o">=</span> <span class="n">sent_vec</span>
        <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="n">name</span> <span class="o">=</span> <span class="s2">"</span><span class="si">%s</span><span class="s2">_fc_</span><span class="si">%d</span><span class="s2">_</span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="nb">id</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
            <span class="n">fc</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span>
                <span class="nb">input</span><span class="o">=</span><span class="n">_input_layer</span><span class="p">,</span>
                <span class="n">size</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
                <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
                <span class="n">param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'</span><span class="si">%s</span><span class="s1">.w'</span> <span class="o">%</span> <span class="n">name</span><span class="p">),</span>
                <span class="n">bias_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'</span><span class="si">%s</span><span class="s1">.b'</span> <span class="o">%</span> <span class="n">name</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="n">_input_layer</span> <span class="o">=</span> <span class="n">fc</span>
    <span class="k">return</span> <span class="n">_input_layer</span>
</pre></div>
<h3>Classification / Regression</h3>
<p>The structure of classification and regression is similar. Below function can be used for both tasks.
Please check the function <code>_build_classification_or_regression_model</code> in <a href="https://github.com/PaddlePaddle/models/blob/develop/dssm/network_conf.py">network_conf.py</a> for detail implementation.</p>
<h3>Pairwise Rank</h3>
<p>Please check the function <code>_build_rank_model</code> in <a href="https://github.com/PaddlePaddle/models/blob/develop/dssm/network_conf.py">network_conf.py</a> for implementation.</p>
<h2>Data Format</h2>
<p>Below is a simple example for the data in <code>./data</code></p>
<h3>Regression data format</h3>
<div class="highlight"><pre><span></span># 3 fields each line:
#   - source word list
#   - target word list
#   - target
&lt;word list&gt; \t &lt;word list&gt; \t &lt;float&gt;
</pre></div>
<p>The example of this format is as follows.</p>
<div class="highlight"><pre><span></span>Six bags of apples    Apple 6s    0.1
The new driver    The driving school    0.9
</pre></div>
<h3>Classification data format</h3>
<div class="highlight"><pre><span></span># 3 fields each line:
#   - source word list
#   - target word list
#   - target
&lt;word list&gt; \t &lt;word list&gt; \t &lt;label&gt;
</pre></div>
<p>The example of this format is as follows.</p>
<div class="highlight"><pre><span></span>Six bags of apples    Apple 6s    0
The new driver    The driving school    1
</pre></div>
<h3>Ranking data format</h3>
<div class="highlight"><pre><span></span># 4 fields each line:
#   - source word list
#   - target1 word list
#   - target2 word list
#   - label
&lt;word list&gt; \t &lt;word list&gt; \t &lt;word list&gt; \t &lt;label&gt;
</pre></div>
<p>The example of this format is as follows.</p>
<div class="highlight"><pre><span></span>Six bags of apples    Apple 6s    The new driver    1
The new driver    The driving school    Apple 6s    1
</pre></div>
<h2>Training</h2>
<p>We use <code>python train.py -y 0 --model_arch 0 --class_num 2</code> with the data in  <code>./data/classification</code> to train a DSSM model for classification. The paremeters to execute the script <code>train.py</code> can be found by execution <code>python infer.py --help</code>. Some important parameters are：</p>
<ul>
<li><code>train_data_path</code> Training data path</li>
<li><code>test_data_path</code>  Test data path, optional</li>
<li><code>source_dic_path</code>  Source dictionary path</li>
<li><code>target_dic_path</code> Target dictionary path</li>
<li><code>model_type</code>  The type of loss function of the model: classification 0, sort 1, regression 2</li>
<li><code>model_arch</code> Model structure: FC 0，CNN 1, RNN 2</li>
<li><code>dnn_dims</code> The dimension of each layer of the model is set, the default is <code>256,128,64,32</code>，with 4 layers.</li>
</ul>
<h2>To predict using the trained model</h2>
<p>The paremeters to execute the script <code>infer.py</code> can be found by execution <code>python infer.py --help</code>. Some important parameters are：</p>
<ul>
<li><code>data_path</code> Path for the data to predict</li>
<li><code>prediction_output_path</code> Prediction output path</li>
</ul>
<h2>References</h2>
<ol>
<li>Huang P S, He X, Gao J, et al. Learning deep structured semantic models for web search using clickthrough data[C]//Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management. ACM, 2013: 2333-2338.</li>
<li><a href="https://www.microsoft.com/en-us/research/project/mslr/">Microsoft Learning to Rank Datasets</a></li>
<li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/wsdm2015.v3.pdf">Gao J, He X, Deng L. Deep Learning for Web Search and Natural Language Processing[J]. Microsoft Research Technical Report, 2015.</a></li>
</ol>
{% endverbatim %}