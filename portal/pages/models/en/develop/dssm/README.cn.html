{% verbatim %}
<p>运行本目录下的程序示例需要使用PaddlePaddle v0.10.0 版本。如果您的PaddlePaddle安装版本低于此版本要求，请按照<a href="http://www.paddlepaddle.org/docs/develop/documentation/zh/build_and_install/pip_install_cn.html">安装文档</a>中的说明更新PaddlePaddle安装版本。</p>
<hr/>
<h1>深度结构化语义模型 (Deep Structured Semantic Models, DSSM)</h1>
<p>DSSM使用DNN模型在一个连续的语义空间中学习文本低纬的表示向量，并且建模两个句子间的语义相似度。本例演示如何使用PaddlePaddle实现一个通用的DSSM 模型，用于建模两个字符串间的语义相似度，模型实现支持通用的数据格式，用户替换数据便可以在真实场景中使用该模型。</p>
<h2>背景介绍</h2>
<p>DSSM [<a href="##参考文献">1</a>]是微软研究院13年提出来的经典的语义模型，用于学习两个文本之间的语义距离，广义上模型也可以推广和适用如下场景：</p>
<ol>
<li>CTR预估模型，衡量用户搜索词（Query）与候选网页集合（Documents）之间的相关联程度。</li>
<li>文本相关性，衡量两个字符串间的语义相关程度。</li>
<li>自动推荐，衡量User与被推荐的Item之间的关联程度。</li>
</ol>
<p>DSSM 已经发展成了一个框架，可以很自然地建模两个记录之间的距离关系，例如对于文本相关性问题，可以用余弦相似度 (cosin similarity) 来刻画语义距离；而对于搜索引擎的结果排序，可以在DSSM上接上Rank损失训练出一个排序模型。</p>
<h2>模型简介</h2>
<p>在原论文[<a href="#参考文献">1</a>]中，DSSM模型用来衡量用户搜索词 Query 和文档集合 Documents 之间隐含的语义关系，模型结构如下</p>
<p align="center">
<img src="./images/dssm.png"/><br/><br/>
图 1. DSSM 原始结构
</p>
<p>其贯彻的思想是， <strong>用DNN将高维特征向量转化为低纬空间的连续向量（图中红色框部分）</strong> ，<strong>在上层使用cosine similarity来衡量用户搜索词与候选文档间的语义相关性</strong> 。</p>
<p>在最顶层损失函数的设计上，原始模型使用类似Word2Vec中负例采样的方法，一个Query会抽取正例 <span class="markdown-equation" id="equation-0">$D+$</span> 和4个负例 <span class="markdown-equation" id="equation-1">$D-$</span> 整体上算条件概率用对数似然函数作为损失，这也就是图 1中类似 <span class="markdown-equation" id="equation-2">$P(D_1|Q)$</span> 的结构，具体细节请参考原论文。</p>
<p>随着后续优化DSSM模型的结构得以简化[<a href="#参考文献">3</a>]，演变为：</p>
<p align="center">
<img src="./images/dssm2.png" width="600"/><br/><br/>
图 2. DSSM通用结构
</p>
<p>图中的空白方框可以用任何模型替代，例如：全连接FC，卷积CNN，RNN等。该模型结构专门用于衡量两个元素（比如字符串）间的语义距离。在实际任务中，DSSM模型会作为基础的积木，搭配上不同的损失函数来实现具体的功能，比如：</p>
<ul>
<li>在排序学习中，将 图 2 中结构添加 pairwise rank损失，变成一个排序模型</li>
<li>在CTR预估中，对点击与否做0，1二元分类，添加交叉熵损失变成一个分类模型</li>
<li>在需要对一个子串打分时，可以使用余弦相似度来计算相似度，变成一个回归模型</li>
</ul>
<p>本例提供一个比较通用的解决方案，在模型任务类型上支持：</p>
<ul>
<li>分类</li>
<li>[-1, 1] 值域内的回归</li>
<li>Pairwise-Rank</li>
</ul>
<p>在生成低纬语义向量的模型结构上，支持以下三种：</p>
<ul>
<li>FC, 多层全连接层</li>
<li>CNN，卷积神经网络</li>
<li>RNN，递归神经网络</li>
</ul>
<h2>模型实现</h2>
<p>DSSM模型可以拆成三部分：分别是左边和右边的DNN，以及顶层的损失函数。在复杂任务中，左右两边DNN的结构可以不同。在原始论文中左右网络分别学习Query和Document的语义向量，两者数据的数据不同，建议对应定制DNN的结构。</p>
<p><strong>本例中为了简便和通用，将左右两个DNN的结构设为相同，因此只提供三个选项FC、CNN、RNN</strong>。</p>
<p>损失函数的设计也支持三种类型：分类, 回归, 排序；其中，在回归和排序两种损失中，左右两边的匹配程度通过余弦相似度（cosine similairty）来计算；在分类任务中，类别预测的分布通过softmax计算。</p>
<p>在其它教程中，对上述很多内容都有过详细的介绍，例如：</p>
<ul>
<li>如何CNN, FC 做文本信息提取可以参考 <a href="https://github.com/PaddlePaddle/models/blob/develop/text_classification/README.md#模型详解">text classification</a></li>
<li>RNN/GRU 的内容可以参考 <a href="https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/README.md#gated-recurrent-unit-gru">Machine Translation</a></li>
<li>Pairwise Rank即排序学习可参考 <a href="https://github.com/PaddlePaddle/models/blob/develop/ltr/README.md">learn to rank</a></li>
</ul>
<p>相关原理在此不再赘述，本文接下来的篇幅主要集中介绍使用PaddlePaddle实现这些结构上。</p>
<p>如图3，回归和分类模型的结构相似:</p>
<p align="center">
<img src="./images/dssm3.jpg"/><br/><br/>
图 3. DSSM for REGRESSION or CLASSIFICATION
</p>
<p>最重要的组成部分包括词向量，图中<code>(1)</code>,<code>(2)</code>两个低纬向量的学习器（可以用RNN/CNN/FC中的任意一种实现），最上层对应的损失函数。</p>
<p>Pairwise Rank的结构会复杂一些，图 4. 中的结构会出现两次，增加了对应的损失函数，模型总体思想是：
- 给定同一个source(源)为左右两个target(目标)分别打分——<code>(a),(b)</code>，学习目标是(a),(b)之间的大小关系
- <code>(a)</code>和<code>(b)</code>类似图3中结构，用于给source和target的pair打分
- <code>(1)</code>和<code>(2)</code>的结构其实是共用的，都表示同一个source，图中为了表达效果展开成两个</p>
<p align="center">
<img src="./images/dssm2.jpg"/><br/><br/>
图 4. DSSM for Pairwise Rank
</p>
<p>下面是各个部分的具体实现，相关代码均包含在 <code>./network_conf.py</code> 中。</p>
<h3>创建文本的词向量表</h3>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">''</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Create word embedding. The `prefix` is added in front of the name of</span>
<span class="sd">    embedding"s learnable parameter.</span>
<span class="sd">    """</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Create embedding table [</span><span class="si">%s</span><span class="s2">] whose dimention is </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span>
                <span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'</span><span class="si">%s</span><span class="s1">_emb.w'</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">emb</span>
</pre></div>
<p>由于输入给词向量表(embedding table)的是一个句子对应的词的ID的列表 ，因此词向量表输出的是词向量的序列。</p>
<h3>CNN 结构实现</h3>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_cnn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">''</span><span class="p">):</span>

    <span class="sd">"""</span>
<span class="sd">    A multi-layer CNN.</span>
<span class="sd">    :param emb: The word embedding.</span>
<span class="sd">    :type emb: paddle.layer</span>
<span class="sd">    :param prefix: The prefix will be added to of layers' names.</span>
<span class="sd">    :type prefix: str</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="nf">create_conv</span><span class="p">(</span><span class="n">context_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">prefix</span><span class="p">):</span>
        <span class="n">key</span> <span class="o">=</span> <span class="s2">"</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">_</span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">context_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">conv</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">sequence_conv_pool</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
            <span class="n">context_len</span><span class="o">=</span><span class="n">context_len</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="c1"># set parameter attr for parameter sharing</span>
            <span class="n">context_proj_param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">key</span> <span class="o">+</span> <span class="s2">"contex_proj.w"</span><span class="p">),</span>
            <span class="n">fc_param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">key</span> <span class="o">+</span> <span class="s2">"_fc.w"</span><span class="p">),</span>
            <span class="n">fc_bias_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">key</span> <span class="o">+</span> <span class="s2">"_fc.b"</span><span class="p">),</span>
            <span class="n">pool_bias_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">key</span> <span class="o">+</span> <span class="s2">"_pool.b"</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">conv</span>

    <span class="n">conv_3</span> <span class="o">=</span> <span class="n">create_conv</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">"cnn"</span><span class="p">)</span>
    <span class="n">conv_4</span> <span class="o">=</span> <span class="n">create_conv</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">"cnn"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">conv_3</span><span class="p">,</span> <span class="n">conv_4</span><span class="p">])</span>
</pre></div>
<p>CNN 接受词向量序列，通过卷积和池化操作捕捉到原始句子的关键信息，最终输出一个语义向量（可以认为是句子向量）。</p>
<p>本例的实现中，分别使用了窗口长度为3和4的CNN学到的句子向量按元素求和得到最终的句子向量。</p>
<h3>RNN 结构实现</h3>
<p>RNN很适合学习变长序列的信息，使用RNN来学习句子的信息几乎是自然语言处理任务的标配。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_rnn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">''</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    A GRU sentence vector learner.</span>
<span class="sd">    """</span>
    <span class="n">gru</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">simple_gru</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">mixed_param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'</span><span class="si">%s</span><span class="s1">_gru_mixed.w'</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">),</span>
        <span class="n">mixed_bias_param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"</span><span class="si">%s</span><span class="s2">_gru_mixed.b"</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">),</span>
        <span class="n">gru_param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'</span><span class="si">%s</span><span class="s1">_gru.w'</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">),</span>
        <span class="n">gru_bias_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"</span><span class="si">%s</span><span class="s2">_gru.b"</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">))</span>
    <span class="n">sent_vec</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">last_seq</span><span class="p">(</span><span class="n">gru</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sent_vec</span>
</pre></div>
<h3>多层全连接网络FC</h3>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_fc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">''</span><span class="p">):</span>

    <span class="sd">"""</span>
<span class="sd">    A multi-layer fully connected neural networks.</span>
<span class="sd">    :param emb: The output of the embedding layer</span>
<span class="sd">    :type emb: paddle.layer</span>
<span class="sd">    :param prefix: A prefix will be added to the layers' names.</span>
<span class="sd">    :type prefix: str</span>
<span class="sd">    """</span>

    <span class="n">_input_layer</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">pooling</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span> <span class="n">pooling_type</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">pooling</span><span class="o">.</span><span class="n">Max</span><span class="p">())</span>
    <span class="n">fc</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">_input_layer</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'</span><span class="si">%s</span><span class="s1">_fc.w'</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">),</span>
        <span class="n">bias_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"</span><span class="si">%s</span><span class="s2">_fc.b"</span> <span class="o">%</span> <span class="n">prefix</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">fc</span>
</pre></div>
<p>在构建全连接网络时首先使用<code>paddle.layer.pooling</code> 对词向量序列进行最大池化操作，将边长序列转化为一个固定维度向量，作为整个句子的语义表达，使用最大池化能够降低句子长度对句向量表达的影响。</p>
<h3>多层DNN</h3>
<p>在 CNN/DNN/FC提取出 semantic vector后，在上层可继续接多层FC来实现深层DNN结构。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_dnn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sent_vec</span><span class="p">,</span> <span class="n">prefix</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_input_layer</span> <span class="o">=</span> <span class="n">sent_vec</span>
        <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dnn_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="n">name</span> <span class="o">=</span> <span class="s2">"</span><span class="si">%s</span><span class="s2">_fc_</span><span class="si">%d</span><span class="s2">_</span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="nb">id</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
            <span class="n">fc</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span>
                <span class="nb">input</span><span class="o">=</span><span class="n">_input_layer</span><span class="p">,</span>
                <span class="n">size</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
                <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
                <span class="n">param_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'</span><span class="si">%s</span><span class="s1">.w'</span> <span class="o">%</span> <span class="n">name</span><span class="p">),</span>
                <span class="n">bias_attr</span><span class="o">=</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'</span><span class="si">%s</span><span class="s1">.b'</span> <span class="o">%</span> <span class="n">name</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="n">_input_layer</span> <span class="o">=</span> <span class="n">fc</span>
    <span class="k">return</span> <span class="n">_input_layer</span>
</pre></div>
<h3>分类及回归</h3>
<p>分类和回归的结构比较相似，具体实现请参考<a href="https://github.com/PaddlePaddle/models/blob/develop/dssm/network_conf.py">network_conf.py</a>中的
<code>_build_classification_or_regression_model</code> 函数。</p>
<h3>Pairwise Rank</h3>
<p>Pairwise Rank复用上面的DNN结构，同一个source对两个target求相似度打分，如果左边的target打分高，预测为1，否则预测为 0。实现请参考 <a href="https://github.com/PaddlePaddle/models/blob/develop/dssm/network_conf.py">network_conf.py</a> 中的<code>_build_rank_model</code> 函数。</p>
<h2>数据格式</h2>
<p>在 <code>./data</code> 中有简单的示例数据</p>
<h3>回归的数据格式</h3>
<div class="highlight"><pre><span></span># 3 fields each line:
#   - source word list
#   - target word list
#   - target
&lt;word list&gt; \t &lt;word list&gt; \t &lt;float&gt;
</pre></div>
<p>比如：</p>
<div class="highlight"><pre><span></span>苹果 六 袋    苹果 6s    0.1
新手 汽车 驾驶    驾校 培训    0.9
</pre></div>
<h3>分类的数据格式</h3>
<div class="highlight"><pre><span></span># 3 fields each line:
#   - source word list
#   - target word list
#   - target
&lt;word list&gt; \t &lt;word list&gt; \t &lt;label&gt;
</pre></div>
<p>比如：</p>
<div class="highlight"><pre><span></span>苹果 六 袋    苹果 6s    0
新手 汽车 驾驶    驾校 培训    1
</pre></div>
<h3>排序的数据格式</h3>
<div class="highlight"><pre><span></span># 4 fields each line:
#   - source word list
#   - target1 word list
#   - target2 word list
#   - label
&lt;word list&gt; \t &lt;word list&gt; \t &lt;word list&gt; \t &lt;label&gt;
</pre></div>
<p>比如：</p>
<div class="highlight"><pre><span></span>苹果 六 袋    苹果 6s    新手 汽车 驾驶    1
新手 汽车 驾驶    驾校 培训    苹果 6s    1
</pre></div>
<h2>执行训练</h2>
<p>可以直接执行 <code>python train.py -y 0 --model_arch 0 --class_num 2</code> 使用 <code>./data/classification</code> 目录里的实例数据来测试能否直接运行训练分类FC模型。</p>
<p>其他模型结构也可以通过命令行实现定制，详细命令行参数请执行 <code>python train.py --help</code>进行查阅。</p>
<p>这里介绍最重要的几个参数：</p>
<ul>
<li><code>train_data_path</code> 训练数据路径</li>
<li><code>test_data_path</code> 测试数据路局，可以不设置</li>
<li><code>source_dic_path</code> 源字典字典路径</li>
<li><code>target_dic_path</code> 目标字典路径</li>
<li><code>model_type</code> 模型的损失函数的类型，分类0，排序1，回归2</li>
<li><code>model_arch</code> 模型结构，FC 0， CNN 1, RNN 2</li>
<li><code>dnn_dims</code> 模型各层的维度设置，默认为 <code>256,128,64,32</code>，即模型有4层，各层维度如上设置</li>
</ul>
<h2>使用训练好的模型预测</h2>
<p>详细命令行参数请执行 <code>python infer.py --help</code>进行查阅。重要参数解释如下：</p>
<ul>
<li><code>data_path</code> 需要预测的数据路径</li>
<li><code>prediction_output_path</code> 预测的输出路径</li>
</ul>
<h2>参考文献</h2>
<ol>
<li>Huang P S, He X, Gao J, et al. Learning deep structured semantic models for web search using clickthrough data[C]//Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management. ACM, 2013: 2333-2338.</li>
<li><a href="https://www.microsoft.com/en-us/research/project/mslr/">Microsoft Learning to Rank Datasets</a></li>
<li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/wsdm2015.v3.pdf">Gao J, He X, Deng L. Deep Learning for Web Search and Natural Language Processing[J]. Microsoft Research Technical Report, 2015.</a></li>
</ol>
{% endverbatim %}