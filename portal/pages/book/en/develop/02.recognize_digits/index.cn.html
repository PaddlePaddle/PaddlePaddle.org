{% verbatim %}
<h1>识别数字</h1>
<p>本教程源代码目录在<a href="https://github.com/PaddlePaddle/book/tree/develop/02.recognize_digits">book/recognize_digits</a>， 初次使用请参考PaddlePaddle<a href="https://github.com/PaddlePaddle/book/blob/develop/README.cn.md#运行这本书">安装教程</a>，更多内容请参考本教程的<a href="http://bit.baidu.com/course/detail/id/167.html">视频课堂</a>。</p>
<h2>背景介绍</h2>
<p>当我们学习编程的时候，编写的第一个程序一般是实现打印"Hello World"。而机器学习（或深度学习）的入门教程，一般都是 <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> 数据库上的手写识别问题。原因是手写识别属于典型的图像分类问题，比较简单，同时MNIST数据集也很完备。MNIST数据集作为一个简单的计算机视觉数据集，包含一系列如图1所示的手写数字图片和对应的标签。图片是28x28的像素矩阵，标签则对应着0~9的10个数字。每张图片都经过了大小归一化和居中处理。</p>
<p align="center">
<img src="image/mnist_example_image.png" width="400"/><br/>
图1. MNIST图片示例
</p>
<p>MNIST数据集是从 <a href="https://www.nist.gov/srd/nist-special-database-19">NIST</a> 的Special Database 3（SD-3）和Special Database 1（SD-1）构建而来。由于SD-3是由美国人口调查局的员工进行标注，SD-1是由美国高中生进行标注，因此SD-3比SD-1更干净也更容易识别。Yann LeCun等人从SD-1和SD-3中各取一半作为MNIST的训练集（60000条数据）和测试集（10000条数据），其中训练集来自250位不同的标注员，此外还保证了训练集和测试集的标注员是不完全相同的。</p>
<p>Yann LeCun早先在手写字符识别上做了很多研究，并在研究过程中提出了卷积神经网络（Convolutional Neural Network），大幅度地提高了手写字符的识别能力，也因此成为了深度学习领域的奠基人之一。如今的深度学习领域，卷积神经网络占据了至关重要的地位，从最早Yann LeCun提出的简单LeNet，到如今ImageNet大赛上的优胜模型VGGNet、GoogLeNet、ResNet等（请参见<a href="https://github.com/PaddlePaddle/book/tree/develop/03.image_classification">图像分类</a> 教程），人们在图像分类领域，利用卷积神经网络得到了一系列惊人的结果。</p>
<p>有很多算法在MNIST上进行实验。1998年，LeCun分别用单层线性分类器、多层感知器（Multilayer Perceptron, MLP）和多层卷积神经网络LeNet进行实验，使得测试集上的误差不断下降（从12%下降到0.7%）[<a href="#参考文献">1</a>]。此后，科学家们又基于K近邻（K-Nearest Neighbors）算法[<a href="#参考文献">2</a>]、支持向量机（SVM）[<a href="#参考文献">3</a>]、神经网络[<a href="#参考文献">4-7</a>]和Boosting方法[<a href="#参考文献">8</a>]等做了大量实验，并采用多种预处理方法（如去除歪曲、去噪、模糊等）来提高识别的准确率。</p>
<p>本教程中，我们从简单的模型Softmax回归开始，带大家入门手写字符识别，并逐步进行模型优化。</p>
<h2>模型概览</h2>
<p>基于MNIST数据训练一个分类器，在介绍本教程使用的三个基本图像分类网络前，我们先给出一些定义：
- <span class="markdown-equation" id="equation-0">$X$</span>是输入：MNIST图片是<span class="markdown-equation" id="equation-1">$28\times28$</span> 的二维图像，为了进行计算，我们将其转化为<span class="markdown-equation" id="equation-2">$784$</span>维向量，即<span class="markdown-equation" id="equation-3">$X=\left ( x_0, x_1, \dots, x_{783} \right )$</span>。
- <span class="markdown-equation" id="equation-4">$Y$</span>是输出：分类器的输出是10类数字（0-9），即<span class="markdown-equation" id="equation-5">$Y=\left ( y_0, y_1, \dots, y_9 \right )$</span>，每一维<span class="markdown-equation" id="equation-6">$y_i$</span>代表图片分类为第<span class="markdown-equation" id="equation-7">$i$</span>类数字的概率。
- <span class="markdown-equation" id="equation-8">$L$</span>是图片的真实标签：<span class="markdown-equation" id="equation-9">$L=\left ( l_0, l_1, \dots, l_9 \right )$</span>也是10维，但只有一维为1，其他都为0。</p>
<h3>Softmax回归(Softmax Regression)</h3>
<p>最简单的Softmax回归模型是先将输入层经过一个全连接层得到的特征，然后直接通过softmax 函数进行多分类[<a href="#参考文献">9</a>]。</p>
<p>输入层的数据<span class="markdown-equation" id="equation-0">$X$</span>传到输出层，在激活操作之前，会乘以相应的权重 <span class="markdown-equation" id="equation-11">$W$</span> ，并加上偏置变量 <span class="markdown-equation" id="equation-12">$b$</span> ，具体如下：</p>
<p><span class="markdown-equation" id="equation-13">$$ y_i = \text{softmax}(\sum_j W_{i,j}x_j + b_i) $$</span></p>
<p>其中 <span class="markdown-equation" id="equation-14">$ \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}} $</span></p>
<p>对于有 <span class="markdown-equation" id="equation-15">$N$</span> 个类别的多分类问题，指定 <span class="markdown-equation" id="equation-15">$N$</span> 个输出节点，<span class="markdown-equation" id="equation-15">$N$</span> 维结果向量经过softmax将归一化为 <span class="markdown-equation" id="equation-15">$N$</span> 个[0,1]范围内的实数值，分别表示该样本属于这 <span class="markdown-equation" id="equation-15">$N$</span> 个类别的概率。此处的 <span class="markdown-equation" id="equation-6">$y_i$</span> 即对应该图片为数字 <span class="markdown-equation" id="equation-7">$i$</span> 的预测概率。</p>
<p>在分类问题中，我们一般采用交叉熵代价损失函数（cross entropy），公式如下：</p>
<p><span class="markdown-equation" id="equation-22">$$  \text{crossentropy}(label, y) = -\sum_i label_ilog(y_i) $$</span></p>
<p>图2为softmax回归的网络图，图中权重用蓝线表示、偏置用红线表示、+1代表偏置参数的系数为1。</p>
<p align="center">
<img src="image/softmax_regression.png" width="400"/><br/>
图2. softmax回归网络结构图<br/>
</p>
<h3>多层感知器(Multilayer Perceptron, MLP)</h3>
<p>Softmax回归模型采用了最简单的两层神经网络，即只有输入层和输出层，因此其拟合能力有限。为了达到更好的识别效果，我们考虑在输入层和输出层中间加上若干个隐藏层[<a href="#参考文献">10</a>]。</p>
<ol>
<li>经过第一个隐藏层，可以得到 <span class="markdown-equation" id="equation-23">$ H_1 = \phi(W_1X + b_1) $</span>，其中<span class="markdown-equation" id="equation-24">$\phi$</span>代表激活函数，常见的有sigmoid、tanh或ReLU等函数。</li>
<li>经过第二个隐藏层，可以得到 <span class="markdown-equation" id="equation-25">$ H_2 = \phi(W_2H_1 + b_2) $</span>。</li>
<li>最后，再经过输出层，得到的<span class="markdown-equation" id="equation-26">$Y=\text{softmax}(W_3H_2 + b_3)$</span>，即为最后的分类结果向量。</li>
</ol>
<p>图3为多层感知器的网络结构图，图中权重用蓝线表示、偏置用红线表示、+1代表偏置参数的系数为1。</p>
<p align="center">
<img src="image/mlp.png" width="500"/><br/>
图3. 多层感知器网络结构图<br/>
</p>
<h3>卷积神经网络(Convolutional Neural Network, CNN)</h3>
<p>在多层感知器模型中，将图像展开成一维向量输入到网络中，忽略了图像的位置和结构信息，而卷积神经网络能够更好的利用图像的结构信息。<a href="http://yann.lecun.com/exdb/lenet/">LeNet-5</a>是一个较简单的卷积神经网络。图4显示了其结构：输入的二维图像，先经过两次卷积层到池化层，再经过全连接层，最后使用softmax分类作为输出层。下面我们主要介绍卷积层和池化层。</p>
<p align="center">
<img src="image/cnn.png"/><br/>
图4. LeNet-5卷积神经网络结构<br/>
</p>
<h4>卷积层</h4>
<p>卷积层是卷积神经网络的核心基石。在图像识别里我们提到的卷积是二维卷积，即离散二维滤波器（也称作卷积核）与二维图像做卷积操作，简单的讲是二维滤波器滑动到二维图像上所有位置，并在每个位置上与该像素点及其领域像素点做内积。卷积操作被广泛应用与图像处理领域，不同卷积核可以提取不同的特征，例如边沿、线性、角等特征。在深层卷积神经网络中，通过卷积操作可以提取出图像低级到复杂的特征。</p>
<p align="center">
<img src="image/conv_layer.png" width="750"/><br/>
图5. 卷积层图片<br/>
</p>
<p>图5给出一个卷积计算过程的示例图，输入图像大小为<span class="markdown-equation" id="equation-27">$H=5,W=5,D=3$</span>，即<span class="markdown-equation" id="equation-28">$5 \times 5$</span>大小的3通道（RGB，也称作深度）彩色图像。这个示例图中包含两（用<span class="markdown-equation" id="equation-29">$K$</span>表示）组卷积核，即图中滤波器<span class="markdown-equation" id="equation-30">$W_0$</span>和<span class="markdown-equation" id="equation-31">$W_1$</span>。在卷积计算中，通常对不同的输入通道采用不同的卷积核，如图示例中每组卷积核包含（<span class="markdown-equation" id="equation-32">$D=3）$</span>个<span class="markdown-equation" id="equation-33">$3 \times 3$</span>（用<span class="markdown-equation" id="equation-34">$F \times F$</span>表示）大小的卷积核。另外，这个示例中卷积核在图像的水平方向（<span class="markdown-equation" id="equation-11">$W$</span>方向）和垂直方向（<span class="markdown-equation" id="equation-36">$H$</span>方向）的滑动步长为2（用<span class="markdown-equation" id="equation-37">$S$</span>表示）；对输入图像周围各填充1（用<span class="markdown-equation" id="equation-38">$P$</span>表示）个0，即图中输入层原始数据为蓝色部分，灰色部分是进行了大小为1的扩展，用0来进行扩展。经过卷积操作得到输出为<span class="markdown-equation" id="equation-39">$3 \times 3 \times 2$</span>（用<span class="markdown-equation" id="equation-40">$H_{o} \times W_{o} \times K$</span>表示）大小的特征图，即<span class="markdown-equation" id="equation-33">$3 \times 3$</span>大小的2通道特征图，其中<span class="markdown-equation" id="equation-42">$H_o$</span>计算公式为：<span class="markdown-equation" id="equation-43">$H_o = (H - F + 2 \times P)/S + 1$</span>，<span class="markdown-equation" id="equation-44">$W_o$</span>同理。 而输出特征图中的每个像素，是每组滤波器与输入图像每个特征图的内积再求和，再加上偏置<span class="markdown-equation" id="equation-45">$b_o$</span>，偏置通常对于每个输出特征图是共享的。输出特征图<span class="markdown-equation" id="equation-46">$o[:,:,0]$</span>中的最后一个<span class="markdown-equation" id="equation-47">$-2$</span>计算如图5右下角公式所示。</p>
<p>在卷积操作中卷积核是可学习的参数，经过上面示例介绍，每层卷积的参数大小为<span class="markdown-equation" id="equation-48">$D \times F \times F \times K$</span>。在多层感知器模型中，神经元通常是全部连接，参数较多。而卷积层的参数较少，这也是由卷积层的主要特性即局部连接和共享权重所决定。</p>
<ul>
<li>
<p>局部连接：每个神经元仅与输入神经元的一块区域连接，这块局部区域称作感受野（receptive field）。在图像卷积操作中，即神经元在空间维度（spatial dimension，即上图示例H和W所在的平面）是局部连接，但在深度上是全部连接。对于二维图像本身而言，也是局部像素关联较强。这种局部连接保证了学习后的过滤器能够对于局部的输入特征有最强的响应。局部连接的思想，也是受启发于生物学里面的视觉系统结构，视觉皮层的神经元就是局部接受信息的。</p>
</li>
<li>
<p>权重共享：计算同一个深度切片的神经元时采用的滤波器是共享的。例如图4中计算<span class="markdown-equation" id="equation-46">$o[:,:,0]$</span>的每个每个神经元的滤波器均相同，都为<span class="markdown-equation" id="equation-30">$W_0$</span>，这样可以很大程度上减少参数。共享权重在一定程度上讲是有意义的，例如图片的底层边缘特征与特征在图中的具体位置无关。但是在一些场景中是无意的，比如输入的图片是人脸，眼睛和头发位于不同的位置，希望在不同的位置学到不同的特征 (参考<a href="http://cs231n.github.io/convolutional-networks/">斯坦福大学公开课</a>)。请注意权重只是对于同一深度切片的神经元是共享的，在卷积层，通常采用多组卷积核提取不同特征，即对应不同深度切片的特征，不同深度切片的神经元权重是不共享。另外，偏重对同一深度切片的所有神经元都是共享的。</p>
</li>
</ul>
<p>通过介绍卷积计算过程及其特性，可以看出卷积是线性操作，并具有平移不变性（shift-invariant），平移不变性即在图像每个位置执行相同的操作。卷积层的局部连接和权重共享使得需要学习的参数大大减小，这样也有利于训练较大卷积神经网络。</p>
<h4>池化层</h4>
<p align="center">
<img src="image/max_pooling.png" width="400px"/><br/>
图6. 池化层图片<br/>
</p>
<p>池化是非线性下采样的一种形式，主要作用是通过减少网络的参数来减小计算量，并且能够在一定程度上控制过拟合。通常在卷积层的后面会加上一个池化层。池化包括最大池化、平均池化等。其中最大池化是用不重叠的矩形框将输入层分成不同的区域，对于每个矩形框的数取最大值作为输出层，如图6所示。</p>
<p>更详细的关于卷积神经网络的具体知识可以参考<a href="http://cs231n.github.io/convolutional-networks/">斯坦福大学公开课</a>和<a href="https://github.com/PaddlePaddle/book/blob/develop/image_classification/README.md">图像分类</a>教程。</p>
<h3>常见激活函数介绍</h3>
<ul>
<li>
<p>sigmoid激活函数： <span class="markdown-equation" id="equation-51">$ f(x) = sigmoid(x) = \frac{1}{1+e^{-x}} $</span></p>
</li>
<li>
<p>tanh激活函数： <span class="markdown-equation" id="equation-52">$ f(x) = tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}} $</span></p>
</li>
</ul>
<p>实际上，tanh函数只是规模变化的sigmoid函数，将sigmoid函数值放大2倍之后再向下平移1个单位：tanh(x) = 2sigmoid(2x) - 1 。</p>
<ul>
<li>ReLU激活函数： <span class="markdown-equation" id="equation-53">$ f(x) = max(0, x) $</span></li>
</ul>
<p>更详细的介绍请参考<a href="https://en.wikipedia.org/wiki/Activation_function">维基百科激活函数</a>。</p>
<h2>数据介绍</h2>
<p>PaddlePaddle在API中提供了自动加载<a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>数据的模块<code>paddle.dataset.mnist</code>。加载后的数据位于<code>/home/username/.cache/paddle/dataset/mnist</code>下：</p>
<table>
<thead>
<tr>
<th>文件名称</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>train-images-idx3-ubyte</td>
<td>训练数据图片，60,000条数据</td>
</tr>
<tr>
<td>train-labels-idx1-ubyte</td>
<td>训练数据标签，60,000条数据</td>
</tr>
<tr>
<td>t10k-images-idx3-ubyte</td>
<td>测试数据图片，10,000条数据</td>
</tr>
<tr>
<td>t10k-labels-idx1-ubyte</td>
<td>测试数据标签，10,000条数据</td>
</tr>
</tbody>
</table>
<h2>配置说明</h2>
<p>首先，加载PaddlePaddle的V2 api包。</p>
<p></p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">paddle.v2</span> <span class="kn">as</span> <span class="nn">paddle</span>
</pre></div>
其次，定义三个不同的分类器：
<ul>
<li>Softmax回归：只通过一层简单的以softmax为激活函数的全连接层，就可以得到分类的结果。</li>
</ul>
<p></p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax_regression</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">predict</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">img</span><span class="p">,</span>
                              <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                              <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Softmax</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">predict</span>
</pre></div>
- 多层感知器：下面代码实现了一个含有两个隐藏层（即全连接层）的多层感知器。其中两个隐藏层的激活函数均采用ReLU，输出层的激活函数用Softmax。
<p></p><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">multilayer_perceptron</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="c1"># 第一个全连接层，激活函数为ReLU</span>
    <span class="n">hidden1</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">img</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Relu</span><span class="p">())</span>
    <span class="c1"># 第二个全连接层，激活函数为ReLU</span>
    <span class="n">hidden2</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">hidden1</span><span class="p">,</span>
                              <span class="n">size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                              <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Relu</span><span class="p">())</span>
    <span class="c1"># 以softmax为激活函数的全连接输出层，输出层的大小必须为数字的个数10</span>
    <span class="n">predict</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">hidden2</span><span class="p">,</span>
                              <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                              <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Softmax</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">predict</span>
</pre></div>
- 卷积神经网络LeNet-5: 输入的二维图像，首先经过两次卷积层到池化层，再经过全连接层，最后使用以softmax为激活函数的全连接层作为输出层。
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">convolutional_neural_network</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="c1"># 第一个卷积-池化层</span>
    <span class="n">conv_pool_1</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">simple_img_conv_pool</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">img</span><span class="p">,</span>
        <span class="n">filter_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">num_filters</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">num_channel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">pool_stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Relu</span><span class="p">())</span>
    <span class="c1"># 第二个卷积-池化层</span>
    <span class="n">conv_pool_2</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">simple_img_conv_pool</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">conv_pool_1</span><span class="p">,</span>
        <span class="n">filter_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">num_filters</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">num_channel</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">pool_stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Relu</span><span class="p">())</span>
    <span class="c1"># 以softmax为激活函数的全连接输出层，输出层的大小必须为数字的个数10</span>
    <span class="n">predict</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">conv_pool_2</span><span class="p">,</span>
                              <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                              <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Softmax</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">predict</span>
</pre></div>
<p>接着，通过<code>layer.data</code>调用来获取数据，然后调用分类器（这里我们提供了三个不同的分类器）得到分类结果。训练时，对该结果计算其损失函数，分类问题常常选择交叉熵损失函数。</p>
<div class="highlight"><pre><span></span><span class="c1"># 该模型运行在单个CPU上</span>
<span class="n">paddle</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">trainer_count</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">images</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">'pixel'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">data_type</span><span class="o">.</span><span class="n">dense_vector</span><span class="p">(</span><span class="mi">784</span><span class="p">))</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">'label'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">data_type</span><span class="o">.</span><span class="n">integer_value</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

<span class="c1"># predict = softmax_regression(images) # Softmax回归</span>
<span class="c1"># predict = multilayer_perceptron(images) #多层感知器</span>
<span class="n">predict</span> <span class="o">=</span> <span class="n">convolutional_neural_network</span><span class="p">(</span><span class="n">images</span><span class="p">)</span> <span class="c1">#LeNet5卷积神经网络</span>

<span class="n">cost</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">classification_cost</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">predict</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
</pre></div>
<p>然后，指定训练相关的参数。
- 训练方法（optimizer)： 代表训练过程在更新权重时采用动量优化器 <code>Momentum</code> ，其中参数0.9代表动量优化每次保持前一次速度的0.9倍。
- 训练速度（learning_rate）： 迭代的速度，与网络的训练收敛速度有关系。
- 正则化（regularization）： 是防止网络过拟合的一种手段，此处采用L2正则化。</p>
<div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span> <span class="o">/</span> <span class="mf">128.0</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">regularization</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">L2Regularization</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.0005</span> <span class="o">*</span> <span class="mi">128</span><span class="p">))</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span>
                             <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                             <span class="n">update_equation</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
</pre></div>
<p>下一步，我们开始训练过程。<code>paddle.dataset.movielens.train()</code>和<code>paddle.dataset.movielens.test()</code>分别做训练和测试数据集。这两个函数各自返回一个reader——PaddlePaddle中的reader是一个Python函数，每次调用的时候返回一个Python yield generator。</p>
<p>下面<code>shuffle</code>是一个reader decorator，它接受一个reader A，返回另一个reader B —— reader B 每次读入<code>buffer_size</code>条训练数据到一个buffer里，然后随机打乱其顺序，并且逐条输出。</p>
<p><code>batch</code>是一个特殊的decorator，它的输入是一个reader，输出是一个batched reader —— 在PaddlePaddle里，一个reader每次yield一条训练数据，而一个batched reader每次yield一个minibatch。</p>
<p><code>event_handler_plot</code>可以用来在训练过程中画图如下：</p>
<p><img alt="png" src="./image/train_and_test.png"/></p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">paddle.v2.plot</span> <span class="kn">import</span> <span class="n">Ploter</span>

<span class="n">train_title</span> <span class="o">=</span> <span class="s2">"Train cost"</span>
<span class="n">test_title</span> <span class="o">=</span> <span class="s2">"Test cost"</span>
<span class="n">cost_ploter</span> <span class="o">=</span> <span class="n">Ploter</span><span class="p">(</span><span class="n">train_title</span><span class="p">,</span> <span class="n">test_title</span><span class="p">)</span>

<span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># event_handler to plot a figure</span>
<span class="k">def</span> <span class="nf">event_handler_plot</span><span class="p">(</span><span class="n">event</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">step</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">paddle</span><span class="o">.</span><span class="n">event</span><span class="o">.</span><span class="n">EndIteration</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">cost_ploter</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_title</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">cost</span><span class="p">)</span>
            <span class="n">cost_ploter</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
        <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">paddle</span><span class="o">.</span><span class="n">event</span><span class="o">.</span><span class="n">EndPass</span><span class="p">):</span>
        <span class="c1"># save parameters</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">'params_pass_</span><span class="si">%d</span><span class="s1">.tar'</span> <span class="o">%</span> <span class="n">event</span><span class="o">.</span><span class="n">pass_id</span><span class="p">,</span> <span class="s1">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">save_parameter_to_tar</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">reader</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span>
            <span class="n">paddle</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="p">(),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">))</span>
        <span class="n">cost_ploter</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_title</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">cost</span><span class="p">)</span>
</pre></div>
<p><code>event_handler</code> 用来在训练过程中输出训练结果
</p><div class="highlight"><pre><span></span><span class="n">lists</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">def</span> <span class="nf">event_handler</span><span class="p">(</span><span class="n">event</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">paddle</span><span class="o">.</span><span class="n">event</span><span class="o">.</span><span class="n">EndIteration</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">event</span><span class="o">.</span><span class="n">batch_id</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="s2">"Pass </span><span class="si">%d</span><span class="s2">, Batch </span><span class="si">%d</span><span class="s2">, Cost </span><span class="si">%f</span><span class="s2">, </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span>
                <span class="n">event</span><span class="o">.</span><span class="n">pass_id</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">batch_id</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">cost</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">paddle</span><span class="o">.</span><span class="n">event</span><span class="o">.</span><span class="n">EndPass</span><span class="p">):</span>
        <span class="c1"># save parameters</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">'params_pass_</span><span class="si">%d</span><span class="s1">.tar'</span> <span class="o">%</span> <span class="n">event</span><span class="o">.</span><span class="n">pass_id</span><span class="p">,</span> <span class="s1">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">save_parameter_to_tar</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">reader</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span>
            <span class="n">paddle</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="p">(),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">))</span>
        <span class="k">print</span> <span class="s2">"Test with Pass </span><span class="si">%d</span><span class="s2">, Cost </span><span class="si">%f</span><span class="s2">, </span><span class="si">%s</span><span class="se">\n</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span>
            <span class="n">event</span><span class="o">.</span><span class="n">pass_id</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">cost</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
        <span class="n">lists</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">event</span><span class="o">.</span><span class="n">pass_id</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">cost</span><span class="p">,</span>
                      <span class="n">result</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">'classification_error_evaluator'</span><span class="p">]))</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">reader</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span>
        <span class="n">paddle</span><span class="o">.</span><span class="n">reader</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span>
            <span class="n">paddle</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="p">(),</span> <span class="n">buf_size</span><span class="o">=</span><span class="mi">8192</span><span class="p">),</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span>
    <span class="n">event_handler</span><span class="o">=</span><span class="n">event_handler_plot</span><span class="p">,</span>
    <span class="n">num_passes</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
<p>训练过程是完全自动的，event_handler里打印的日志类似如下所示：</p>
<div class="highlight"><pre><span></span># Pass 0, Batch 0, Cost 2.780790, {'classification_error_evaluator': 0.9453125}
# Pass 0, Batch 100, Cost 0.635356, {'classification_error_evaluator': 0.2109375}
# Pass 0, Batch 200, Cost 0.326094, {'classification_error_evaluator': 0.1328125}
# Pass 0, Batch 300, Cost 0.361920, {'classification_error_evaluator': 0.1015625}
# Pass 0, Batch 400, Cost 0.410101, {'classification_error_evaluator': 0.125}
# Test with Pass 0, Cost 0.326659, {'classification_error_evaluator': 0.09470000118017197}
</pre></div>
<p>训练之后，检查模型的预测准确度。用 MNIST 训练的时候，一般 softmax回归模型的分类准确率为约为 92.34%，多层感知器为97.66%，卷积神经网络可以达到 99.20%。</p>
<h2>应用模型</h2>
<p>可以使用训练好的模型对手写体数字图片进行分类，下面程序展示了如何使用paddle.infer接口进行推断。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="k">def</span> <span class="nf">load_image</span><span class="p">(</span><span class="nb">file</span><span class="p">):</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s1">'L'</span><span class="p">)</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">im</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">Image</span><span class="o">.</span><span class="n">ANTIALIAS</span><span class="p">)</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">im</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">im</span> <span class="o">/</span> <span class="mf">255.0</span>
    <span class="k">return</span> <span class="n">im</span>

<span class="n">test_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">cur_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span>
<span class="n">test_data</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">load_image</span><span class="p">(</span><span class="n">cur_dir</span> <span class="o">+</span> <span class="s1">'/image/infer_3.png'</span><span class="p">),))</span>

<span class="n">probs</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span>
    <span class="n">output_layer</span><span class="o">=</span><span class="n">predict</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">test_data</span><span class="p">)</span>
<span class="n">lab</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="n">probs</span><span class="p">)</span> <span class="c1"># probs and lab are the results of one batch data</span>
<span class="k">print</span> <span class="s2">"Label of image/infer_3.png is: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">lab</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
<h2>总结</h2>
<p>本教程的softmax回归、多层感知器和卷积神经网络是最基础的深度学习模型，后续章节中复杂的神经网络都是从它们衍生出来的，因此这几个模型对之后的学习大有裨益。同时，我们也观察到从最简单的softmax回归变换到稍复杂的卷积神经网络的时候，MNIST数据集上的识别准确率有了大幅度的提升，原因是卷积层具有局部连接和共享权重的特性。在之后学习新模型的时候，希望大家也要深入到新模型相比原模型带来效果提升的关键之处。此外，本教程还介绍了PaddlePaddle模型搭建的基本流程，从dataprovider的编写、网络层的构建，到最后的训练和预测。对这个流程熟悉以后，大家就可以用自己的数据，定义自己的网络模型，并完成自己的训练和预测任务了。</p>
<h2>参考文献</h2>
<ol>
<li>LeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. <a href="http://ieeexplore.ieee.org/abstract/document/726791/">"Gradient-based learning applied to document recognition."</a> Proceedings of the IEEE 86, no. 11 (1998): 2278-2324.</li>
<li>Wejéus, Samuel. <a href="http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A753279&amp;dswid=-434">"A Neural Network Approach to Arbitrary SymbolRecognition on Modern Smartphones."</a> (2014).</li>
<li>Decoste, Dennis, and Bernhard Schölkopf. <a href="http://link.springer.com/article/10.1023/A:1012454411458">"Training invariant support vector machines."</a> Machine learning 46, no. 1-3 (2002): 161-190.</li>
<li>Simard, Patrice Y., David Steinkraus, and John C. Platt. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.8494&amp;rep=rep1&amp;type=pdf">"Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis."</a> In ICDAR, vol. 3, pp. 958-962. 2003.</li>
<li>Salakhutdinov, Ruslan, and Geoffrey E. Hinton. <a href="http://www.jmlr.org/proceedings/papers/v2/salakhutdinov07a/salakhutdinov07a.pdf">"Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure."</a> In AISTATS, vol. 11. 2007.</li>
<li>Cireşan, Dan Claudiu, Ueli Meier, Luca Maria Gambardella, and Jürgen Schmidhuber. <a href="http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00052">"Deep, big, simple neural nets for handwritten digit recognition."</a> Neural computation 22, no. 12 (2010): 3207-3220.</li>
<li>Deng, Li, Michael L. Seltzer, Dong Yu, Alex Acero, Abdel-rahman Mohamed, and Geoffrey E. Hinton. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.185.1908&amp;rep=rep1&amp;type=pdf">"Binary coding of speech spectrograms using a deep auto-encoder."</a> In Interspeech, pp. 1692-1695. 2010.</li>
<li>Kégl, Balázs, and Róbert Busa-Fekete. <a href="http://dl.acm.org/citation.cfm?id=1553439">"Boosting products of base classifiers."</a> In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 497-504. ACM, 2009.</li>
<li>Rosenblatt, Frank. <a href="http://psycnet.apa.org/journals/rev/65/6/386/">"The perceptron: A probabilistic model for information storage and organization in the brain."</a> Psychological review 65, no. 6 (1958): 386.</li>
<li>Bishop, Christopher M. <a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf">"Pattern recognition."</a> Machine Learning 128 (2006): 1-58.</li>
</ol>
<p><br/>
<a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license"><img alt="知识共享许可协议" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width:0"/></a><br/><span href="http://purl.org/dc/dcmitype/Text" property="dct:title" rel="dct:type" xmlns:dct="http://purl.org/dc/terms/">本教程</span> 由 <a href="http://book.paddlepaddle.org" property="cc:attributionName" rel="cc:attributionURL" xmlns:cc="http://creativecommons.org/ns#">PaddlePaddle</a> 创作，采用 <a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license">知识共享 署名-相同方式共享 4.0 国际 许可协议</a>进行许可。</p>
{% endverbatim %}