{% verbatim %}
<h1>MNIST classification by PaddlePaddle</h1>
<p><img alt="screencast" src="https://cloud.githubusercontent.com/assets/80381/11339453/f04f885e-923c-11e5-8845-33c16978c54d.gif"/></p>
<h2>Usage</h2>
<p>This MNIST classification demo consists of two parts: a PaddlePaddle
inference server and a Javascript front end. We will start them
separately.</p>
<p>We will use Docker to run the demo, if you are not familiar with
Docker, please checkout
this
<a href="https://github.com/PaddlePaddle/Paddle/wiki/TLDR-for-new-docker-user">tutorial</a>.</p>
<h3>Start the Inference Server</h3>
<p>The inference server can be used to inference any model trained by
PaddlePaddle. Please see <a href="../serve/README.md">here</a> for more details.</p>
<ol>
<li>
<p>Download the MNIST inference model topylogy and parameters to the
   current working directory.</p>
<div class="highlight"><pre><span></span>wget https://s3.us-east-2.amazonaws.com/models.paddlepaddle/end-to-end-mnist/inference_topology.pkl
wget https://s3.us-east-2.amazonaws.com/models.paddlepaddle/end-to-end-mnist/param.tar
</pre></div>
</li>
<li>
<p>Run following command to start the inference server:</p>
<div class="highlight"><pre><span></span>docker run --name paddle_serve -v <span class="sb">`</span><span class="nb">pwd</span><span class="sb">`</span>:/data -d -p <span class="m">8000</span>:80 -e <span class="nv">WITH_GPU</span><span class="o">=</span><span class="m">0</span> paddlepaddle/book:serve
</pre></div>
<p>The above command will mount the current working directory to the
<code>/data</code> directory inside the docker container. The inference
server will load the model topology and parameters that we just
downloaded from there.</p>
<p>After you are done with the demo, you can run <code>docker stop
paddle_serve</code> to stop this docker container.</p>
</li>
</ol>
<h3>Start the Front End</h3>
<ol>
<li>Run the following command
   <div class="highlight"><pre><span></span>docker run -it -p <span class="m">5000</span>:5000 -e <span class="nv">BACKEND_URL</span><span class="o">=</span>http://localhost:8000/ paddlepaddle/book:mnist
</pre></div></li>
</ol>
<p><code>BACKEND_URL</code> in the above command specifies the inference server
   endpoint. If you started the inference server on another machine,
   or want to visit the front end remotely, you may want to change its
   value.</p>
<ol>
<li>Visit http://localhost:5000 and you will see the PaddlePaddle MNIST demo.</li>
</ol>
<h2>Build</h2>
<p>We have already prepared the pre-built docker image
<code>paddlepaddle/book:mnist</code>, here is the command if you want to build
the docker image again.</p>
<div class="highlight"><pre><span></span>docker build -t paddlepaddle/book:mnist .
</pre></div>
<h2>Acknowledgement</h2>
<p>Thanks to the great project https://github.com/sugyan/tensorflow-mnist
. Most of the code in this project comes from there.</p>
{% endverbatim %}