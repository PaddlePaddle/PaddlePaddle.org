{% verbatim %}
<h1>词向量</h1>
<p>本教程源代码目录在<a href="https://github.com/PaddlePaddle/book/tree/develop/04.word2vec">book/word2vec</a>， 初次使用请参考PaddlePaddle<a href="https://github.com/PaddlePaddle/book/blob/develop/README.cn.md#运行这本书">安装教程</a>，更多内容请参考本教程的<a href="http://bit.baidu.com/course/detail/id/175.html">视频课堂</a>。</p>
<h2>背景介绍</h2>
<p>本章我们介绍词的向量表征，也称为word embedding。词向量是自然语言处理中常见的一个操作，是搜索引擎、广告系统、推荐系统等互联网服务背后常见的基础技术。</p>
<p>在这些互联网服务里，我们经常要比较两个词或者两段文本之间的相关性。为了做这样的比较，我们往往先要把词表示成计算机适合处理的方式。最自然的方式恐怕莫过于向量空间模型(vector space model)。
在这种方式里，每个词被表示成一个实数向量（one-hot vector），其长度为字典大小，每个维度对应一个字典里的每个词，除了这个词对应维度上的值是1，其他元素都是0。</p>
<p>One-hot vector虽然自然，但是用处有限。比如，在互联网广告系统里，如果用户输入的query是“母亲节”，而有一个广告的关键词是“康乃馨”。虽然按照常理，我们知道这两个词之间是有联系的——母亲节通常应该送给母亲一束康乃馨；但是这两个词对应的one-hot vectors之间的距离度量，无论是欧氏距离还是余弦相似度(cosine similarity)，由于其向量正交，都认为这两个词毫无相关性。 得出这种与我们相悖的结论的根本原因是：每个词本身的信息量都太小。所以，仅仅给定两个词，不足以让我们准确判别它们是否相关。要想精确计算相关性，我们还需要更多的信息——从大量数据里通过机器学习方法归纳出来的知识。</p>
<p>在机器学习领域里，各种“知识”被各种模型表示，词向量模型(word embedding model)就是其中的一类。通过词向量模型可将一个 one-hot vector映射到一个维度更低的实数向量（embedding vector），如<span class="markdown-equation" id="equation-0">$embedding(母亲节) = [0.3, 4.2, -1.5, ...], embedding(康乃馨) = [0.2, 5.6, -2.3, ...]$</span>。在这个映射到的实数向量表示中，希望两个语义（或用法）上相似的词对应的词向量“更像”，这样如“母亲节”和“康乃馨”的对应词向量的余弦相似度就不再为零了。</p>
<p>词向量模型可以是概率模型、共生矩阵(co-occurrence matrix)模型或神经元网络模型。在用神经网络求词向量之前，传统做法是统计一个词语的共生矩阵<span class="markdown-equation" id="equation-1">$X$</span>。<span class="markdown-equation" id="equation-1">$X$</span>是一个<span class="markdown-equation" id="equation-3">$|V| \times |V|$</span> 大小的矩阵，<span class="markdown-equation" id="equation-4">$X_{ij}$</span>表示在所有语料中，词汇表<code>V</code>(vocabulary)中第i个词和第j个词同时出现的词数，<span class="markdown-equation" id="equation-5">$|V|$</span>为词汇表的大小。对<span class="markdown-equation" id="equation-1">$X$</span>做矩阵分解（如奇异值分解，Singular Value Decomposition [<a href="#参考文献">5</a>]），得到的<span class="markdown-equation" id="equation-7">$U$</span>即视为所有词的词向量：</p>
<p><span class="markdown-equation" id="equation-8">$$X = USV^T$$</span></p>
<p>但这样的传统做法有很多问题：<br/>
1) 由于很多词没有出现，导致矩阵极其稀疏，因此需要对词频做额外处理来达到好的矩阵分解效果；<br/>
2) 矩阵非常大，维度太高(通常达到<span class="markdown-equation" id="equation-9">$10^6*10^6$</span>的数量级)；<br/>
3) 需要手动去掉停用词（如although, a,...），不然这些频繁出现的词也会影响矩阵分解的效果。</p>
<p>基于神经网络的模型不需要计算存储一个在全语料上统计的大表，而是通过学习语义信息得到词向量，因此能很好地解决以上问题。在本章里，我们将展示基于神经网络训练词向量的细节，以及如何用PaddlePaddle训练一个词向量模型。</p>
<h2>效果展示</h2>
<p>本章中，当词向量训练好后，我们可以用数据可视化算法t-SNE[<a href="#参考文献">4</a>]画出词语特征在二维上的投影（如下图所示）。从图中可以看出，语义相关的词语（如a, the, these; big, huge）在投影上距离很近，语意无关的词（如say, business; decision, japan）在投影上的距离很远。</p>
<p align="center">
<img src="image/2d_similarity.png" width="400"/><br/>
    图1. 词向量的二维投影
</p>
<p>另一方面，我们知道两个向量的余弦值在<span class="markdown-equation" id="equation-10">$[-1,1]$</span>的区间内：两个完全相同的向量余弦值为1, 两个相互垂直的向量之间余弦值为0，两个方向完全相反的向量余弦值为-1，即相关性和余弦值大小成正比。因此我们还可以计算两个词向量的余弦相似度:</p>
<div class="highlight"><pre><span></span>similarity: 0.899180685161
please input two words: big huge

please input two words: from company
similarity: -0.0997506977351
</pre></div>
<p>以上结果可以通过运行<code>calculate_dis.py</code>, 加载字典里的单词和对应训练特征结果得到，我们将在<a href="#应用模型">应用模型</a>中详细描述用法。</p>
<h2>模型概览</h2>
<p>在这里我们介绍三个训练词向量的模型：N-gram模型，CBOW模型和Skip-gram模型，它们的中心思想都是通过上下文得到一个词出现的概率。对于N-gram模型，我们会先介绍语言模型的概念，并在之后的<a href="#训练模型">训练模型</a>中，带大家用PaddlePaddle实现它。而后两个模型，是近年来最有名的神经元词向量模型，由 Tomas Mikolov 在Google 研发[<a href="#参考文献">3</a>]，虽然它们很浅很简单，但训练效果很好。</p>
<h3>语言模型</h3>
<p>在介绍词向量模型之前，我们先来引入一个概念：语言模型。
语言模型旨在为语句的联合概率函数<span class="markdown-equation" id="equation-11">$P(w_1, ..., w_T)$</span>建模, 其中<span class="markdown-equation" id="equation-12">$w_i$</span>表示句子中的第i个词。语言模型的目标是，希望模型对有意义的句子赋予大概率，对没意义的句子赋予小概率。
这样的模型可以应用于很多领域，如机器翻译、语音识别、信息检索、词性标注、手写识别等，它们都希望能得到一个连续序列的概率。 以信息检索为例，当你在搜索“how long is a football bame”时（bame是一个医学名词），搜索引擎会提示你是否希望搜索"how long is a football game", 这是因为根据语言模型计算出“how long is a football bame”的概率很低，而与bame近似的，可能引起错误的词中，game会使该句生成的概率最大。</p>
<p>对语言模型的目标概率<span class="markdown-equation" id="equation-11">$P(w_1, ..., w_T)$</span>，如果假设文本中每个词都是相互独立的，则整句话的联合概率可以表示为其中所有词语条件概率的乘积，即：</p>
<p><span class="markdown-equation" id="equation-14">$$P(w_1, ..., w_T) = \prod_{t=1}^TP(w_t)$$</span></p>
<p>然而我们知道语句中的每个词出现的概率都与其前面的词紧密相关, 所以实际上通常用条件概率表示语言模型：</p>
<p><span class="markdown-equation" id="equation-15">$$P(w_1, ..., w_T) = \prod_{t=1}^TP(w_t | w_1, ... , w_{t-1})$$</span></p>
<h3>N-gram neural model</h3>
<p>在计算语言学中，n-gram是一种重要的文本表示方法，表示一个文本中连续的n个项。基于具体的应用场景，每一项可以是一个字母、单词或者音节。 n-gram模型也是统计语言模型中的一种重要方法，用n-gram训练语言模型时，一般用每个n-gram的历史n-1个词语组成的内容来预测第n个词。</p>
<p>Yoshua Bengio等科学家就于2003年在著名论文 Neural Probabilistic Language Models [<a href="#参考文献">1</a>] 中介绍如何学习一个神经元网络表示的词向量模型。文中的神经概率语言模型（Neural Network Language Model，NNLM）通过一个线性映射和一个非线性隐层连接，同时学习了语言模型和词向量，即通过学习大量语料得到词语的向量表达，通过这些向量得到整个句子的概率。用这种方法学习语言模型可以克服维度灾难（curse of dimensionality）,即训练和测试数据不同导致的模型不准。注意：由于“神经概率语言模型”说法较为泛泛，我们在这里不用其NNLM的本名，考虑到其具体做法，本文中称该模型为N-gram neural model。</p>
<p>我们在上文中已经讲到用条件概率建模语言模型，即一句话中第<span class="markdown-equation" id="equation-16">$t$</span>个词的概率和该句话的前<span class="markdown-equation" id="equation-17">$t-1$</span>个词相关。可实际上越远的词语其实对该词的影响越小，那么如果考虑一个n-gram, 每个词都只受其前面<code>n-1</code>个词的影响，则有：</p>
<p><span class="markdown-equation" id="equation-18">$$P(w_1, ..., w_T) = \prod_{t=n}^TP(w_t|w_{t-1}, w_{t-2}, ..., w_{t-n+1})$$</span></p>
<p>给定一些真实语料，这些语料中都是有意义的句子，N-gram模型的优化目标则是最大化目标函数:</p>
<p><span class="markdown-equation" id="equation-19">$$\frac{1}{T}\sum_t f(w_t, w_{t-1}, ..., w_{t-n+1};\theta) + R(\theta)$$</span></p>
<p>其中<span class="markdown-equation" id="equation-20">$f(w_t, w_{t-1}, ..., w_{t-n+1})$</span>表示根据历史n-1个词得到当前词<span class="markdown-equation" id="equation-21">$w_t$</span>的条件概率，<span class="markdown-equation" id="equation-22">$R(\theta)$</span>表示参数正则项。</p>
<p align="center">
<img src="image/nnlm.png" width="500"/><br/>
       图2. N-gram神经网络模型
</p>
<p>图2展示了N-gram神经网络模型，从下往上看，该模型分为以下几个部分：
 - 对于每个样本，模型输入<span class="markdown-equation" id="equation-23">$w_{t-n+1},...w_{t-1}$</span>, 输出句子第t个词为字典中<code>|V|</code>个词的概率。</p>
<p>每个输入词<span class="markdown-equation" id="equation-23">$w_{t-n+1},...w_{t-1}$</span>首先通过映射矩阵映射到词向量<span class="markdown-equation" id="equation-25">$C(w_{t-n+1}),...C(w_{t-1})$</span>。</p>
<ul>
<li>
<p>然后所有词语的词向量连接成一个大向量，并经过一个非线性映射得到历史词语的隐层表示：</p>
<p><span class="markdown-equation" id="equation-26">$$g=Utanh(\theta^Tx + b_1) + Wx + b_2$$</span></p>
<p>其中，<span class="markdown-equation" id="equation-27">$x$</span>为所有词语的词向量连接成的大向量，表示文本历史特征；<span class="markdown-equation" id="equation-28">$\theta$</span>、<span class="markdown-equation" id="equation-7">$U$</span>、<span class="markdown-equation" id="equation-30">$b_1$</span>、<span class="markdown-equation" id="equation-31">$b_2$</span>和<span class="markdown-equation" id="equation-32">$W$</span>分别为词向量层到隐层连接的参数。<span class="markdown-equation" id="equation-33">$g$</span>表示未经归一化的所有输出单词概率，<span class="markdown-equation" id="equation-34">$g_i$</span>表示未经归一化的字典中第<span class="markdown-equation" id="equation-35">$i$</span>个单词的输出概率。</p>
</li>
<li>
<p>根据softmax的定义，通过归一化<span class="markdown-equation" id="equation-34">$g_i$</span>, 生成目标词<span class="markdown-equation" id="equation-21">$w_t$</span>的概率为：</p>
</li>
</ul>
<p><span class="markdown-equation" id="equation-38">$$P(w_t | w_1, ..., w_{t-n+1}) = \frac{e^{g_{w_t}}}{\sum_i^{|V|} e^{g_i}}$$</span></p>
<ul>
<li>整个网络的损失值(cost)为多类分类交叉熵，用公式表示为</li>
</ul>
<p><span class="markdown-equation" id="equation-39">$$J(\theta) = -\sum_{i=1}^N\sum_{c=1}^{|V|}y_k^{i}log(softmax(g_k^i))$$</span></p>
<p>其中<span class="markdown-equation" id="equation-40">$y_k^i$</span>表示第<span class="markdown-equation" id="equation-35">$i$</span>个样本第<span class="markdown-equation" id="equation-42">$k$</span>类的真实标签(0或1)，<span class="markdown-equation" id="equation-43">$softmax(g_k^i)$</span>表示第i个样本第k类softmax输出的概率。</p>
<h3>Continuous Bag-of-Words model(CBOW)</h3>
<p>CBOW模型通过一个词的上下文（各N个词）预测当前词。当N=2时，模型如下图所示：</p>
<p align="center">
<img src="image/cbow.png" width="250"/><br/>
    图3. CBOW模型
</p>
<p>具体来说，不考虑上下文的词语输入顺序，CBOW是用上下文词语的词向量的均值来预测当前词。即：</p>
<p><span class="markdown-equation" id="equation-44">$$context = \frac{x_{t-1} + x_{t-2} + x_{t+1} + x_{t+2}}{4}$$</span></p>
<p>其中<span class="markdown-equation" id="equation-45">$x_t$</span>为第<span class="markdown-equation" id="equation-16">$t$</span>个词的词向量，分类分数（score）向量 <span class="markdown-equation" id="equation-47">$z=U*context$</span>，最终的分类<span class="markdown-equation" id="equation-48">$y$</span>采用softmax，损失函数采用多类分类交叉熵。</p>
<h3>Skip-gram model</h3>
<p>CBOW的好处是对上下文词语的分布在词向量上进行了平滑，去掉了噪声，因此在小数据集上很有效。而Skip-gram的方法中，用一个词预测其上下文，得到了当前词上下文的很多样本，因此可用于更大的数据集。</p>
<p align="center">
<img src="image/skipgram.png" width="250"/><br/>
    图4. Skip-gram模型
</p>
<p>如上图所示，Skip-gram模型的具体做法是，将一个词的词向量映射到<span class="markdown-equation" id="equation-49">$2n$</span>个词的词向量（<span class="markdown-equation" id="equation-49">$2n$</span>表示当前输入词的前后各<span class="markdown-equation" id="equation-51">$n$</span>个词），然后分别通过softmax得到这<span class="markdown-equation" id="equation-49">$2n$</span>个词的分类损失值之和。</p>
<h2>数据准备</h2>
<h3>数据介绍</h3>
<p>本教程使用Penn Treebank （PTB）（经Tomas Mikolov预处理过的版本）数据集。PTB数据集较小，训练速度快，应用于Mikolov的公开语言模型训练工具[<a href="#参考文献">2</a>]中。其统计情况如下：</p>
<p align="center">
</p><table>
<tr>
<td>训练数据</td>
<td>验证数据</td>
<td>测试数据</td>
</tr>
<tr>
<td>ptb.train.txt</td>
<td>ptb.valid.txt</td>
<td>ptb.test.txt</td>
</tr>
<tr>
<td>42068句</td>
<td>3370句</td>
<td>3761句</td>
</tr>
</table>
<h3>数据预处理</h3>
<p>本章训练的是5-gram模型，表示在PaddlePaddle训练时，每条数据的前4个词用来预测第5个词。PaddlePaddle提供了对应PTB数据集的python包<code>paddle.dataset.imikolov</code>，自动做数据的下载与预处理，方便大家使用。</p>
<p>预处理会把数据集中的每一句话前后加上开始符号<code>&lt;s&gt;</code>以及结束符号<code>&lt;e&gt;</code>。然后依据窗口大小（本教程中为5），从头到尾每次向右滑动窗口并生成一条数据。</p>
<p>如"I have a dream that one day" 一句提供了5条数据：</p>
<div class="highlight"><pre><span></span>&lt;s&gt; I have a dream
I have a dream that
have a dream that one
a dream that one day
dream that one day &lt;e&gt;
</pre></div>
<p>最后，每个输入会按其单词次在字典里的位置，转化成整数的索引序列，作为PaddlePaddle的输入。</p>
<h2>编程实现</h2>
<p>本配置的模型结构如下图所示：</p>
<p align="center">
<img src="image/ngram.png" width="400"/><br/>
    图5. 模型配置中的N-gram神经网络模型
</p>
<p>首先，加载所需要的包：</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">paddle.v2</span> <span class="kn">as</span> <span class="nn">paddle</span>
</pre></div>
<p>然后，定义参数：
</p><div class="highlight"><pre><span></span><span class="n">embsize</span> <span class="o">=</span> <span class="mi">32</span> <span class="c1"># 词向量维度</span>
<span class="n">hiddensize</span> <span class="o">=</span> <span class="mi">256</span> <span class="c1"># 隐层维度</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># 训练5-Gram</span>
</pre></div>
<p>用于保存和加载word_dict和embedding table的函数
</p><div class="highlight"><pre><span></span><span class="c1"># save and load word dict and embedding table</span>
<span class="k">def</span> <span class="nf">save_dict_and_embedding</span><span class="p">(</span><span class="n">word_dict</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"word_dict"</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">word_dict</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">key</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">word_dict</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"embedding_table"</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">','</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">load_dict_and_embedding</span><span class="p">():</span>
    <span class="n">word_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"word_dict"</span><span class="p">,</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">" "</span><span class="p">)</span>
            <span class="n">word_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">"embedding_table"</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">","</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">word_dict</span><span class="p">,</span> <span class="n">embeddings</span>
</pre></div>
<p>接着，定义网络结构：</p>
<ul>
<li>将<span class="markdown-equation" id="equation-21">$w_t$</span>之前的<span class="markdown-equation" id="equation-54">$n-1$</span>个词 <span class="markdown-equation" id="equation-23">$w_{t-n+1},...w_{t-1}$</span>，通过<span class="markdown-equation" id="equation-56">$|V|\times D$</span>的矩阵映射到D维词向量（本例中取D=32）。</li>
</ul>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">wordemb</span><span class="p">(</span><span class="n">inlayer</span><span class="p">):</span>
    <span class="n">wordemb</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">table_projection</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">inlayer</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="n">embsize</span><span class="p">,</span>
        <span class="n">param_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"_proj"</span><span class="p">,</span>
            <span class="n">initial_std</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">l2_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">sparse_update</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">wordemb</span>
</pre></div>
<ul>
<li>定义输入层接受的数据类型以及名字。</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">paddle</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">trainer_count</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># 初始化PaddlePaddle</span>
<span class="n">word_dict</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">imikolov</span><span class="o">.</span><span class="n">build_dict</span><span class="p">()</span>
<span class="n">dict_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_dict</span><span class="p">)</span>
<span class="c1"># 每个输入层都接受整形数据，这些数据的范围是[0, dict_size)</span>
<span class="n">firstword</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">"firstw"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">data_type</span><span class="o">.</span><span class="n">integer_value</span><span class="p">(</span><span class="n">dict_size</span><span class="p">))</span>
<span class="n">secondword</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">"secondw"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">data_type</span><span class="o">.</span><span class="n">integer_value</span><span class="p">(</span><span class="n">dict_size</span><span class="p">))</span>
<span class="n">thirdword</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">"thirdw"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">data_type</span><span class="o">.</span><span class="n">integer_value</span><span class="p">(</span><span class="n">dict_size</span><span class="p">))</span>
<span class="n">fourthword</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">"fourthw"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">data_type</span><span class="o">.</span><span class="n">integer_value</span><span class="p">(</span><span class="n">dict_size</span><span class="p">))</span>
<span class="n">nextword</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">"fifthw"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">data_type</span><span class="o">.</span><span class="n">integer_value</span><span class="p">(</span><span class="n">dict_size</span><span class="p">))</span>

<span class="n">Efirst</span> <span class="o">=</span> <span class="n">wordemb</span><span class="p">(</span><span class="n">firstword</span><span class="p">)</span>
<span class="n">Esecond</span> <span class="o">=</span> <span class="n">wordemb</span><span class="p">(</span><span class="n">secondword</span><span class="p">)</span>
<span class="n">Ethird</span> <span class="o">=</span> <span class="n">wordemb</span><span class="p">(</span><span class="n">thirdword</span><span class="p">)</span>
<span class="n">Efourth</span> <span class="o">=</span> <span class="n">wordemb</span><span class="p">(</span><span class="n">fourthword</span><span class="p">)</span>
</pre></div>
<ul>
<li>将这n-1个词向量经过concat_layer连接成一个大向量作为历史文本特征。</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">contextemb</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">Efirst</span><span class="p">,</span> <span class="n">Esecond</span><span class="p">,</span> <span class="n">Ethird</span><span class="p">,</span> <span class="n">Efourth</span><span class="p">])</span>
</pre></div>
<ul>
<li>将历史文本特征经过一个全连接得到文本隐层特征。</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">hidden1</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">contextemb</span><span class="p">,</span>
                          <span class="n">size</span><span class="o">=</span><span class="n">hiddensize</span><span class="p">,</span>
                          <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
                          <span class="n">layer_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Extra</span><span class="p">(</span><span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
                          <span class="n">bias_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
                          <span class="n">param_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span>
                                <span class="n">initial_std</span><span class="o">=</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">embsize</span> <span class="o">*</span> <span class="mi">8</span><span class="p">),</span>
                                <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
<ul>
<li>将文本隐层特征，再经过一个全连接，映射成一个<span class="markdown-equation" id="equation-5">$|V|$</span>维向量，同时通过softmax归一化得到这<code>|V|</code>个词的生成概率。</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">predictword</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">hidden1</span><span class="p">,</span>
                              <span class="n">size</span><span class="o">=</span><span class="n">dict_size</span><span class="p">,</span>
                              <span class="n">bias_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
                              <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Softmax</span><span class="p">())</span>
</pre></div>
<ul>
<li>网络的损失函数为多分类交叉熵，可直接调用<code>classification_cost</code>函数。</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">cost</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">classification_cost</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">predictword</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">nextword</span><span class="p">)</span>
</pre></div>
<p>然后，指定训练相关的参数：</p>
<ul>
<li>训练方法（optimizer)： 代表训练过程在更新权重时采用动量优化器，本教程使用Adam优化器。</li>
<li>训练速度（learning_rate）： 迭代的速度，与网络的训练收敛速度有关系。</li>
<li>正则化（regularization）： 是防止网络过拟合的一种手段，此处采用L2正则化。</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
<span class="n">adagrad</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">AdaGrad</span><span class="p">(</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">3e-3</span><span class="p">,</span>
    <span class="n">regularization</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">L2Regularization</span><span class="p">(</span><span class="mf">8e-4</span><span class="p">))</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">adagrad</span><span class="p">)</span>
</pre></div>
<p>下一步，我们开始训练过程。<code>paddle.dataset.imikolov.train()</code>和<code>paddle.dataset.imikolov.test()</code>分别做训练和测试数据集。这两个函数各自返回一个reader——PaddlePaddle中的reader是一个Python函数，每次调用的时候返回一个Python generator。</p>
<p><code>paddle.batch</code>的输入是一个reader，输出是一个batched reader —— 在PaddlePaddle里，一个reader每次yield一条训练数据，而一个batched reader每次yield一个minbatch。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">event_handler</span><span class="p">(</span><span class="n">event</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">paddle</span><span class="o">.</span><span class="n">event</span><span class="o">.</span><span class="n">EndIteration</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">event</span><span class="o">.</span><span class="n">batch_id</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="s2">"Pass </span><span class="si">%d</span><span class="s2">, Batch </span><span class="si">%d</span><span class="s2">, Cost </span><span class="si">%f</span><span class="s2">, </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span>
                <span class="n">event</span><span class="o">.</span><span class="n">pass_id</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">batch_id</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">cost</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">paddle</span><span class="o">.</span><span class="n">event</span><span class="o">.</span><span class="n">EndPass</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span>
                    <span class="n">paddle</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span>
                        <span class="n">paddle</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">imikolov</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">word_dict</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="mi">32</span><span class="p">))</span>
        <span class="k">print</span> <span class="s2">"Pass </span><span class="si">%d</span><span class="s2">, Testing metrics </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">event</span><span class="o">.</span><span class="n">pass_id</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"model_</span><span class="si">%d</span><span class="s2">.tar"</span><span class="o">%</span><span class="n">event</span><span class="o">.</span><span class="n">pass_id</span><span class="p">,</span> <span class="s1">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">save_parameter_to_tar</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">paddle</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">paddle</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">imikolov</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">word_dict</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="mi">32</span><span class="p">),</span>
    <span class="n">num_passes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">event_handler</span><span class="o">=</span><span class="n">event_handler</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span>Pass 0, Batch 0, Cost 7.870579, {'classification_error_evaluator': 1.0}, Testing metrics {'classification_error_evaluator': 0.999591588973999}
Pass 0, Batch 100, Cost 6.136420, {'classification_error_evaluator': 0.84375}, Testing metrics {'classification_error_evaluator': 0.8328699469566345}
Pass 0, Batch 200, Cost 5.786797, {'classification_error_evaluator': 0.8125}, Testing metrics {'classification_error_evaluator': 0.8328542709350586}
...
</pre></div>
<p>训练过程是完全自动的，event_handler里打印的日志类似如上所示：</p>
<p>经过30个pass，我们将得到平均错误率为classification_error_evaluator=0.735611。</p>
<h2>保存词典和embedding</h2>
<p>训练完成之后，我们可以把词典和embedding table单独保存下来，后面可以直接使用</p>
<div class="highlight"><pre><span></span><span class="c1"># save word dict and embedding table</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">parameters</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"_proj"</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_dict</span><span class="p">),</span> <span class="n">embsize</span><span class="p">)</span>
<span class="n">save_dict_and_embedding</span><span class="p">(</span><span class="n">word_dict</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</pre></div>
<h2>应用模型</h2>
<p>训练模型后，我们可以加载模型参数，用训练出来的词向量初始化其他模型，也可以将模型查看参数用来做后续应用。</p>
<h3>查看词向量</h3>
<p>PaddlePaddle训练出来的参数可以直接使用<code>parameters.get()</code>获取出来。例如查看单词<code>apple</code>的词向量，即为</p>
<div class="highlight"><pre><span></span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">parameters</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"_proj"</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_dict</span><span class="p">),</span> <span class="n">embsize</span><span class="p">)</span>

<span class="k">print</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">word_dict</span><span class="p">[</span><span class="s1">'apple'</span><span class="p">]]</span>
</pre></div>
<div class="highlight"><pre><span></span>[-0.38961065 -0.02392169 -0.00093231  0.36301503  0.13538605  0.16076435
-0.0678709   0.1090285   0.42014077 -0.24119169 -0.31847557  0.20410083
0.04910378  0.19021918 -0.0122014  -0.04099389 -0.16924137  0.1911236
-0.10917275  0.13068172 -0.23079982  0.42699069 -0.27679482 -0.01472992
0.2069038   0.09005053 -0.3282454   0.12717034 -0.24218646  0.25304323
0.19072419 -0.24286366]
</pre></div>
<h3>修改词向量</h3>
<p>获得到的embedding为一个标准的numpy矩阵。我们可以对这个numpy矩阵进行修改，然后赋值回去。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">modify_embedding</span><span class="p">(</span><span class="n">emb</span><span class="p">):</span>
    <span class="c1"># Add your modification here.</span>
    <span class="k">pass</span>

<span class="n">modify_embedding</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
<span class="n">parameters</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">"_proj"</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</pre></div>
<h3>计算词语之间的余弦距离</h3>
<p>两个向量之间的距离可以用余弦值来表示，余弦值在<span class="markdown-equation" id="equation-10">$[-1,1]$</span>的区间内，向量间余弦值越大，其距离越近。这里我们在<code>calculate_dis.py</code>中实现不同词语的距离度量。
用法如下：</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">spatial</span>

<span class="n">emb_1</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">word_dict</span><span class="p">[</span><span class="s1">'world'</span><span class="p">]]</span>
<span class="n">emb_2</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">word_dict</span><span class="p">[</span><span class="s1">'would'</span><span class="p">]]</span>

<span class="k">print</span> <span class="n">spatial</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">cosine</span><span class="p">(</span><span class="n">emb_1</span><span class="p">,</span> <span class="n">emb_2</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span>0.99375076448
</pre></div>
<h2>总结</h2>
<p>本章中，我们介绍了词向量、语言模型和词向量的关系、以及如何通过训练神经网络模型获得词向量。在信息检索中，我们可以根据向量间的余弦夹角，来判断query和文档关键词这二者间的相关性。在句法分析和语义分析中，训练好的词向量可以用来初始化模型，以得到更好的效果。在文档分类中，有了词向量之后，可以用聚类的方法将文档中同义词进行分组。希望大家在本章后能够自行运用词向量进行相关领域的研究。</p>
<h2>参考文献</h2>
<ol>
<li>Bengio Y, Ducharme R, Vincent P, et al. <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A neural probabilistic language model</a>[J]. journal of machine learning research, 2003, 3(Feb): 1137-1155.</li>
<li>Mikolov T, Kombrink S, Deoras A, et al. <a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/rnnlm-demo.pdf">Rnnlm-recurrent neural network language modeling toolkit</a>[C]//Proc. of the 2011 ASRU Workshop. 2011: 196-201.</li>
<li>Mikolov T, Chen K, Corrado G, et al. <a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient estimation of word representations in vector space</a>[J]. arXiv preprint arXiv:1301.3781, 2013.</li>
<li>Maaten L, Hinton G. <a href="https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf">Visualizing data using t-SNE</a>[J]. Journal of Machine Learning Research, 2008, 9(Nov): 2579-2605.</li>
<li>https://en.wikipedia.org/wiki/Singular_value_decomposition</li>
</ol>
<p><br/>
<a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license"><img alt="知识共享许可协议" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width:0"/></a><br/><span href="http://purl.org/dc/dcmitype/Text" property="dct:title" rel="dct:type" xmlns:dct="http://purl.org/dc/terms/">本教程</span> 由 <a href="http://book.paddlepaddle.org" property="cc:attributionName" rel="cc:attributionURL" xmlns:cc="http://creativecommons.org/ns#">PaddlePaddle</a> 创作，采用 <a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license">知识共享 署名-相同方式共享 4.0 国际 许可协议</a>进行许可。</p>
{% endverbatim %}