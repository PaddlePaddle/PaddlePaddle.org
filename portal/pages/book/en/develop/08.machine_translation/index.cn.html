{% verbatim %}
<h1>机器翻译</h1>
<p>本教程源代码目录在<a href="https://github.com/PaddlePaddle/book/tree/develop/08.machine_translation">book/machine_translation</a>， 初次使用请参考PaddlePaddle<a href="https://github.com/PaddlePaddle/book/blob/develop/README.cn.md#运行这本书">安装教程</a>，更多内容请参考本教程的<a href="http://bit.baidu.com/course/detail/id/179.html">视频课堂</a>。</p>
<h2>背景介绍</h2>
<p>机器翻译（machine translation, MT）是用计算机来实现不同语言之间翻译的技术。被翻译的语言通常称为源语言（source language），翻译成的结果语言称为目标语言（target language）。机器翻译即实现从源语言到目标语言转换的过程，是自然语言处理的重要研究领域之一。</p>
<p>早期机器翻译系统多为基于规则的翻译系统，需要由语言学家编写两种语言之间的转换规则，再将这些规则录入计算机。该方法对语言学家的要求非常高，而且我们几乎无法总结一门语言会用到的所有规则，更何况两种甚至更多的语言。因此，传统机器翻译方法面临的主要挑战是无法得到一个完备的规则集合[<a href="#参考文献">1</a>]。</p>
<p>为解决以上问题，统计机器翻译（Statistical Machine Translation, SMT）技术应运而生。在统计机器翻译技术中，转化规则是由机器自动从大规模的语料中学习得到的，而非我们人主动提供规则。因此，它克服了基于规则的翻译系统所面临的知识获取瓶颈的问题，但仍然存在许多挑战：1）人为设计许多特征（feature），但永远无法覆盖所有的语言现象；2）难以利用全局的特征；3）依赖于许多预处理环节，如词语对齐、分词或符号化（tokenization）、规则抽取、句法分析等，而每个环节的错误会逐步累积，对翻译的影响也越来越大。</p>
<p>近年来，深度学习技术的发展为解决上述挑战提供了新的思路。将深度学习应用于机器翻译任务的方法大致分为两类：1）仍以统计机器翻译系统为框架，只是利用神经网络来改进其中的关键模块，如语言模型、调序模型等（见图1的左半部分）；2）不再以统计机器翻译系统为框架，而是直接用神经网络将源语言映射到目标语言，即端到端的神经网络机器翻译（End-to-End Neural Machine Translation, End-to-End NMT）（见图1的右半部分），简称为NMT模型。
</p><p align="center">
<img src="image/nmt.png" width="400"/><br/>
图1. 基于神经网络的机器翻译系统
</p>
<p>本教程主要介绍NMT模型，以及如何用PaddlePaddle来训练一个NMT模型。</p>
<h2>效果展示</h2>
<p>以中英翻译（中文翻译到英文）的模型为例，当模型训练完毕时，如果输入如下已分词的中文句子：
</p><div class="highlight"><pre><span></span>这些 是 希望 的 曙光 和 解脱 的 迹象 .
</pre></div>
如果设定显示翻译结果的条数（即<a href="#柱搜索算法">柱搜索算法</a>的宽度）为3，生成的英语句子如下：
<div class="highlight"><pre><span></span>0 -5.36816   These are signs of hope and relief . &lt;e&gt;
1 -6.23177   These are the light of hope and relief . &lt;e&gt;
2 -7.7914  These are the light of hope and the relief of hope . &lt;e&gt;
</pre></div>
- 左起第一列是生成句子的序号；左起第二列是该条句子的得分（从大到小），分值越高越好；左起第三列是生成的英语句子。
- 另外有两个特殊标志：<code>&lt;e&gt;</code>表示句子的结尾，<code>&lt;unk&gt;</code>表示未登录词（unknown word），即未在训练字典中出现的词。
<h2>模型概览</h2>
<p>本节依次介绍GRU（Gated Recurrent Unit，门控循环单元），双向循环神经网络（Bi-directional Recurrent Neural Network），NMT模型中典型的编码器-解码器（Encoder-Decoder）框架和注意力（Attention）机制，以及柱搜索（beam search）算法。</p>
<h3>GRU</h3>
<p>我们已经在<a href="https://github.com/PaddlePaddle/book/blob/develop/06.understand_sentiment/README.cn.md">情感分析</a>一章中介绍了循环神经网络（RNN）及长短时间记忆网络（LSTM）。相比于简单的RNN，LSTM增加了记忆单元（memory cell）、输入门（input gate）、遗忘门（forget gate）及输出门（output gate），这些门及记忆单元组合起来大大提升了RNN处理远距离依赖问题的能力。</p>
<p>GRU[<a href="#参考文献">2</a>]是Cho等人在LSTM上提出的简化版本，也是RNN的一种扩展，如下图所示。GRU单元只有两个门：
- 重置门（reset gate）：如果重置门关闭，会忽略掉历史信息，即历史不相干的信息不会影响未来的输出。
- 更新门（update gate）：将LSTM的输入门和遗忘门合并，用于控制历史信息对当前时刻隐层输出的影响。如果更新门接近1，会把历史信息传递下去。
</p><p align="center">
<img src="image/gru.png" width="700"/><br/>
图2. GRU（门控循环单元）
</p>
<p>一般来说，具有短距离依赖属性的序列，其重置门比较活跃；相反，具有长距离依赖属性的序列，其更新门比较活跃。另外，Chung等人[<a href="#参考文献">3</a>]通过多组实验表明，GRU虽然参数更少，但是在多个任务上都和LSTM有相近的表现。</p>
<h3>双向循环神经网络</h3>
<p>我们已经在<a href="https://github.com/PaddlePaddle/book/blob/develop/07.label_semantic_roles/README.cn.md">语义角色标注</a>一章中介绍了一种双向循环神经网络，这里介绍Bengio团队在论文[<a href="#参考文献">2</a>,<a href="#参考文献">4</a>]中提出的另一种结构。该结构的目的是输入一个序列，得到其在每个时刻的特征表示，即输出的每个时刻都用定长向量表示到该时刻的上下文语义信息。</p>
<p>具体来说，该双向循环神经网络分别在时间维以顺序和逆序——即前向（forward）和后向（backward）——依次处理输入序列，并将每个时间步RNN的输出拼接成为最终的输出层。这样每个时间步的输出节点，都包含了输入序列中当前时刻完整的过去和未来的上下文信息。下图展示的是一个按时间步展开的双向循环神经网络。该网络包含一个前向和一个后向RNN，其中有六个权重矩阵：输入到前向隐层和后向隐层的权重矩阵（<span class="markdown-equation" id="equation-0">$W_1, W_3$</span>），隐层到隐层自己的权重矩阵（<span class="markdown-equation" id="equation-1">$W_2,W_5$</span>），前向隐层和后向隐层到输出层的权重矩阵（<span class="markdown-equation" id="equation-2">$W_4, W_6$</span>）。注意，该网络的前向隐层和后向隐层之间没有连接。</p>
<p align="center">
<img src="image/bi_rnn.png" width="450"/><br/>
图3. 按时间步展开的双向循环神经网络
</p>
<h3>编码器-解码器框架</h3>
<p>编码器-解码器（Encoder-Decoder）[<a href="#参考文献">2</a>]框架用于解决由一个任意长度的源序列到另一个任意长度的目标序列的变换问题。即编码阶段将整个源序列编码成一个向量，解码阶段通过最大化预测序列概率，从中解码出整个目标序列。编码和解码的过程通常都使用RNN实现。
</p><p align="center">
<img src="image/encoder_decoder.png" width="700"/><br/>
图4. 编码器-解码器框架
</p>
<h4>编码器</h4>
<p>编码阶段分为三步：</p>
<ol>
<li>
<p>one-hot vector表示：将源语言句子<span class="markdown-equation" id="equation-3">$x=\left \{ x_1,x_2,...,x_T \right \}$</span>的每个词<span class="markdown-equation" id="equation-4">$x_i$</span>表示成一个列向量<span class="markdown-equation" id="equation-5">$w_i\epsilon \left \{ 0,1 \right \}^{\left | V \right |},i=1,2,...,T$</span>。这个向量<span class="markdown-equation" id="equation-6">$w_i$</span>的维度与词汇表大小<span class="markdown-equation" id="equation-7">$\left | V \right |$</span> 相同，并且只有一个维度上有值1（该位置对应该词在词汇表中的位置），其余全是0。</p>
</li>
<li>
<p>映射到低维语义空间的词向量：one-hot vector表示存在两个问题，1）生成的向量维度往往很大，容易造成维数灾难；2）难以刻画词与词之间的关系（如语义相似性，也就是无法很好地表达语义）。因此，需再one-hot vector映射到低维的语义空间，由一个固定维度的稠密向量（称为词向量）表示。记映射矩阵为<span class="markdown-equation" id="equation-8">$C\epsilon R^{K\times \left | V \right |}$</span>，用<span class="markdown-equation" id="equation-9">$s_i=Cw_i$</span>表示第<span class="markdown-equation" id="equation-10">$i$</span>个词的词向量，<span class="markdown-equation" id="equation-11">$K$</span>为向量维度。</p>
</li>
<li>
<p>用RNN编码源语言词序列：这一过程的计算公式为<span class="markdown-equation" id="equation-12">$h_i=\varnothing _\theta \left ( h_{i-1}, s_i \right )$</span>，其中<span class="markdown-equation" id="equation-13">$h_0$</span>是一个全零的向量，<span class="markdown-equation" id="equation-14">$\varnothing _\theta$</span>是一个非线性激活函数，最后得到的<span class="markdown-equation" id="equation-15">$\mathbf{h}=\left \{ h_1,..., h_T \right \}$</span>就是RNN依次读入源语言<span class="markdown-equation" id="equation-16">$T$</span>个词的状态编码序列。整句话的向量表示可以采用<span class="markdown-equation" id="equation-17">$\mathbf{h}$</span>在最后一个时间步<span class="markdown-equation" id="equation-16">$T$</span>的状态编码，或使用时间维上的池化（pooling）结果。</p>
</li>
</ol>
<p>第3步也可以使用双向循环神经网络实现更复杂的句编码表示，具体可以用双向GRU实现。前向GRU按照词序列<span class="markdown-equation" id="equation-19">$(x_1,x_2,...,x_T)$</span>的顺序依次编码源语言端词，并得到一系列隐层状态<span class="markdown-equation" id="equation-20">$(\overrightarrow{h_1},\overrightarrow{h_2},...,\overrightarrow{h_T})$</span>。类似的，后向GRU按照<span class="markdown-equation" id="equation-21">$(x_T,x_{T-1},...,x_1)$</span>的顺序依次编码源语言端词，得到<span class="markdown-equation" id="equation-22">$(\overleftarrow{h_1},\overleftarrow{h_2},...,\overleftarrow{h_T})$</span>。最后对于词<span class="markdown-equation" id="equation-4">$x_i$</span>，通过拼接两个GRU的结果得到它的隐层状态，即<span class="markdown-equation" id="equation-24">$h_i=\left [ \overrightarrow{h_i^T},\overleftarrow{h_i^T} \right ]^{T}$</span>。</p>
<p align="center">
<img src="image/encoder_attention.png" width="500"/><br/>
图5. 使用双向GRU的编码器
</p>
<h4>解码器</h4>
<p>机器翻译任务的训练过程中，解码阶段的目标是最大化下一个正确的目标语言词的概率。思路是：</p>
<ol>
<li>每一个时刻，根据源语言句子的编码信息（又叫上下文向量，context vector）<span class="markdown-equation" id="equation-25">$c$</span>、真实目标语言序列的第<span class="markdown-equation" id="equation-10">$i$</span>个词<span class="markdown-equation" id="equation-27">$u_i$</span>和<span class="markdown-equation" id="equation-10">$i$</span>时刻RNN的隐层状态<span class="markdown-equation" id="equation-29">$z_i$</span>，计算出下一个隐层状态<span class="markdown-equation" id="equation-30">$z_{i+1}$</span>。计算公式如下：</li>
</ol>
<p><span class="markdown-equation" id="equation-31">$$z_{i+1}=\phi _{\theta '}\left ( c,u_i,z_i \right )$$</span></p>
<p>其中<span class="markdown-equation" id="equation-32">$\phi _{\theta '}$</span>是一个非线性激活函数；<span class="markdown-equation" id="equation-33">$c=q\mathbf{h}$</span>是源语言句子的上下文向量，在不使用<a href="#注意力机制">注意力机制</a>时，如果<a href="#编码器">编码器</a>的输出是源语言句子编码后的最后一个元素，则可以定义<span class="markdown-equation" id="equation-34">$c=h_T$</span>；<span class="markdown-equation" id="equation-27">$u_i$</span>是目标语言序列的第<span class="markdown-equation" id="equation-10">$i$</span>个单词，<span class="markdown-equation" id="equation-37">$u_0$</span>是目标语言序列的开始标记<code>&lt;s&gt;</code>，表示解码开始；<span class="markdown-equation" id="equation-29">$z_i$</span>是<span class="markdown-equation" id="equation-10">$i$</span>时刻解码RNN的隐层状态，<span class="markdown-equation" id="equation-40">$z_0$</span>是一个全零的向量。</p>
<ol>
<li>将<span class="markdown-equation" id="equation-30">$z_{i+1}$</span>通过<code>softmax</code>归一化，得到目标语言序列的第<span class="markdown-equation" id="equation-42">$i+1$</span>个单词的概率分布<span class="markdown-equation" id="equation-43">$p_{i+1}$</span>。概率分布公式如下：</li>
</ol>
<p><span class="markdown-equation" id="equation-44">$$p\left ( u_{i+1}|u_{&lt;i+1},\mathbf{x} \right )=softmax(W_sz_{i+1}+b_z)$$</span></p>
<p>其中<span class="markdown-equation" id="equation-45">$W_sz_{i+1}+b_z$</span>是对每个可能的输出单词进行打分，再用softmax归一化就可以得到第<span class="markdown-equation" id="equation-42">$i+1$</span>个词的概率<span class="markdown-equation" id="equation-43">$p_{i+1}$</span>。</p>
<ol>
<li>根据<span class="markdown-equation" id="equation-43">$p_{i+1}$</span>和<span class="markdown-equation" id="equation-49">$u_{i+1}$</span>计算代价。</li>
<li>重复步骤1~3，直到目标语言序列中的所有词处理完毕。</li>
</ol>
<p>机器翻译任务的生成过程，通俗来讲就是根据预先训练的模型来翻译源语言句子。生成过程中的解码阶段和上述训练过程的有所差异，具体介绍请见<a href="#柱搜索算法">柱搜索算法</a>。</p>
<h3>注意力机制</h3>
<p>如果编码阶段的输出是一个固定维度的向量，会带来以下两个问题：1）不论源语言序列的长度是5个词还是50个词，如果都用固定维度的向量去编码其中的语义和句法结构信息，对模型来说是一个非常高的要求，特别是对长句子序列而言；2）直觉上，当人类翻译一句话时，会对与当前译文更相关的源语言片段上给予更多关注，且关注点会随着翻译的进行而改变。而固定维度的向量则相当于，任何时刻都对源语言所有信息给予了同等程度的关注，这是不合理的。因此，Bahdanau等人[<a href="#参考文献">4</a>]引入注意力（attention）机制，可以对编码后的上下文片段进行解码，以此来解决长句子的特征学习问题。下面介绍在注意力机制下的解码器结构。</p>
<p>与简单的解码器不同，这里<span class="markdown-equation" id="equation-29">$z_i$</span>的计算公式为：</p>
<p><span class="markdown-equation" id="equation-51">$$z_{i+1}=\phi _{\theta '}\left ( c_i,u_i,z_i \right )$$</span></p>
<p>可见，源语言句子的编码向量表示为第<span class="markdown-equation" id="equation-10">$i$</span>个词的上下文片段<span class="markdown-equation" id="equation-53">$c_i$</span>，即针对每一个目标语言中的词<span class="markdown-equation" id="equation-27">$u_i$</span>，都有一个特定的<span class="markdown-equation" id="equation-53">$c_i$</span>与之对应。<span class="markdown-equation" id="equation-53">$c_i$</span>的计算公式如下：</p>
<p><span class="markdown-equation" id="equation-57">$$c_i=\sum _{j=1}^{T}a_{ij}h_j, a_i=\left[ a_{i1},a_{i2},...,a_{iT}\right ]$$</span></p>
<p>从公式中可以看出，注意力机制是通过对编码器中各时刻的RNN状态<span class="markdown-equation" id="equation-58">$h_j$</span>进行加权平均实现的。权重<span class="markdown-equation" id="equation-59">$a_{ij}$</span>表示目标语言中第<span class="markdown-equation" id="equation-10">$i$</span>个词对源语言中第<span class="markdown-equation" id="equation-61">$j$</span>个词的注意力大小，<span class="markdown-equation" id="equation-59">$a_{ij}$</span>的计算公式如下：</p>
<p>begin{align}
a_{ij}&amp;=frac{exp(e_{ij})}{sum_{k=1}^{T}exp(e_{ik})}\\
e_{ij}&amp;=align(z_i,h_j)\\
end{align}</p>
<p>其中，<span class="markdown-equation" id="equation-63">$align$</span>可以看作是一个对齐模型，用来衡量目标语言中第<span class="markdown-equation" id="equation-10">$i$</span>个词和源语言中第<span class="markdown-equation" id="equation-61">$j$</span>个词的匹配程度。具体而言，这个程度是通过解码RNN的第<span class="markdown-equation" id="equation-10">$i$</span>个隐层状态<span class="markdown-equation" id="equation-29">$z_i$</span>和源语言句子的第<span class="markdown-equation" id="equation-61">$j$</span>个上下文片段<span class="markdown-equation" id="equation-58">$h_j$</span>计算得到的。传统的对齐模型中，目标语言的每个词明确对应源语言的一个或多个词（hard alignment）；而在注意力模型中采用的是soft alignment，即任何两个目标语言和源语言词间均存在一定的关联，且这个关联强度是由模型计算得到的实数，因此可以融入整个NMT框架，并通过反向传播算法进行训练。</p>
<p align="center">
<img src="image/decoder_attention.png" width="500"/><br/>
图6. 基于注意力机制的解码器
</p>
<h3>柱搜索算法</h3>
<p>柱搜索（<a href="http://en.wikipedia.org/wiki/Beam_search">beam search</a>）是一种启发式图搜索算法，用于在图或树中搜索有限集合中的最优扩展节点，通常用在解空间非常大的系统（如机器翻译、语音识别）中，原因是内存无法装下图或树中所有展开的解。如在机器翻译任务中希望翻译“<code>&lt;s&gt;你好&lt;e&gt;</code>”，就算目标语言字典中只有3个词（<code>&lt;s&gt;</code>, <code>&lt;e&gt;</code>, <code>hello</code>），也可能生成无限句话（<code>hello</code>循环出现的次数不定），为了找到其中较好的翻译结果，我们可采用柱搜索算法。</p>
<p>柱搜索算法使用广度优先策略建立搜索树，在树的每一层，按照启发代价（heuristic cost）（本教程中，为生成词的log概率之和）对节点进行排序，然后仅留下预先确定的个数（文献中通常称为beam width、beam size、柱宽度等）的节点。只有这些节点会在下一层继续扩展，其他节点就被剪掉了，也就是说保留了质量较高的节点，剪枝了质量较差的节点。因此，搜索所占用的空间和时间大幅减少，但缺点是无法保证一定获得最优解。</p>
<p>使用柱搜索算法的解码阶段，目标是最大化生成序列的概率。思路是：</p>
<ol>
<li>每一个时刻，根据源语言句子的编码信息<span class="markdown-equation" id="equation-25">$c$</span>、生成的第<span class="markdown-equation" id="equation-10">$i$</span>个目标语言序列单词<span class="markdown-equation" id="equation-27">$u_i$</span>和<span class="markdown-equation" id="equation-10">$i$</span>时刻RNN的隐层状态<span class="markdown-equation" id="equation-29">$z_i$</span>，计算出下一个隐层状态<span class="markdown-equation" id="equation-30">$z_{i+1}$</span>。</li>
<li>将<span class="markdown-equation" id="equation-30">$z_{i+1}$</span>通过<code>softmax</code>归一化，得到目标语言序列的第<span class="markdown-equation" id="equation-42">$i+1$</span>个单词的概率分布<span class="markdown-equation" id="equation-43">$p_{i+1}$</span>。</li>
<li>根据<span class="markdown-equation" id="equation-43">$p_{i+1}$</span>采样出单词<span class="markdown-equation" id="equation-49">$u_{i+1}$</span>。</li>
<li>重复步骤1~3，直到获得句子结束标记<code>&lt;e&gt;</code>或超过句子的最大生成长度为止。</li>
</ol>
<p>注意：<span class="markdown-equation" id="equation-30">$z_{i+1}$</span>和<span class="markdown-equation" id="equation-43">$p_{i+1}$</span>的计算公式同<a href="#解码器">解码器</a>中的一样。且由于生成时的每一步都是通过贪心法实现的，因此并不能保证得到全局最优解。</p>
<h2>数据介绍</h2>
<p>本教程使用<a href="http://www-lium.univ-lemans.fr/~schwenk/cslm_joint_paper/">WMT-14</a>数据集中的<a href="http://www-lium.univ-lemans.fr/~schwenk/cslm_joint_paper/data/bitexts.tgz">bitexts(after selection)</a>作为训练集，<a href="http://www-lium.univ-lemans.fr/~schwenk/cslm_joint_paper/data/dev+test.tgz">dev+test data</a>作为测试集和生成集。</p>
<h3>数据预处理</h3>
<p>我们的预处理流程包括两步：
- 将每个源语言到目标语言的平行语料库文件合并为一个文件：
  - 合并每个<code>XXX.src</code>和<code>XXX.trg</code>文件为<code>XXX</code>。
  - <code>XXX</code>中的第<span class="markdown-equation" id="equation-10">$i$</span>行内容为<code>XXX.src</code>中的第<span class="markdown-equation" id="equation-10">$i$</span>行和<code>XXX.trg</code>中的第<span class="markdown-equation" id="equation-10">$i$</span>行连接，用't'分隔。
- 创建训练数据的“源字典”和“目标字典”。每个字典都有<strong>DICTSIZE</strong>个单词，包括：语料中词频最高的（DICTSIZE - 3）个单词，和3个特殊符号<code>&lt;s&gt;</code>（序列的开始）、<code>&lt;e&gt;</code>（序列的结束）和<code>&lt;unk&gt;</code>（未登录词）。</p>
<h3>示例数据</h3>
<p>因为完整的数据集数据量较大，为了验证训练流程，PaddlePaddle接口paddle.dataset.wmt14中默认提供了一个经过预处理的<a href="http://paddlepaddle.bj.bcebos.com/demo/wmt_shrinked_data/wmt14.tgz">较小规模的数据集</a>。</p>
<p>该数据集有193319条训练数据，6003条测试数据，词典长度为30000。因为数据规模限制，使用该数据集训练出来的模型效果无法保证。</p>
<h2>流程说明</h2>
<h3>paddle初始化</h3>
<div class="highlight"><pre><span></span><span class="c1"># 加载 paddle的python包</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">paddle.v2</span> <span class="kn">as</span> <span class="nn">paddle</span>

<span class="c1"># 配置只使用cpu，并且使用一个cpu进行训练</span>
<span class="n">paddle</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">trainer_count</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># 训练模式False，生成模式True</span>
<span class="n">is_generating</span> <span class="o">=</span> <span class="bp">False</span>
</pre></div>
<h3>模型结构</h3>
<ol>
<li>
<p>首先，定义了一些全局变量。</p>
<div class="highlight"><pre><span></span><span class="n">dict_size</span> <span class="o">=</span> <span class="mi">30000</span> <span class="c1"># 字典维度</span>
<span class="n">source_dict_dim</span> <span class="o">=</span> <span class="n">dict_size</span> <span class="c1"># 源语言字典维度</span>
<span class="n">target_dict_dim</span> <span class="o">=</span> <span class="n">dict_size</span> <span class="c1"># 目标语言字典维度</span>
<span class="n">word_vector_dim</span> <span class="o">=</span> <span class="mi">512</span> <span class="c1"># 词向量维度</span>
<span class="n">encoder_size</span> <span class="o">=</span> <span class="mi">512</span> <span class="c1"># 编码器中的GRU隐层大小</span>
<span class="n">decoder_size</span> <span class="o">=</span> <span class="mi">512</span> <span class="c1"># 解码器中的GRU隐层大小</span>
<span class="n">beam_size</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># 柱宽度</span>
<span class="n">max_length</span> <span class="o">=</span> <span class="mi">250</span> <span class="c1"># 生成句子的最大长度</span>
</pre></div>
</li>
<li>
<p>其次，实现编码器框架。分为三步：</p>
</li>
<li>
<p>输入是一个文字序列，被表示成整型的序列。序列中每个元素是文字在字典中的索引。所以，我们定义数据层的数据类型为<code>integer_value_sequence</code>（整型序列），序列中每个元素的范围是<code>[0, source_dict_dim)</code>。</p>
</li>
</ol>
<p></p><div class="highlight"><pre><span></span> <span class="n">src_word_id</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span>
     <span class="n">name</span><span class="o">=</span><span class="s1">'source_language_word'</span><span class="p">,</span>
     <span class="nb">type</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">data_type</span><span class="o">.</span><span class="n">integer_value_sequence</span><span class="p">(</span><span class="n">source_dict_dim</span><span class="p">))</span>
</pre></div>
   - 将上述编码映射到低维语言空间的词向量<span class="markdown-equation" id="equation-86">$\mathbf{s}$</span>。
<p></p><div class="highlight"><pre><span></span> <span class="n">src_embedding</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span>
     <span class="nb">input</span><span class="o">=</span><span class="n">src_word_id</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">word_vector_dim</span><span class="p">)</span>
</pre></div>
   - 用双向GRU编码源语言序列，拼接两个GRU的编码结果得到<span class="markdown-equation" id="equation-17">$\mathbf{h}$</span>。
<div class="highlight"><pre><span></span> <span class="n">src_forward</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">simple_gru</span><span class="p">(</span>
     <span class="nb">input</span><span class="o">=</span><span class="n">src_embedding</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">encoder_size</span><span class="p">)</span>
 <span class="n">src_backward</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">simple_gru</span><span class="p">(</span>
     <span class="nb">input</span><span class="o">=</span><span class="n">src_embedding</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">encoder_size</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
 <span class="n">encoded_vector</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">src_forward</span><span class="p">,</span> <span class="n">src_backward</span><span class="p">])</span>
</pre></div>
<ol>
<li>
<p>接着，定义基于注意力机制的解码器框架。分为三步：</p>
</li>
<li>
<p>对源语言序列编码后的结果（见2的最后一步），过一个前馈神经网络（Feed Forward Neural Network），得到其映射。</p>
</li>
</ol>
<div class="highlight"><pre><span></span><span class="n">encoded_proj</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span>
      <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Linear</span><span class="p">(),</span>
      <span class="n">size</span><span class="o">=</span><span class="n">decoder_size</span><span class="p">,</span>
      <span class="n">bias_attr</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
      <span class="nb">input</span><span class="o">=</span><span class="n">encoded_vector</span><span class="p">)</span>
</pre></div>
<ul>
<li>构造解码器RNN的初始状态。由于解码器需要预测时序目标序列，但在0时刻并没有初始值，所以我们希望对其进行初始化。这里采用的是将源语言序列逆序编码后的最后一个状态进行非线性映射，作为该初始值，即<span class="markdown-equation" id="equation-88">$c_0=h_T$</span>。</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">backward_first</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">first_seq</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">src_backward</span><span class="p">)</span>
<span class="n">decoder_boot</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span>
      <span class="n">size</span><span class="o">=</span><span class="n">decoder_size</span><span class="p">,</span>
      <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
      <span class="n">bias_attr</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
      <span class="nb">input</span><span class="o">=</span><span class="n">backward_first</span><span class="p">)</span>
</pre></div>
<ul>
<li>定义解码阶段每一个时间步的RNN行为，即根据当前时刻的源语言上下文向量<span class="markdown-equation" id="equation-53">$c_i$</span>、解码器隐层状态<span class="markdown-equation" id="equation-29">$z_i$</span>和目标语言中第<span class="markdown-equation" id="equation-10">$i$</span>个词<span class="markdown-equation" id="equation-27">$u_i$</span>，来预测第<span class="markdown-equation" id="equation-42">$i+1$</span>个词的概率<span class="markdown-equation" id="equation-43">$p_{i+1}$</span>。<ul>
<li>decoder_mem记录了前一个时间步的隐层状态<span class="markdown-equation" id="equation-29">$z_i$</span>，其初始状态是decoder_boot。</li>
<li>context通过调用<code>simple_attention</code>函数，实现公式<span class="markdown-equation" id="equation-96">$c_i=\sum {j=1}^{T}a_{ij}h_j$</span>。其中，enc_vec是<span class="markdown-equation" id="equation-58">$h_j$</span>，enc_proj是<span class="markdown-equation" id="equation-58">$h_j$</span>的映射（见3.1），权重<span class="markdown-equation" id="equation-59">$a_{ij}$</span>的计算已经封装在<code>simple_attention</code>函数中。</li>
<li>decoder_inputs融合了<span class="markdown-equation" id="equation-53">$c_i$</span>和当前目标词current_word（即<span class="markdown-equation" id="equation-27">$u_i$</span>）的表示。</li>
<li>gru_step通过调用<code>gru_step_layer</code>函数，在decoder_inputs和decoder_mem上做了激活操作，即实现公式<span class="markdown-equation" id="equation-102">$z_{i+1}=\phi _{\theta '}\left ( c_i,u_i,z_i \right )$</span>。</li>
<li>最后，使用softmax归一化计算单词的概率，将out结果返回，即实现公式<span class="markdown-equation" id="equation-103">$p\left ( u_i|u_{&lt;i},\mathbf{x} \right )=softmax(W_sz_i+b_z)$</span>。</li>
</ul>
</li>
</ul>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gru_decoder_with_attention</span><span class="p">(</span><span class="n">enc_vec</span><span class="p">,</span> <span class="n">enc_proj</span><span class="p">,</span> <span class="n">current_word</span><span class="p">):</span>
     <span class="n">decoder_mem</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">memory</span><span class="p">(</span>
         <span class="n">name</span><span class="o">=</span><span class="s1">'gru_decoder'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">decoder_size</span><span class="p">,</span> <span class="n">boot_layer</span><span class="o">=</span><span class="n">decoder_boot</span><span class="p">)</span>

     <span class="n">context</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">simple_attention</span><span class="p">(</span>
         <span class="n">encoded_sequence</span><span class="o">=</span><span class="n">enc_vec</span><span class="p">,</span>
         <span class="n">encoded_proj</span><span class="o">=</span><span class="n">enc_proj</span><span class="p">,</span>
         <span class="n">decoder_state</span><span class="o">=</span><span class="n">decoder_mem</span><span class="p">)</span>

     <span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span>
         <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Linear</span><span class="p">(),</span>
         <span class="n">size</span><span class="o">=</span><span class="n">decoder_size</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span>
         <span class="n">bias_attr</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
         <span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">context</span><span class="p">,</span> <span class="n">current_word</span><span class="p">],</span>
         <span class="n">layer_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">ExtraLayerAttribute</span><span class="p">(</span>
             <span class="n">error_clipping_threshold</span><span class="o">=</span><span class="mf">100.0</span><span class="p">))</span>

     <span class="n">gru_step</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">gru_step</span><span class="p">(</span>
         <span class="n">name</span><span class="o">=</span><span class="s1">'gru_decoder'</span><span class="p">,</span>
         <span class="nb">input</span><span class="o">=</span><span class="n">decoder_inputs</span><span class="p">,</span>
         <span class="n">output_mem</span><span class="o">=</span><span class="n">decoder_mem</span><span class="p">,</span>
         <span class="n">size</span><span class="o">=</span><span class="n">decoder_size</span><span class="p">)</span>

     <span class="n">out</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">mixed</span><span class="p">(</span>
         <span class="n">size</span><span class="o">=</span><span class="n">target_dict_dim</span><span class="p">,</span>
         <span class="n">bias_attr</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
         <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(),</span>
         <span class="nb">input</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">full_matrix_projection</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">gru_step</span><span class="p">))</span>
     <span class="k">return</span> <span class="n">out</span>
</pre></div>
<ol>
<li>
<p>定义解码器框架名字，和<code>gru_decoder_with_attention</code>函数的前两个输入。注意：这两个输入使用<code>StaticInput</code>，具体说明可见<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/howto/deep_model/rnn/recurrent_group_cn.md#输入">StaticInput文档</a>。</p>
<div class="highlight"><pre><span></span><span class="n">decoder_group_name</span> <span class="o">=</span> <span class="s2">"decoder_group"</span>
<span class="n">group_input1</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">StaticInput</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">encoded_vector</span><span class="p">)</span>
<span class="n">group_input2</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">StaticInput</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">encoded_proj</span><span class="p">)</span>
<span class="n">group_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">group_input1</span><span class="p">,</span> <span class="n">group_input2</span><span class="p">]</span>
</pre></div>
</li>
<li>
<p>训练模式下的解码器调用：</p>
</li>
<li>
<p>首先，将目标语言序列的词向量trg_embedding，直接作为训练模式下的current_word传给<code>gru_decoder_with_attention</code>函数。</p>
</li>
<li>其次，使用<code>recurrent_group</code>函数循环调用<code>gru_decoder_with_attention</code>函数。</li>
<li>接着，使用目标语言的下一个词序列作为标签层lbl，即预测目标词。</li>
<li>最后，用多类交叉熵损失函数<code>classification_cost</code>来计算损失值。</li>
</ol>
<p>```python
   if not is_generating:
       trg_embedding = paddle.layer.embedding(
           input=paddle.layer.data(
               name='target_language_word',
               type=paddle.data_type.integer_value_sequence(target_dict_dim)),
           size=word_vector_dim,
           param_attr=paddle.attr.ParamAttr(name='_target_language_embedding'))
       group_inputs.append(trg_embedding)</p>
<pre><code>   # For decoder equipped with attention mechanism, in training,
   # target embeding (the groudtruth) is the data input,
   # while encoded source sequence is accessed to as an unbounded memory.
   # Here, the StaticInput defines a read-only memory
   # for the recurrent_group.
   decoder = paddle.layer.recurrent_group(
       name=decoder_group_name,
       step=gru_decoder_with_attention,
       input=group_inputs)

   lbl = paddle.layer.data(
       name='target_language_next_word',
       type=paddle.data_type.integer_value_sequence(target_dict_dim))
   cost = paddle.layer.classification_cost(input=decoder, label=lbl)
```
</code></pre>
<ol>
<li>
<p>生成模式下的解码器调用：</p>
</li>
<li>
<p>首先，在序列生成任务中，由于解码阶段的RNN总是引用上一时刻生成出的词的词向量，作为当前时刻的输入，因此，使用<code>GeneratedInput</code>来自动完成这一过程。具体说明可见<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/howto/deep_model/rnn/recurrent_group_cn.md#输入">GeneratedInput文档</a>。</p>
</li>
<li>其次，使用<code>beam_search</code>函数循环调用<code>gru_decoder_with_attention</code>函数，生成出序列id。</li>
</ol>
<div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">is_generating</span><span class="p">:</span>
   <span class="c1"># In generation, the decoder predicts a next target word based on</span>
   <span class="c1"># the encoded source sequence and the previous generated target word.</span>

   <span class="c1"># The encoded source sequence (encoder's output) must be specified by</span>
   <span class="c1"># StaticInput, which is a read-only memory.</span>
   <span class="c1"># Embedding of the previous generated word is automatically retrieved</span>
   <span class="c1"># by GeneratedInputs initialized by a start mark &lt;s&gt;.</span>

    <span class="n">trg_embedding</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">GeneratedInput</span><span class="p">(</span>
        <span class="n">size</span><span class="o">=</span><span class="n">target_dict_dim</span><span class="p">,</span>
        <span class="n">embedding_name</span><span class="o">=</span><span class="s1">'_target_language_embedding'</span><span class="p">,</span>
        <span class="n">embedding_size</span><span class="o">=</span><span class="n">word_vector_dim</span><span class="p">)</span>
    <span class="n">group_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">trg_embedding</span><span class="p">)</span>

    <span class="n">beam_gen</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">beam_search</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">decoder_group_name</span><span class="p">,</span>
        <span class="n">step</span><span class="o">=</span><span class="n">gru_decoder_with_attention</span><span class="p">,</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">group_inputs</span><span class="p">,</span>
        <span class="n">bos_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">eos_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">beam_size</span><span class="o">=</span><span class="n">beam_size</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>
</pre></div>
<p>注意：我们提供的配置在Bahdanau的论文[<a href="#参考文献">4</a>]上做了一些简化，可参考<a href="https://github.com/PaddlePaddle/Paddle/issues/1133">issue #1133</a>。</p>
<h3>训练模型</h3>
<ol>
<li>
<p>参数定义</p>
<p>依据模型配置的<code>cost</code>定义模型参数。可以打印参数名字，如果在网络配置中没有指定名字，则默认生成。</p>
<div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">is_generating</span><span class="p">:</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">print</span> <span class="n">param</span>
</pre></div>
</li>
<li>
<p>数据定义</p>
<p>获取wmt14的dataset reader。</p>
<div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">is_generating</span><span class="p">:</span>
    <span class="n">wmt14_reader</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span>
        <span class="n">paddle</span><span class="o">.</span><span class="n">reader</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span>
            <span class="n">paddle</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">wmt14</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">dict_size</span><span class="o">=</span><span class="n">dict_size</span><span class="p">),</span> <span class="n">buf_size</span><span class="o">=</span><span class="mi">8192</span><span class="p">),</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</li>
<li>
<p>构造trainer</p>
<p>根据优化目标cost,网络拓扑结构和模型参数来构造出trainer用来训练，在构造时还需指定优化方法，这里使用最基本的SGD方法。</p>
<div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">is_generating</span><span class="p">:</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">,</span>
        <span class="n">regularization</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">L2Regularization</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">8e-4</span><span class="p">))</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">cost</span><span class="p">,</span>
                                 <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                                 <span class="n">update_equation</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</li>
<li>
<p>构造event_handler</p>
<p>可以通过自定义回调函数来评估训练过程中的各种状态，比如错误率等。下面的代码通过event.batch_id % 2 == 0 指定每2个batch打印一次日志，包含cost等信息。</p>
<div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">is_generating</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">event_handler</span><span class="p">(</span><span class="n">event</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">paddle</span><span class="o">.</span><span class="n">event</span><span class="o">.</span><span class="n">EndIteration</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">event</span><span class="o">.</span><span class="n">batch_id</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">print</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Pass </span><span class="si">%d</span><span class="s2">, Batch </span><span class="si">%d</span><span class="s2">, Cost </span><span class="si">%f</span><span class="s2">, </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span>
                    <span class="n">event</span><span class="o">.</span><span class="n">pass_id</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">batch_id</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">cost</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
</pre></div>
</li>
<li>
<p>启动训练</p>
<div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">is_generating</span><span class="p">:</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
            <span class="n">reader</span><span class="o">=</span><span class="n">wmt14_reader</span><span class="p">,</span> <span class="n">event_handler</span><span class="o">=</span><span class="n">event_handler</span><span class="p">,</span> <span class="n">num_passes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</li>
</ol>
<p>训练开始后，可以观察到event_handler输出的日志如下：
 </p><div class="highlight"><pre><span></span>Pass 0, Batch 0, Cost 148.444983, {'classification_error_evaluator': 1.0}
.........
Pass 0, Batch 10, Cost 335.896802, {'classification_error_evaluator': 0.9325153231620789}
.........
</pre></div>
<h3>生成模型</h3>
<ol>
<li>
<p>加载预训练的模型</p>
<p>由于NMT模型的训练非常耗时，我们在50个物理节点（每节点含有2颗6核CPU）的集群中，花了5天时间训练了一个模型供大家直接下载使用。该模型大小为205MB，<a href="#BLEU评估">BLEU评估</a>值为26.92。</p>
<p></p><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">is_generating</span><span class="p">:</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">wmt14</span><span class="o">.</span><span class="n">model</span><span class="p">()</span>
</pre></div>
2. 数据定义
<p>从wmt14的生成集中读取前3个样本作为源语言句子。</p>
<p></p><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">is_generating</span><span class="p">:</span>
    <span class="n">gen_creator</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">wmt14</span><span class="o">.</span><span class="n">gen</span><span class="p">(</span><span class="n">dict_size</span><span class="p">)</span>
    <span class="n">gen_data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">gen_num</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">gen_creator</span><span class="p">():</span>
        <span class="n">gen_data</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gen_data</span><span class="p">)</span> <span class="o">==</span> <span class="n">gen_num</span><span class="p">:</span>
            <span class="k">break</span>
</pre></div>
3. 构造infer
<p>根据网络拓扑结构和模型参数构造出infer用来生成，在预测时还需要指定输出域<code>field</code>，这里使用生成句子的概率<code>prob</code>和句子中每个词的<code>id</code>。</p>
<div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">is_generating</span><span class="p">:</span>
    <span class="n">beam_result</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span>
        <span class="n">output_layer</span><span class="o">=</span><span class="n">beam_gen</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">gen_data</span><span class="p">,</span>
        <span class="n">field</span><span class="o">=</span><span class="p">[</span><span class="s1">'prob'</span><span class="p">,</span> <span class="s1">'id'</span><span class="p">])</span>
</pre></div>
</li>
<li>
<p>打印生成结果</p>
<p>根据源/目标语言字典，将源语言句子和<code>beam_size</code>个生成句子打印输出。</p>
<div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">is_generating</span><span class="p">:</span>
    <span class="c1"># load the dictionary</span>
    <span class="n">src_dict</span><span class="p">,</span> <span class="n">trg_dict</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">wmt14</span><span class="o">.</span><span class="n">get_dict</span><span class="p">(</span><span class="n">dict_size</span><span class="p">)</span>

    <span class="n">gen_sen_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">beam_result</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">gen_sen_idx</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">gen_data</span><span class="p">)</span> <span class="o">*</span> <span class="n">beam_size</span>

    <span class="c1"># -1 is the delimiter of generated sequences.</span>
    <span class="c1"># the first element of each generated sequence its length.</span>
    <span class="n">start_pos</span><span class="p">,</span> <span class="n">end_pos</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gen_data</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">" "</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">src_dict</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">beam_size</span><span class="p">):</span>
            <span class="n">end_pos</span> <span class="o">=</span> <span class="n">gen_sen_idx</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">beam_size</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">"</span><span class="si">%.4f</span><span class="se">\t</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">beam_result</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="s2">" "</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="n">trg_dict</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">beam_result</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">])))</span>
            <span class="n">start_pos</span> <span class="o">=</span> <span class="n">end_pos</span> <span class="o">+</span> <span class="mi">2</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</li>
</ol>
<p>生成开始后，可以观察到输出的日志如下：
  </p><div class="highlight"><pre><span></span>Les &lt;unk&gt; se &lt;unk&gt; au sujet de la largeur des sièges alors que de grosses commandes sont en jeu
-19.0196        The &lt;unk&gt; will be rotated about the width of the seats , while large orders are at stake . &lt;e&gt;
-19.1131        The &lt;unk&gt; will be rotated about the width of the seats , while large commands are at stake . &lt;e&gt;
-19.5129        The &lt;unk&gt; will be rotated about the width of the seats , while large commands are at play . &lt;e&gt;
</pre></div>
<h2>总结</h2>
<p>端到端的神经网络机器翻译是近几年兴起的一种全新的机器翻译方法。本章中，我们介绍了NMT中典型的“编码器-解码器”框架和“注意力”机制。由于NMT是一个典型的Seq2Seq（Sequence to Sequence，序列到序列）学习问题，因此，Seq2Seq中的query改写（query rewriting）、摘要、单轮对话等问题都可以用本教程的模型来解决。</p>
<h2>参考文献</h2>
<ol>
<li>Koehn P. <a href="https://books.google.com.hk/books?id=4v_Cx1wIMLkC&amp;printsec=frontcover&amp;hl=zh-CN&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false">Statistical machine translation</a>[M]. Cambridge University Press, 2009.</li>
<li>Cho K, Van Merriënboer B, Gulcehre C, et al. <a href="http://www.aclweb.org/anthology/D/D14/D14-1179.pdf">Learning phrase representations using RNN encoder-decoder for statistical machine translation</a>[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014: 1724-1734.</li>
<li>Chung J, Gulcehre C, Cho K H, et al. <a href="https://arxiv.org/abs/1412.3555">Empirical evaluation of gated recurrent neural networks on sequence modeling</a>[J]. arXiv preprint arXiv:1412.3555, 2014.</li>
<li>Bahdanau D, Cho K, Bengio Y. <a href="https://arxiv.org/abs/1409.0473">Neural machine translation by jointly learning to align and translate</a>[C]//Proceedings of ICLR 2015, 2015.</li>
<li>Papineni K, Roukos S, Ward T, et al. <a href="http://dl.acm.org/citation.cfm?id=1073135">BLEU: a method for automatic evaluation of machine translation</a>[C]//Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 2002: 311-318.</li>
</ol>
<p><br/>
<a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license"><img alt="知识共享许可协议" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width:0"/></a><br/><span href="http://purl.org/dc/dcmitype/Text" property="dct:title" rel="dct:type" xmlns:dct="http://purl.org/dc/terms/">本教程</span> 由 <a href="http://book.paddlepaddle.org" property="cc:attributionName" rel="cc:attributionURL" xmlns:cc="http://creativecommons.org/ns#">PaddlePaddle</a> 创作，采用 <a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license">知识共享 署名-相同方式共享 4.0 国际 许可协议</a>进行许可。</p>
{% endverbatim %}