{% verbatim %}
<h1>语义角色标注</h1>
<p>本教程源代码目录在<a href="https://github.com/PaddlePaddle/book/tree/develop/07.label_semantic_roles">book/label_semantic_roles</a>， 初次使用请参考PaddlePaddle<a href="https://github.com/PaddlePaddle/book/blob/develop/README.cn.md#运行这本书">安装教程</a>，更多内容请参考本教程的<a href="http://bit.baidu.com/course/detail/id/178.html">视频课堂</a>。</p>
<h2>背景介绍</h2>
<p>自然语言分析技术大致分为三个层面：词法分析、句法分析和语义分析。语义角色标注是实现浅层语义分析的一种方式。在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为论元。语义角色是指论元在动词所指事件中担任的角色。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等。</p>
<p>请看下面的例子，“遇到” 是谓词（Predicate，通常简写为“Pred”），“小明”是施事者（Agent），“小红”是受事者（Patient），“昨天” 是事件发生的时间（Time），“公园”是事情发生的地点（Location）。</p>
<p><span class="markdown-equation" id="equation-0">$$\mbox{[小明]}_{\mbox{Agent}}\mbox{[昨天]}_{\mbox{Time}}\mbox{[晚上]}_\mbox{Time}\mbox{在[公园]}_{\mbox{Location}}\mbox{[遇到]}_{\mbox{Predicate}}\mbox{了[小红]}_{\mbox{Patient}}\mbox{。}$$</span></p>
<p>语义角色标注（Semantic Role Labeling，SRL）以句子的谓词为中心，不对句子所包含的语义信息进行深入分析，只分析句子中各成分与谓词之间的关系，即句子的谓词（Predicate）- 论元（Argument）结构，并用语义角色来描述这些结构关系，是许多自然语言理解任务（如信息抽取，篇章分析，深度问答等）的一个重要中间步骤。在研究中一般都假定谓词是给定的，所要做的就是找出给定谓词的各个论元和它们的语义角色。</p>
<p>传统的SRL系统大多建立在句法分析基础之上，通常包括5个流程：</p>
<ol>
<li>构建一棵句法分析树，例如，图1是对上面例子进行依存句法分析得到的一棵句法树。</li>
<li>从句法树上识别出给定谓词的候选论元。</li>
<li>候选论元剪除；一个句子中的候选论元可能很多，候选论元剪除就是从大量的候选项中剪除那些最不可能成为论元的候选项。</li>
<li>论元识别：这个过程是从上一步剪除之后的候选中判断哪些是真正的论元，通常当做一个二分类问题来解决。</li>
<li>对第4步的结果，通过多分类得到论元的语义角色标签。可以看到，句法分析是基础，并且后续步骤常常会构造的一些人工特征，这些特征往往也来自句法分析。</li>
</ol>
<div align="center">
<img align="center" src="image/dependency_parsing.png" width="80%"/><br/>
图1. 依存句法分析句法树示例
</div>
<p>然而，完全句法分析需要确定句子所包含的全部句法信息，并确定句子各成分之间的关系，是一个非常困难的任务，目前技术下的句法分析准确率并不高，句法分析的细微错误都会导致SRL的错误。为了降低问题的复杂度，同时获得一定的句法结构信息，“浅层句法分析”的思想应运而生。浅层句法分析也称为部分句法分析（partial parsing）或语块划分（chunking）。和完全句法分析得到一颗完整的句法树不同，浅层句法分析只需要识别句子中某些结构相对简单的独立成分，例如：动词短语，这些被识别出来的结构称为语块。为了回避 “无法获得准确率较高的句法树” 所带来的困难，一些研究[<a href="#参考文献">1</a>]也提出了基于语块（chunk）的SRL方法。基于语块的SRL方法将SRL作为一个序列标注问题来解决。序列标注任务一般都会采用BIO表示方式来定义序列标注的标签集，我们先来介绍这种表示方法。在BIO表示法中，B代表语块的开始，I代表语块的中间，O代表语块结束。通过B、I、O 三种标记将不同的语块赋予不同的标签，例如：对于一个角色为A的论元，将它所包含的第一个语块赋予标签B-A，将它所包含的其它语块赋予标签I-A，不属于任何论元的语块赋予标签O。</p>
<p>我们继续以上面的这句话为例，图1展示了BIO表示方法。</p>
<div align="center">
<img align="center" src="image/bio_example.png" width="90%"/><br/>
图2. BIO标注方法示例
</div>
<p>从上面的例子可以看到，根据序列标注结果可以直接得到论元的语义角色标注结果，是一个相对简单的过程。这种简单性体现在：（1）依赖浅层句法分析，降低了句法分析的要求和难度；（2）没有了候选论元剪除这一步骤；（3）论元的识别和论元标注是同时实现的。这种一体化处理论元识别和论元标注的方法，简化了流程，降低了错误累积的风险，往往能够取得更好的结果。</p>
<p>与基于语块的SRL方法类似，在本教程中我们也将SRL看作一个序列标注问题，不同的是，我们只依赖输入文本序列，不依赖任何额外的语法解析结果或是复杂的人造特征，利用深度神经网络构建一个端到端学习的SRL系统。我们以<a href="http://www.cs.upc.edu/~srlconll/">CoNLL-2004 and CoNLL-2005 Shared Tasks</a>任务中SRL任务的公开数据集为例，实践下面的任务：给定一句话和这句话里的一个谓词，通过序列标注的方式，从句子中找到谓词对应的论元，同时标注它们的语义角色。</p>
<h2>模型概览</h2>
<p>循环神经网络（Recurrent Neural Network）是一种对序列建模的重要模型，在自然语言处理任务中有着广泛地应用。不同于前馈神经网络（Feed-forward Neural Network），RNN能够处理输入之间前后关联的问题。LSTM是RNN的一种重要变种，常用来学习长序列中蕴含的长程依赖关系，我们在<a href="https://github.com/PaddlePaddle/book/tree/develop/05.understand_sentiment">情感分析</a>一篇中已经介绍过，这一篇中我们依然利用LSTM来解决SRL问题。</p>
<h3>栈式循环神经网络（Stacked Recurrent Neural Network）</h3>
<p>深层网络有助于形成层次化特征，网络上层在下层已经学习到的初级特征基础上，形成更复杂的高级特征。尽管LSTM沿时间轴展开后等价于一个非常“深”的前馈网络，但由于LSTM各个时间步参数共享，<span class="markdown-equation" id="equation-1">$t-1$</span>时刻状态到<span class="markdown-equation" id="equation-2">$t$</span>时刻的映射，始终只经过了一次非线性映射，也就是说单层LSTM对状态转移的建模是 “浅” 的。堆叠多个LSTM单元，令前一个LSTM<span class="markdown-equation" id="equation-2">$t$</span>时刻的输出，成为下一个LSTM单元<span class="markdown-equation" id="equation-2">$t$</span>时刻的输入，帮助我们构建起一个深层网络，我们把它称为第一个版本的栈式循环神经网络。深层网络提高了模型拟合复杂模式的能力，能够更好地建模跨不同时间步的模式[<a href="#参考文献">2</a>]。</p>
<p>然而，训练一个深层LSTM网络并非易事。纵向堆叠多个LSTM单元可能遇到梯度在纵向深度上传播受阻的问题。通常，堆叠4层LSTM单元可以正常训练，当层数达到4~8层时，会出现性能衰减，这时必须考虑一些新的结构以保证梯度纵向顺畅传播，这是训练深层LSTM网络必须解决的问题。我们可以借鉴LSTM解决 “梯度消失梯度爆炸” 问题的智慧之一：在记忆单元（Memory Cell）这条信息传播的路线上没有非线性映射，当梯度反向传播时既不会衰减、也不会爆炸。因此，深层LSTM模型也可以在纵向上添加一条保证梯度顺畅传播的路径。</p>
<p>一个LSTM单元完成的运算可以被分为三部分：（1）输入到隐层的映射（input-to-hidden） ：每个时间步输入信息<span class="markdown-equation" id="equation-5">$x$</span>会首先经过一个矩阵映射，再作为遗忘门，输入门，记忆单元，输出门的输入，注意，这一次映射没有引入非线性激活；（2）隐层到隐层的映射（hidden-to-hidden）：这一步是LSTM计算的主体，包括遗忘门，输入门，记忆单元更新，输出门的计算；（3）隐层到输出的映射（hidden-to-output）：通常是简单的对隐层向量进行激活。我们在第一个版本的栈式网络的基础上，加入一条新的路径：除上一层LSTM输出之外，将前层LSTM的输入到隐层的映射作为的一个新的输入，同时加入一个线性映射去学习一个新的变换。</p>
<p>图3是最终得到的栈式循环神经网络结构示意图。</p>
<p align="center">
<img align="center" src="./image/stacked_lstm.png" width="40%"/><br/>
图3. 基于LSTM的栈式循环神经网络结构示意图
</p>
<h3>双向循环神经网络（Bidirectional Recurrent Neural Network）</h3>
<p>在LSTM中，<span class="markdown-equation" id="equation-2">$t$</span>时刻的隐藏层向量编码了到<span class="markdown-equation" id="equation-2">$t$</span>时刻为止所有输入的信息，但<span class="markdown-equation" id="equation-2">$t$</span>时刻的LSTM可以看到历史，却无法看到未来。在绝大多数自然语言处理任务中，我们几乎总是能拿到整个句子。这种情况下，如果能够像获取历史信息一样，得到未来的信息，对序列学习任务会有很大的帮助。</p>
<p>为了克服这一缺陷，我们可以设计一种双向循环网络单元，它的思想简单且直接：对上一节的栈式循环神经网络进行一个小小的修改，堆叠多个LSTM单元，让每一层LSTM单元分别以：正向、反向、正向 …… 的顺序学习上一层的输出序列。于是，从第2层开始，<span class="markdown-equation" id="equation-2">$t$</span>时刻我们的LSTM单元便总是可以看到历史和未来的信息。图4是基于LSTM的双向循环神经网络结构示意图。</p>
<p align="center">
<img align="center" src="./image/bidirectional_stacked_lstm.png" width="60%"/><br/>
图4. 基于LSTM的双向循环神经网络结构示意图
</p>
<p>需要说明的是，这种双向RNN结构和Bengio等人在机器翻译任务中使用的双向RNN结构[<a href="#参考文献">3</a>, <a href="#参考文献">4</a>] 并不相同，我们会在后续<a href="https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/README.cn.md">机器翻译</a>任务中，介绍另一种双向循环神经网络。</p>
<h3>条件随机场 (Conditional Random Field)</h3>
<p>使用神经网络模型解决问题的思路通常是：前层网络学习输入的特征表示，网络的最后一层在特征基础上完成最终的任务。在SRL任务中，深层LSTM网络学习输入的特征表示，条件随机场（Conditional Random Filed， CRF）在特征的基础上完成序列标注，处于整个网络的末端。</p>
<p>CRF是一种概率化结构模型，可以看作是一个概率无向图模型，结点表示随机变量，边表示随机变量之间的概率依赖关系。简单来讲，CRF学习条件概率<span class="markdown-equation" id="equation-10">$P(X|Y)$</span>，其中 <span class="markdown-equation" id="equation-11">$X = (x_1, x_2, ... , x_n)$</span> 是输入序列，<span class="markdown-equation" id="equation-12">$Y = (y_1, y_2, ... , y_n)$</span> 是标记序列；解码过程是给定 <span class="markdown-equation" id="equation-13">$X$</span>序列求解令<span class="markdown-equation" id="equation-14">$P(Y|X)$</span>最大的<span class="markdown-equation" id="equation-15">$Y$</span>序列，即<span class="markdown-equation" id="equation-16">$Y^* = \mbox{arg max}_{Y} P(Y | X)$</span>。</p>
<p>序列标注任务只需要考虑输入和输出都是一个线性序列，并且由于我们只是将输入序列作为条件，不做任何条件独立假设，因此输入序列的元素之间并不存在图结构。综上，在序列标注任务中使用的是如图5所示的定义在链式图上的CRF，称之为线性链条件随机场（Linear Chain Conditional Random Field）。</p>
<p align="center">
<img align="center" src="./image/linear_chain_crf.png" width="35%"/><br/>
图5. 序列标注任务中使用的线性链条件随机场
</p>
<p>根据线性链条件随机场上的因子分解定理[<a href="#参考文献">5</a>]，在给定观测序列<span class="markdown-equation" id="equation-13">$X$</span>时，一个特定标记序列<span class="markdown-equation" id="equation-15">$Y$</span>的概率可以定义为：</p>
<p><span class="markdown-equation" id="equation-19">$$p(Y | X) = \frac{1}{Z(X)} \text{exp}\left(\sum_{i=1}^{n}\left(\sum_{j}\lambda_{j}t_{j} (y_{i - 1}, y_{i}, X, i) + \sum_{k} \mu_k s_k (y_i, X, i)\right)\right)$$</span></p>
<p>其中<span class="markdown-equation" id="equation-20">$Z(X)$</span>是归一化因子，<span class="markdown-equation" id="equation-21">$t_j$</span> 是定义在边上的特征函数，依赖于当前和前一个位置，称为转移特征，表示对于输入序列<span class="markdown-equation" id="equation-13">$X$</span>及其标注序列在 <span class="markdown-equation" id="equation-23">$i$</span>及<span class="markdown-equation" id="equation-24">$i - 1$</span>位置上标记的转移概率。<span class="markdown-equation" id="equation-25">$s_k$</span>是定义在结点上的特征函数，称为状态特征，依赖于当前位置，表示对于观察序列<span class="markdown-equation" id="equation-13">$X$</span>及其<span class="markdown-equation" id="equation-23">$i$</span>位置的标记概率。<span class="markdown-equation" id="equation-28">$\lambda_j$</span> 和 <span class="markdown-equation" id="equation-29">$\mu_k$</span> 分别是转移特征函数和状态特征函数对应的权值。实际上，<span class="markdown-equation" id="equation-2">$t$</span>和<span class="markdown-equation" id="equation-31">$s$</span>可以用相同的数学形式表示，再对转移特征和状态特在各个位置<span class="markdown-equation" id="equation-23">$i$</span>求和有：<span class="markdown-equation" id="equation-33">$f_{k}(Y, X) = \sum_{i=1}^{n}f_k({y_{i - 1}, y_i, X, i})$</span>，把<span class="markdown-equation" id="equation-34">$f$</span>统称为特征函数，于是<span class="markdown-equation" id="equation-14">$P(Y|X)$</span>可表示为：</p>
<p><span class="markdown-equation" id="equation-36">$$p(Y|X, W) = \frac{1}{Z(X)}\text{exp}\sum_{k}\omega_{k}f_{k}(Y, X)$$</span></p>
<p><span class="markdown-equation" id="equation-37">$\omega$</span>是特征函数对应的权值，是CRF模型要学习的参数。训练时，对于给定的输入序列和对应的标记序列集合<span class="markdown-equation" id="equation-38">$D = \left[(X_1,  Y_1), (X_2 , Y_2) , ... , (X_N, Y_N)\right]$</span> ，通过正则化的极大似然估计，求解如下优化目标：</p>
<p><span class="markdown-equation" id="equation-39">$$\DeclareMathOperator*{\argmax}{arg\,max} L(\lambda, D) = - \text{log}\left(\prod_{m=1}^{N}p(Y_m|X_m, W)\right) + C \frac{1}{2}\lVert W\rVert^{2}$$</span></p>
<p>这个优化目标可以通过反向传播算法和整个神经网络一起求解。解码时，对于给定的输入序列<span class="markdown-equation" id="equation-13">$X$</span>，通过解码算法（通常有：维特比算法、Beam Search）求令出条件概率<span class="markdown-equation" id="equation-41">$\bar{P}(Y|X)$</span>最大的输出序列 <span class="markdown-equation" id="equation-42">$\bar{Y}$</span>。</p>
<h3>深度双向LSTM（DB-LSTM）SRL模型</h3>
<p>在SRL任务中，输入是 “谓词” 和 “一句话”，目标是从这句话中找到谓词的论元，并标注论元的语义角色。如果一个句子含有<span class="markdown-equation" id="equation-43">$n$</span>个谓词，这个句子会被处理<span class="markdown-equation" id="equation-43">$n$</span>次。一个最为直接的模型是下面这样：</p>
<ol>
<li>构造输入；</li>
<li>输入1是谓词，输入2是句子</li>
<li>将输入1扩展成和输入2一样长的序列，用one-hot方式表示；</li>
<li>one-hot方式的谓词序列和句子序列通过词表，转换为实向量表示的词向量序列；</li>
<li>将步骤2中的2个词向量序列作为双向LSTM的输入，学习输入序列的特征表示；</li>
<li>CRF以步骤3中模型学习到的特征为输入，以标记序列为监督信号，实现序列标注；</li>
</ol>
<p>大家可以尝试上面这种方法。这里，我们提出一些改进，引入两个简单但对提高系统性能非常有效的特征：</p>
<ul>
<li>谓词上下文：上面的方法中，只用到了谓词的词向量表达谓词相关的所有信息，这种方法始终是非常弱的，特别是如果谓词在句子中出现多次，有可能引起一定的歧义。从经验出发，谓词前后若干个词的一个小片段，能够提供更丰富的信息，帮助消解歧义。于是，我们把这样的经验也添加到模型中，为每个谓词同时抽取一个“谓词上下文” 片段，也就是从这个谓词前后各取<span class="markdown-equation" id="equation-43">$n$</span>个词构成的一个窗口片段；</li>
<li>谓词上下文区域标记：为句子中的每一个词引入一个0-1二值变量，表示它们是否在“谓词上下文”片段中；</li>
</ul>
<p>修改后的模型如下（图6是一个深度为4的模型结构示意图）：</p>
<ol>
<li>构造输入</li>
<li>输入1是句子序列，输入2是谓词序列，输入3是谓词上下文，从句子中抽取这个谓词前后各<span class="markdown-equation" id="equation-43">$n$</span>个词，构成谓词上下文，用one-hot方式表示，输入4是谓词上下文区域标记，标记了句子中每一个词是否在谓词上下文中；</li>
<li>将输入2~3均扩展为和输入1一样长的序列；</li>
<li>输入1~4均通过词表取词向量转换为实向量表示的词向量序列；其中输入1、3共享同一个词表，输入2和4各自独有词表；</li>
<li>第2步的4个词向量序列作为双向LSTM模型的输入；LSTM模型学习输入序列的特征表示，得到新的特性表示序列；</li>
<li>CRF以第3步中LSTM学习到的特征为输入，以标记序列为监督信号，完成序列标注；</li>
</ol>
<div align="center">
<img align="center" src="image/db_lstm_network.png" width="60%"/><br/>
图6. SRL任务上的深层双向LSTM模型
</div>
<h2>数据介绍</h2>
<p>在此教程中，我们选用<a href="http://www.cs.upc.edu/~srlconll/">CoNLL 2005</a>SRL任务开放出的数据集作为示例。需要特别说明的是，CoNLL 2005 SRL任务的训练数集和开发集在比赛之后并非免费进行公开，目前，能够获取到的只有测试集，包括Wall Street Journal的23节和Brown语料集中的3节。在本教程中，我们以测试集中的WSJ数据为训练集来讲解模型。但是，由于测试集中样本的数量远远不够，如果希望训练一个可用的神经网络SRL系统，请考虑付费获取全量数据。</p>
<p>原始数据中同时包括了词性标注、命名实体识别、语法解析树等多种信息。本教程中，我们使用test.wsj文件夹中的数据进行训练和测试，并只会用到words文件夹（文本序列）和props文件夹（标注结果）下的数据。本教程使用的数据目录如下：</p>
<div class="highlight"><pre><span></span>conll05st-release/
└── test.wsj
    ├── props  # 标注结果
    └── words  # 输入文本序列
</pre></div>
<p>标注信息源自Penn TreeBank[<a href="#参考文献">7</a>]和PropBank[<a href="#参考文献">8</a>]的标注结果。PropBank标注结果的标签和我们在文章一开始示例中使用的标注结果标签不同，但原理是相同的，关于标注结果标签含义的说明，请参考论文[<a href="#参考文献">9</a>]。</p>
<p>原始数据需要进行数据预处理才能被PaddlePaddle处理，预处理包括下面几个步骤:</p>
<ol>
<li>将文本序列和标记序列其合并到一条记录中；</li>
<li>一个句子如果含有<span class="markdown-equation" id="equation-43">$n$</span>个谓词，这个句子会被处理<span class="markdown-equation" id="equation-43">$n$</span>次，变成<span class="markdown-equation" id="equation-43">$n$</span>条独立的训练样本，每个样本一个不同的谓词；</li>
<li>抽取谓词上下文和构造谓词上下文区域标记；</li>
<li>构造以BIO法表示的标记；</li>
<li>依据词典获取词对应的整数索引。</li>
</ol>
<div class="highlight"><pre><span></span><span class="c1"># import paddle.v2.dataset.conll05 as conll05</span>
<span class="c1"># conll05.corpus_reader函数完成上面第1步和第2步.</span>
<span class="c1"># conll05.reader_creator函数完成上面第3步到第5步.</span>
<span class="c1"># conll05.test函数可以获取处理之后的每条样本来供PaddlePaddle训练.</span>
</pre></div>
<p>预处理完成之后一条训练样本包含9个特征，分别是：句子序列、谓词、谓词上下文（占 5 列）、谓词上下区域标志、标注序列。下表是一条训练样本的示例。</p>
<table>
<thead>
<tr>
<th>句子序列</th>
<th>谓词</th>
<th>谓词上下文（窗口 = 5）</th>
<th>谓词上下文区域标记</th>
<th>标注序列</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>set</td>
<td>n't been set . ×</td>
<td>0</td>
<td>B-A1</td>
</tr>
<tr>
<td>record</td>
<td>set</td>
<td>n't been set . ×</td>
<td>0</td>
<td>I-A1</td>
</tr>
<tr>
<td>date</td>
<td>set</td>
<td>n't been set . ×</td>
<td>0</td>
<td>I-A1</td>
</tr>
<tr>
<td>has</td>
<td>set</td>
<td>n't been set . ×</td>
<td>0</td>
<td>O</td>
</tr>
<tr>
<td>n't</td>
<td>set</td>
<td>n't been set . ×</td>
<td>1</td>
<td>B-AM-NEG</td>
</tr>
<tr>
<td>been</td>
<td>set</td>
<td>n't been set . ×</td>
<td>1</td>
<td>O</td>
</tr>
<tr>
<td>set</td>
<td>set</td>
<td>n't been set . ×</td>
<td>1</td>
<td>B-V</td>
</tr>
<tr>
<td>.</td>
<td>set</td>
<td>n't been set . ×</td>
<td>1</td>
<td>O</td>
</tr>
</tbody>
</table>
<p>除数据之外，我们同时提供了以下资源：</p>
<table>
<thead>
<tr>
<th>文件名称</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>word_dict</td>
<td>输入句子的词典，共计44068个词</td>
</tr>
<tr>
<td>label_dict</td>
<td>标记的词典，共计106个标记</td>
</tr>
<tr>
<td>predicate_dict</td>
<td>谓词的词典，共计3162个词</td>
</tr>
<tr>
<td>emb</td>
<td>一个训练好的词表，32维</td>
</tr>
</tbody>
</table>
<p>我们在英文维基百科上训练语言模型得到了一份词向量用来初始化SRL模型。在SRL模型训练过程中，词向量不再被更新。关于语言模型和词向量可以参考<a href="https://github.com/PaddlePaddle/book/blob/develop/04.word2vec/README.cn.md">词向量</a> 这篇教程。我们训练语言模型的语料共有995,000,000个token，词典大小控制为4900,000词。CoNLL 2005训练语料中有5%的词不在这4900,000个词中，我们将它们全部看作未登录词，用<code>&lt;unk&gt;</code>表示。</p>
<p>获取词典，打印词典大小：</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">paddle.v2</span> <span class="kn">as</span> <span class="nn">paddle</span>
<span class="kn">import</span> <span class="nn">paddle.v2.dataset.conll05</span> <span class="kn">as</span> <span class="nn">conll05</span>
<span class="kn">import</span> <span class="nn">paddle.v2.evaluator</span> <span class="kn">as</span> <span class="nn">evaluator</span>

<span class="n">paddle</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">trainer_count</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">word_dict</span><span class="p">,</span> <span class="n">verb_dict</span><span class="p">,</span> <span class="n">label_dict</span> <span class="o">=</span> <span class="n">conll05</span><span class="o">.</span><span class="n">get_dict</span><span class="p">()</span>
<span class="n">word_dict_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_dict</span><span class="p">)</span>
<span class="n">label_dict_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">label_dict</span><span class="p">)</span>
<span class="n">pred_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">verb_dict</span><span class="p">)</span>

<span class="k">print</span> <span class="n">word_dict_len</span>
<span class="k">print</span> <span class="n">label_dict_len</span>
<span class="k">print</span> <span class="n">pred_len</span>
</pre></div>
<h2>模型配置说明</h2>
<ul>
<li>定义输入数据维度及模型超参数。</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">mark_dict_len</span> <span class="o">=</span> <span class="mi">2</span>    <span class="c1"># 谓上下文区域标志的维度，是一个0-1 2值特征，因此维度为2</span>
<span class="n">word_dim</span> <span class="o">=</span> <span class="mi">32</span>        <span class="c1"># 词向量维度</span>
<span class="n">mark_dim</span> <span class="o">=</span> <span class="mi">5</span>         <span class="c1"># 谓词上下文区域通过词表被映射为一个实向量，这个是相邻的维度</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">512</span>     <span class="c1"># LSTM隐层向量的维度 ： 512 / 4</span>
<span class="n">depth</span> <span class="o">=</span> <span class="mi">8</span>            <span class="c1"># 栈式LSTM的深度</span>

<span class="c1"># 一条样本总共9个特征，下面定义了9个data层，每个层类型为integer_value_sequence，表示整数ID的序列类型.</span>
<span class="k">def</span> <span class="nf">d_type</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">paddle</span><span class="o">.</span><span class="n">data_type</span><span class="o">.</span><span class="n">integer_value_sequence</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>

<span class="c1"># 句子序列</span>
<span class="n">word</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'word_data'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">d_type</span><span class="p">(</span><span class="n">word_dict_len</span><span class="p">))</span>
<span class="c1"># 谓词</span>
<span class="n">predicate</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'verb_data'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">d_type</span><span class="p">(</span><span class="n">pred_len</span><span class="p">))</span>

<span class="c1"># 谓词上下文5个特征</span>
<span class="n">ctx_n2</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'ctx_n2_data'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">d_type</span><span class="p">(</span><span class="n">word_dict_len</span><span class="p">))</span>
<span class="n">ctx_n1</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'ctx_n1_data'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">d_type</span><span class="p">(</span><span class="n">word_dict_len</span><span class="p">))</span>
<span class="n">ctx_0</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'ctx_0_data'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">d_type</span><span class="p">(</span><span class="n">word_dict_len</span><span class="p">))</span>
<span class="n">ctx_p1</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'ctx_p1_data'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">d_type</span><span class="p">(</span><span class="n">word_dict_len</span><span class="p">))</span>
<span class="n">ctx_p2</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'ctx_p2_data'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">d_type</span><span class="p">(</span><span class="n">word_dict_len</span><span class="p">))</span>

<span class="c1"># 谓词上下区域标志</span>
<span class="n">mark</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'mark_data'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">d_type</span><span class="p">(</span><span class="n">mark_dict_len</span><span class="p">))</span>

<span class="c1"># 标注序列</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'target'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">d_type</span><span class="p">(</span><span class="n">label_dict_len</span><span class="p">))</span>
</pre></div>
<p>这里需要特别说明的是hidden_dim = 512指定了LSTM隐层向量的维度为128维，关于这一点请参考PaddlePaddle官方文档中<a href="http://www.paddlepaddle.org/doc/ui/api/trainer_config_helpers/layers.html#lstmemory">lstmemory</a>的说明。</p>
<ul>
<li>将句子序列、谓词、谓词上下文、谓词上下文区域标记通过词表，转换为实向量表示的词向量序列。</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># 在本教程中，我们加载了预训练的词向量，这里设置了：is_static=True</span>
<span class="c1"># is_static 为 True 时保证了在训练 SRL 模型过程中，词表不再更新</span>
<span class="n">emb_para</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'emb'</span><span class="p">,</span> <span class="n">initial_std</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">is_static</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># 设置超参数</span>
<span class="n">default_std</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">3.0</span>
<span class="n">std_default</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">initial_std</span><span class="o">=</span><span class="n">default_std</span><span class="p">)</span>
<span class="n">std_0</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">initial_std</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>

<span class="n">predicate_embedding</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span>
    <span class="n">size</span><span class="o">=</span><span class="n">word_dim</span><span class="p">,</span>
    <span class="nb">input</span><span class="o">=</span><span class="n">predicate</span><span class="p">,</span>
    <span class="n">param_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">'vemb'</span><span class="p">,</span> <span class="n">initial_std</span><span class="o">=</span><span class="n">default_std</span><span class="p">))</span>
<span class="n">mark_embedding</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span>
    <span class="n">size</span><span class="o">=</span><span class="n">mark_dim</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">mark</span><span class="p">,</span> <span class="n">param_attr</span><span class="o">=</span><span class="n">std_0</span><span class="p">)</span>

<span class="n">word_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="p">,</span> <span class="n">ctx_n2</span><span class="p">,</span> <span class="n">ctx_n1</span><span class="p">,</span> <span class="n">ctx_0</span><span class="p">,</span> <span class="n">ctx_p1</span><span class="p">,</span> <span class="n">ctx_p2</span><span class="p">]</span>
<span class="n">emb_layers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span>
        <span class="n">size</span><span class="o">=</span><span class="n">word_dim</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">param_attr</span><span class="o">=</span><span class="n">emb_para</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">word_input</span>
<span class="p">]</span>
<span class="n">emb_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predicate_embedding</span><span class="p">)</span>
<span class="n">emb_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mark_embedding</span><span class="p">)</span>
</pre></div>
<ul>
<li>8个LSTM单元以“正向/反向”的顺序对所有输入序列进行学习。</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">hidden_0</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">mixed</span><span class="p">(</span>
    <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
    <span class="n">bias_attr</span><span class="o">=</span><span class="n">std_default</span><span class="p">,</span>
    <span class="nb">input</span><span class="o">=</span><span class="p">[</span>
        <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">full_matrix_projection</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span> <span class="n">param_attr</span><span class="o">=</span><span class="n">std_default</span><span class="p">)</span> <span class="k">for</span> <span class="n">emb</span> <span class="ow">in</span> <span class="n">emb_layers</span>
    <span class="p">])</span>

<span class="n">mix_hidden_lr</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">lstm_para_attr</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">initial_std</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">hidden_para_attr</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span>
    <span class="n">initial_std</span><span class="o">=</span><span class="n">default_std</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">mix_hidden_lr</span><span class="p">)</span>

<span class="n">lstm_0</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">lstmemory</span><span class="p">(</span>
    <span class="nb">input</span><span class="o">=</span><span class="n">hidden_0</span><span class="p">,</span>
    <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Relu</span><span class="p">(),</span>
    <span class="n">gate_act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">state_act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">bias_attr</span><span class="o">=</span><span class="n">std_0</span><span class="p">,</span>
    <span class="n">param_attr</span><span class="o">=</span><span class="n">lstm_para_attr</span><span class="p">)</span>

<span class="c1">#stack L-LSTM and R-LSTM with direct edges</span>
<span class="n">input_tmp</span> <span class="o">=</span> <span class="p">[</span><span class="n">hidden_0</span><span class="p">,</span> <span class="n">lstm_0</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
    <span class="n">mix_hidden</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">mixed</span><span class="p">(</span>
        <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
        <span class="n">bias_attr</span><span class="o">=</span><span class="n">std_default</span><span class="p">,</span>
        <span class="nb">input</span><span class="o">=</span><span class="p">[</span>
            <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">full_matrix_projection</span><span class="p">(</span>
                <span class="nb">input</span><span class="o">=</span><span class="n">input_tmp</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">param_attr</span><span class="o">=</span><span class="n">hidden_para_attr</span><span class="p">),</span>
            <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">full_matrix_projection</span><span class="p">(</span>
                <span class="nb">input</span><span class="o">=</span><span class="n">input_tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">param_attr</span><span class="o">=</span><span class="n">lstm_para_attr</span><span class="p">)</span>
        <span class="p">])</span>

    <span class="n">lstm</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">lstmemory</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">mix_hidden</span><span class="p">,</span>
        <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Relu</span><span class="p">(),</span>
        <span class="n">gate_act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
        <span class="n">state_act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
        <span class="n">reverse</span><span class="o">=</span><span class="p">((</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">bias_attr</span><span class="o">=</span><span class="n">std_0</span><span class="p">,</span>
        <span class="n">param_attr</span><span class="o">=</span><span class="n">lstm_para_attr</span><span class="p">)</span>

    <span class="n">input_tmp</span> <span class="o">=</span> <span class="p">[</span><span class="n">mix_hidden</span><span class="p">,</span> <span class="n">lstm</span><span class="p">]</span>
</pre></div>
<ul>
<li>在PaddlePaddle中，CRF的状态特征和转移特征分别由一个全连接层和一个PaddlePaddle中的CRF层分别学习。在这个例子中，我们用线性激活的paddle.layer.mixed 来学习CRF的状态特征（也可以使用paddle.layer.fc），而 paddle.layer.crf只学习转移特征。paddle.layer.crf层是一个 cost 层，处于整个网络的末端，输出给定输入序列下，标记序列的log probability作为代价。训练阶段，该层需要输入正确的标记序列作为学习目标。</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># 取最后一个栈式LSTM的输出和这个LSTM单元的输入到隐层映射，</span>
<span class="c1"># 经过一个全连接层映射到标记字典的维度，来学习 CRF 的状态特征</span>

<span class="n">feature_out</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">mixed</span><span class="p">(</span>
    <span class="n">size</span><span class="o">=</span><span class="n">label_dict_len</span><span class="p">,</span>
    <span class="n">bias_attr</span><span class="o">=</span><span class="n">std_default</span><span class="p">,</span>
    <span class="nb">input</span><span class="o">=</span><span class="p">[</span>
        <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">full_matrix_projection</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">input_tmp</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">param_attr</span><span class="o">=</span><span class="n">hidden_para_attr</span><span class="p">),</span>
        <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">full_matrix_projection</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">input_tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">param_attr</span><span class="o">=</span><span class="n">lstm_para_attr</span><span class="p">)</span>
    <span class="p">],</span> <span class="p">)</span>

<span class="c1"># 学习 CRF 的转移特征</span>
<span class="n">crf_cost</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">crf</span><span class="p">(</span>
    <span class="n">size</span><span class="o">=</span><span class="n">label_dict_len</span><span class="p">,</span>
    <span class="nb">input</span><span class="o">=</span><span class="n">feature_out</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
    <span class="n">param_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">'crfw'</span><span class="p">,</span>
        <span class="n">initial_std</span><span class="o">=</span><span class="n">default_std</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">mix_hidden_lr</span><span class="p">))</span>
</pre></div>
<ul>
<li>CRF解码和CRF层参数名字相同，即：加载了<code>paddle.layer.crf</code>层学习到的参数。在训练阶段，为<code>paddle.layer.crf_decoding</code> 输入了正确的标记序列(target)，这一层会输出是否正确标记，<code>evaluator.sum</code> 用来计算序列上的标记错误率，可以用来评估模型。解码阶段，没有输入正确的数据标签，该层通过寻找概率最高的标记序列，解码出标记结果。</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">crf_dec</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">crf_decoding</span><span class="p">(</span>
   <span class="n">size</span><span class="o">=</span><span class="n">label_dict_len</span><span class="p">,</span>
   <span class="nb">input</span><span class="o">=</span><span class="n">feature_out</span><span class="p">,</span>
   <span class="n">label</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
   <span class="n">param_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'crfw'</span><span class="p">))</span>
<span class="n">evaluator</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">crf_dec</span><span class="p">)</span>
</pre></div>
<h2>训练模型</h2>
<h3>定义参数</h3>
<p>首先依据模型配置的<code>crf_cost</code>定义模型参数。</p>
<div class="highlight"><pre><span></span><span class="c1"># create parameters</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">crf_cost</span><span class="p">)</span>
</pre></div>
<p>可以打印参数名字，如果在网络配置中没有指定名字，则默认生成。</p>
<div class="highlight"><pre><span></span><span class="k">print</span> <span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
<p>如上文提到，我们用基于英文维基百科训练好的词向量来初始化序列输入、谓词上下文总共6个特征的embedding层参数，在训练中不更新。</p>
<div class="highlight"><pre><span></span><span class="c1"># 这里加载PaddlePaddle上版保存的二进制模型</span>
<span class="k">def</span> <span class="nf">load_parameter</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">parameters</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s1">'emb'</span><span class="p">,</span> <span class="n">load_parameter</span><span class="p">(</span><span class="n">conll05</span><span class="o">.</span><span class="n">get_embedding</span><span class="p">(),</span> <span class="mi">44068</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
</pre></div>
<h3>构造训练(Trainer)</h3>
<p>然后根据网络拓扑结构和模型参数来构造出trainer用来训练，在构造时还需指定优化方法，这里使用最基本的SGD方法(momentum设置为0)，同时设定了学习率、正则等。</p>
<div class="highlight"><pre><span></span><span class="c1"># create optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">regularization</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">L2Regularization</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">8e-4</span><span class="p">),</span>
    <span class="n">model_average</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">ModelAverage</span><span class="p">(</span>
        <span class="n">average_window</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_average_window</span><span class="o">=</span><span class="mi">10000</span><span class="p">),</span> <span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">crf_cost</span><span class="p">,</span>
                             <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                             <span class="n">update_equation</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                             <span class="n">extra_layers</span><span class="o">=</span><span class="n">crf_dec</span><span class="p">)</span>
</pre></div>
<h3>训练</h3>
<p>数据介绍部分提到CoNLL 2005训练集付费，这里我们使用测试集训练供大家学习。<code>conll05.test()</code>每次产生一条样本，包含9个特征，shuffle和组完batch后作为训练的输入。</p>
<div class="highlight"><pre><span></span><span class="n">reader</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span>
    <span class="n">paddle</span><span class="o">.</span><span class="n">reader</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span>
        <span class="n">conll05</span><span class="o">.</span><span class="n">test</span><span class="p">(),</span> <span class="n">buf_size</span><span class="o">=</span><span class="mi">8192</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<p>通过<code>feeding</code>来指定每一个数据和data_layer的对应关系。 例如 下面<code>feeding</code>表示: <code>conll05.test()</code>产生数据的第0列对应<code>word_data</code>层的特征。</p>
<div class="highlight"><pre><span></span><span class="n">feeding</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'word_data'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">'ctx_n2_data'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">'ctx_n1_data'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">'ctx_0_data'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s1">'ctx_p1_data'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s1">'ctx_p2_data'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s1">'verb_data'</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="s1">'mark_data'</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
    <span class="s1">'target'</span><span class="p">:</span> <span class="mi">8</span>
<span class="p">}</span>
</pre></div>
<p>可以使用<code>event_handler</code>回调函数来观察训练过程，或进行测试等。这里我们打印了训练过程的cost，该回调函数是<code>trainer.train</code>函数里设定。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">event_handler</span><span class="p">(</span><span class="n">event</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">paddle</span><span class="o">.</span><span class="n">event</span><span class="o">.</span><span class="n">EndIteration</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">event</span><span class="o">.</span><span class="n">batch_id</span> <span class="ow">and</span> <span class="n">event</span><span class="o">.</span><span class="n">batch_id</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="s2">"Pass </span><span class="si">%d</span><span class="s2">, Batch </span><span class="si">%d</span><span class="s2">, Cost </span><span class="si">%f</span><span class="s2">, </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span>
                <span class="n">event</span><span class="o">.</span><span class="n">pass_id</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">batch_id</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">cost</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">event</span><span class="o">.</span><span class="n">batch_id</span> <span class="o">%</span> <span class="mi">400</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">reader</span><span class="o">=</span><span class="n">reader</span><span class="p">,</span> <span class="n">feeding</span><span class="o">=</span><span class="n">feeding</span><span class="p">)</span>
            <span class="k">print</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Test with Pass </span><span class="si">%d</span><span class="s2">, Batch </span><span class="si">%d</span><span class="s2">, </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">event</span><span class="o">.</span><span class="n">pass_id</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">batch_id</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">paddle</span><span class="o">.</span><span class="n">event</span><span class="o">.</span><span class="n">EndPass</span><span class="p">):</span>
        <span class="c1"># save parameters</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">'params_pass_</span><span class="si">%d</span><span class="s1">.tar'</span> <span class="o">%</span> <span class="n">event</span><span class="o">.</span><span class="n">pass_id</span><span class="p">,</span> <span class="s1">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">save_parameter_to_tar</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">reader</span><span class="o">=</span><span class="n">reader</span><span class="p">,</span> <span class="n">feeding</span><span class="o">=</span><span class="n">feeding</span><span class="p">)</span>
        <span class="k">print</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Test with Pass </span><span class="si">%d</span><span class="s2">, </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">event</span><span class="o">.</span><span class="n">pass_id</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
</pre></div>
<p>通过<code>trainer.train</code>函数训练：</p>
<div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">reader</span><span class="o">=</span><span class="n">reader</span><span class="p">,</span>
    <span class="n">event_handler</span><span class="o">=</span><span class="n">event_handler</span><span class="p">,</span>
    <span class="n">num_passes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">feeding</span><span class="o">=</span><span class="n">feeding</span><span class="p">)</span>
</pre></div>
<h3>应用模型</h3>
<p>训练完成之后，需要依据某个我们关心的性能指标选择最优的模型进行预测，可以简单的选择测试集上标记错误最少的那个模型。预测时使用 <code>paddle.layer.crf_decoding</code>，和训练不同的是，该层没有正确的标签层作为输入。如下所示：</p>
<div class="highlight"><pre><span></span><span class="n">predict</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">crf_decoding</span><span class="p">(</span>
    <span class="n">size</span><span class="o">=</span><span class="n">label_dict_len</span><span class="p">,</span>
    <span class="nb">input</span><span class="o">=</span><span class="n">feature_out</span><span class="p">,</span>
    <span class="n">param_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'crfw'</span><span class="p">))</span>
</pre></div>
<p>这里选用测试集的一条数据作为示例。</p>
<div class="highlight"><pre><span></span><span class="n">test_creator</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">conll05</span><span class="o">.</span><span class="n">test</span><span class="p">()</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">test_creator</span><span class="p">():</span>
    <span class="n">test_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
<p>推断接口<code>paddle.infer</code>返回标签的索引，并查询词典<code>labels_reverse</code>，打印出标记的结果。</p>
<div class="highlight"><pre><span></span><span class="n">labs</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span>
    <span class="n">output_layer</span><span class="o">=</span><span class="n">predict</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">test_data</span><span class="p">,</span> <span class="n">field</span><span class="o">=</span><span class="s1">'id'</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">labs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">labels_reverse</span><span class="o">=</span><span class="p">{}</span>
<span class="k">for</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">)</span> <span class="ow">in</span> <span class="n">label_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">labels_reverse</span><span class="p">[</span><span class="n">v</span><span class="p">]</span><span class="o">=</span><span class="n">k</span>
<span class="n">pre_lab</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels_reverse</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">labs</span><span class="p">]</span>
<span class="k">print</span> <span class="n">pre_lab</span>
</pre></div>
<h2>总结</h2>
<p>语义角色标注是许多自然语言理解任务的重要中间步骤。这篇教程中我们以语义角色标注任务为例，介绍如何利用PaddlePaddle进行序列标注任务。教程中所介绍的模型来自我们发表的论文[<a href="#参考文献">10</a>]。由于 CoNLL 2005 SRL任务的训练数据目前并非完全开放，教程中只使用测试数据作为示例。在这个过程中，我们希望减少对其它自然语言处理工具的依赖，利用神经网络数据驱动、端到端学习的能力，得到一个和传统方法可比、甚至更好的模型。在论文中我们证实了这种可能性。关于模型更多的信息和讨论可以在论文中找到。</p>
<h2>参考文献</h2>
<ol>
<li>Sun W, Sui Z, Wang M, et al. <a href="http://www.aclweb.org/anthology/D09-1#page=1513">Chinese semantic role labeling with shallow parsing</a>[C]//Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3. Association for Computational Linguistics, 2009: 1475-1483.</li>
<li>Pascanu R, Gulcehre C, Cho K, et al. <a href="https://arxiv.org/abs/1312.6026">How to construct deep recurrent neural networks</a>[J]. arXiv preprint arXiv:1312.6026, 2013.</li>
<li>Cho K, Van Merriënboer B, Gulcehre C, et al. <a href="https://arxiv.org/abs/1406.1078">Learning phrase representations using RNN encoder-decoder for statistical machine translation</a>[J]. arXiv preprint arXiv:1406.1078, 2014.</li>
<li>Bahdanau D, Cho K, Bengio Y. <a href="https://arxiv.org/abs/1409.0473">Neural machine translation by jointly learning to align and translate</a>[J]. arXiv preprint arXiv:1409.0473, 2014.</li>
<li>Lafferty J, McCallum A, Pereira F. <a href="http://www.jmlr.org/papers/volume15/doppa14a/source/biblio.bib.old">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</a>[C]//Proceedings of the eighteenth international conference on machine learning, ICML. 2001, 1: 282-289.</li>
<li>李航. 统计学习方法[J]. 清华大学出版社, 北京, 2012.</li>
<li>Marcus M P, Marcinkiewicz M A, Santorini B. <a href="http://repository.upenn.edu/cgi/viewcontent.cgi?article=1246&amp;context=cis_reports">Building a large annotated corpus of English: The Penn Treebank</a>[J]. Computational linguistics, 1993, 19(2): 313-330.</li>
<li>Palmer M, Gildea D, Kingsbury P. <a href="http://www.mitpressjournals.org/doi/pdfplus/10.1162/0891201053630264">The proposition bank: An annotated corpus of semantic roles</a>[J]. Computational linguistics, 2005, 31(1): 71-106.</li>
<li>Carreras X, Màrquez L. <a href="http://www.cs.upc.edu/~srlconll/st05/papers/intro.pdf">Introduction to the CoNLL-2005 shared task: Semantic role labeling</a>[C]//Proceedings of the Ninth Conference on Computational Natural Language Learning. Association for Computational Linguistics, 2005: 152-164.</li>
<li>Zhou J, Xu W. <a href="http://www.aclweb.org/anthology/P/P15/P15-1109.pdf">End-to-end learning of semantic role labeling using recurrent neural networks</a>[C]//Proceedings of the Annual Meeting of the Association for Computational Linguistics. 2015.</li>
</ol>
<p><br/>
<a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license"><img alt="知识共享许可协议" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="border-width:0"/></a><br/><span href="http://purl.org/dc/dcmitype/Text" property="dct:title" rel="dct:type" xmlns:dct="http://purl.org/dc/terms/">本教程</span> 由 <a href="http://book.paddlepaddle.org" property="cc:attributionName" rel="cc:attributionURL" xmlns:cc="http://creativecommons.org/ns#">PaddlePaddle</a> 创作，采用 <a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license">知识共享 署名-相同方式共享 4.0 国际 许可协议</a>进行许可。</p>
{% endverbatim %}