<html><body><p>{% verbatim %}
</p><p>This tutorial introduces techniques we use to profile and tune the
CPU performance of PaddlePaddle.  We will use Python packages
<code>cProfile</code> and <code>yep</code>, and Google's <code>perftools</code>.</p>
<p>Profiling is the process that reveals performance bottlenecks,
which could be very different from what's in the developers' mind.
Performance tuning is done to fix these bottlenecks. Performance optimization
repeats the steps of profiling and tuning alternatively.</p>
<p>PaddlePaddle users program AI applications by calling the Python API, which calls
into <code>libpaddle.so.</code> written in C++.  In this tutorial, we focus on
the profiling and tuning of</p>
<ol>
<li>the Python code and</li>
<li>the mixture of Python and C++ code.</li>
</ol>
<h1>Profiling the Python Code</h1>
<h3>Generate the Performance Profiling File</h3>
<p>We can use Python standard
package, <a href="https://docs.python.org/2/library/profile.html"><code>cProfile</code></a>,
to generate Python profiling file.  For example:</p>
<div class="highlight"><pre><span></span>python -m cProfile -o profile.out main.py
</pre></div>
<p>where <code>main.py</code> is the program we are going to profile, <code>-o</code> specifies
the output file.  Without <code>-o</code>, <code>cProfile</code> would outputs to standard
output.</p>
<h3>Look into the Profiling File</h3>
<p><code>cProfile</code> generates <code>profile.out</code> after <code>main.py</code> completes. We can
use <a href="https://github.com/ymichael/cprofilev"><code>cprofilev</code></a> to look into
the details:</p>
<div class="highlight"><pre><span></span>cprofilev -a <span class="m">0</span>.0.0.0 -p <span class="m">3214</span> -f profile.out main.py
</pre></div>
<p>where <code>-a</code> specifies the HTTP IP, <code>-p</code> specifies the port, <code>-f</code>
specifies the profiling file, and <code>main.py</code> is the source file.</p>
<p>Open the Web browser and points to the local IP and the specifies
port, we will see the output like the following:</p>
<div class="highlight"><pre><span></span>   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.284    0.284   29.514   29.514 main.py:1(&lt;module&gt;)
     4696    0.128    0.000   15.748    0.003 /home/yuyang/perf_test/.env/lib/python2.7/site-packages/paddle/fluid/executor.py:20(run)
     4696   12.040    0.003   12.040    0.003 {built-in method run}
        1    0.144    0.144    6.534    6.534 /home/yuyang/perf_test/.env/lib/python2.7/site-packages/paddle/v2/__init__.py:14(&lt;module&gt;)
</pre></div>
<p>where each line corresponds to Python function, and the meaning of
each column is as follows:</p>
<table>
<thead>
<tr>
<th>column</th>
<th>meaning </th>
</tr>
</thead>
<tbody>
<tr>
<td> ncalls</td>
<td> the number of calls into a function</td>
</tr>
<tr>
<td>tottime</td>
<td> the total execution time of the function, not including the execution time of other functions called by the function</td>
</tr>
<tr>
<td> percall </td>
<td> tottime divided by ncalls</td>
</tr>
<tr>
<td> cumtime</td>
<td> the total execution time of the function, including the execution time of other functions being called</td>
</tr>
<tr>
<td> percall</td>
<td> cumtime divided by ncalls</td>
</tr>
<tr>
<td> filename:lineno(function) </td>
<td> where the function is define </td>
</tr>
</tbody>
</table>
<h3>Identify Performance Bottlenecks</h3>
<p>Usually, <code>tottime</code> and the related <code>percall</code> time is what we want to
focus on. We can sort above profiling file by tottime:</p>
<div class="highlight"><pre><span></span>     4696   12.040    0.003   12.040    0.003 {built-in method run}
   300005    0.874    0.000    1.681    0.000 /home/yuyang/perf_test/.env/lib/python2.7/site-packages/paddle/v2/dataset/mnist.py:38(reader)
   107991    0.676    0.000    1.519    0.000 /home/yuyang/perf_test/.env/lib/python2.7/site-packages/paddle/fluid/framework.py:219(__init__)
     4697    0.626    0.000    2.291    0.000 /home/yuyang/perf_test/.env/lib/python2.7/site-packages/paddle/fluid/framework.py:428(sync_with_cpp)
        1    0.618    0.618    0.618    0.618 /home/yuyang/perf_test/.env/lib/python2.7/site-packages/paddle/fluid/__init__.py:1(&lt;module&gt;)
</pre></div>
<p>We can see that the most time-consuming function is the <code>built-in
method run</code>, which is a C++ function in <code>libpaddle.so</code>.  We will
explain how to profile C++ code in the next section.  At this
moment, let's look into the third function <code>sync_with_cpp</code>, which is a
Python function.  We can click it to understand more about it:</p>
<div class="highlight"><pre><span></span>Called By:

   Ordered by: internal time
   List reduced from 4497 to 2 due to restriction &lt;'sync_with_cpp'&gt;

Function                                                                                                 was called by...
                                                                                                             ncalls  tottime  cumtime
/home/yuyang/perf_test/.env/lib/python2.7/site-packages/paddle/fluid/framework.py:428(sync_with_cpp)  &lt;-    4697    0.626    2.291  /home/yuyang/perf_test/.env/lib/python2.7/site-packages/paddle/fluid/framework.py:562(sync_with_cpp)
/home/yuyang/perf_test/.env/lib/python2.7/site-packages/paddle/fluid/framework.py:562(sync_with_cpp)  &lt;-    4696    0.019    2.316  /home/yuyang/perf_test/.env/lib/python2.7/site-packages/paddle/fluid/framework.py:487(clone)
                                                                                                                  1    0.000    0.001  /home/yuyang/perf_test/.env/lib/python2.7/site-packages/paddle/fluid/framework.py:534(append_backward)


Called:

   Ordered by: internal time
   List reduced from 4497 to 2 due to restriction &lt;'sync_with_cpp'&gt;
</pre></div>
<p>The lists of the callers of <code>sync_with_cpp</code> might help us understand
how to improve the function definition.</p>
<h2>Profiling Python and C++ Code</h2>
<h3>Generate the Profiling File</h3>
<p>To profile a mixture of Python and C++ code, we can use a Python
package, <code>yep</code>, that can work with Google's <code>perftools</code>, which is a
commonly-used profiler for C/C++ code.</p>
<p>In Ubuntu systems, we can install <code>yep</code> and <code>perftools</code> by running the
following commands:</p>
<div class="highlight"><pre><span></span>apt update
apt install libgoogle-perftools-dev
pip install yep
</pre></div>
<p>Then we can run the following command</p>
<div class="highlight"><pre><span></span>python -m yep -v main.py
</pre></div>
<p>to generate the profiling file.  The default filename is
<code>main.py.prof</code>.</p>
<p>Please be aware of the <code>-v</code> command line option, which prints the
analysis results after generating the profiling file.  By examining the
 the print result, we'd know that if we stripped debug
information from <code>libpaddle.so</code> at build time.  The following hints
help make sure that the analysis results are readable:</p>
<ol>
<li>
<p>Use GCC command line option <code>-g</code> when building <code>libpaddle.so</code> so to
   include the debug information.  The standard building system of
   PaddlePaddle is CMake, so you might want to set
   <code>CMAKE_BUILD_TYPE=RelWithDebInfo</code>.</p>
</li>
<li>
<p>Use GCC command line option <code>-O2</code> or <code>-O3</code> to generate optimized
   binary code. It doesn't make sense to profile <code>libpaddle.so</code>
   without optimization, because it would anyway run slowly.</p>
</li>
<li>
<p>Profiling the single-threaded binary file before the
   multi-threading version, because the latter often generates tangled
   profiling analysis result.  You might want to set environment
   variable <code>OMP_NUM_THREADS=1</code> to prevents OpenMP from automatically
   starting multiple threads.</p>
</li>
</ol>
<h3>Examining the Profiling File</h3>
<p>The tool we used to examine the profiling file generated by
<code>perftools</code> is <a href="https://github.com/google/pprof"><code>pprof</code></a>, which
provides a Web-based GUI like <code>cprofilev</code>.</p>
<p>We can rely on the standard Go toolchain to retrieve the source code
of <code>pprof</code> and build it:</p>
<div class="highlight"><pre><span></span>go get github.com/google/pprof
</pre></div>
<p>Then we can use it to profile <code>main.py.prof</code> generated in the previous
section:</p>
<div class="highlight"><pre><span></span>pprof -http<span class="o">=</span><span class="m">0</span>.0.0.0:3213 <span class="sb">`</span>which python<span class="sb">`</span>  ./main.py.prof
</pre></div>
<p>Where <code>-http</code> specifies the IP and port of the HTTP service.
Directing our Web browser to the service, we would see something like
the following:</p>
<p><img alt="result" src="../../../../_images/pprof_1.png"/></p>
<h3>Identifying the Performance Bottlenecks</h3>
<p>Similar to how we work with <code>cprofilev</code>, we'd focus on <code>tottime</code> and
<code>cumtime</code>.</p>
<p><img alt="kernel_perf" src="../../../../_images/pprof_2.png"/></p>
<p>We can see that the execution time of multiplication and the computing
of the gradient of multiplication takes 2% to 4% of the total running
time, and <code>MomentumOp</code> takes about 17%. Obviously, we'd want to
optimize <code>MomentumOp</code>.</p>
<p><code>pprof</code> would mark performance critical parts of the program in
red. It's a good idea to follow the hints.</p>
{% endverbatim %}</body></html>