{% verbatim %}
<h1>Optimizer Design</h1>
<h2>The Problem</h2>
<p>A PaddlePaddle program, or a block, is a sequence of operators operating variables.  A training program needs to do three kinds of works:</p>
<ol>
<li>the forward pass, which computes intermediate results and the cost(s),</li>
<li>the backward pass, which derives gradients from intermediate results and costs, and</li>
<li>the optimization pass, which update model parameters to optimize the cost(s).</li>
</ol>
<p>These works rely on three kinds of operators:</p>
<ol>
<li>forward operators,</li>
<li>gradient operators, and</li>
<li>optimization operators.</li>
</ol>
<p>It's true that users should be able to create all these operators manually by calling some low-level API, but it would be much more convenient if they could only describe the forward pass and let PaddlePaddle create the backward and optimization operators automatically.</p>
<p>In this design, we propose a high-level API that automatically derives the optimisation pass and operators from the forward pass.</p>
<h2>High-level Python API to describe the training process</h2>
<ol>
<li>
<p>User write code to describe the network:</p>
<div class="highlight"><pre><span></span><span class="n">images</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="s2">&quot;images&quot;</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">)</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;w1&quot;</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;b1&quot;</span><span class="p">)</span>
<span class="n">hidden</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="n">w1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b1</span><span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>

<p>The above code snippet will create forward operators in <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/design/block.md">Block</a>.</p>
</li>
<li>
<p>Users create a certain kind of Optimizer with some argument.</p>
<div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdagradOptimizer</span><span class="p">(</span><span class="n">learing_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>

</li>
<li>
<p>Users use the optimizer to <code>minimize</code> a certain <code>cost</code> through updating parameters in parameter_list.</p>
<p><div class="highlight"><pre><span></span><span class="n">opt_op_list</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">parameter_list</span><span class="o">=</span><span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">])</span>
</pre></div>
The above code snippet will create gradient and optimization operators in Block. The return value of <code>minimize()</code> is list of optimization operators that will be run by session.</p>
</li>
<li>
<p>Users use Session/Executor to run this opt_op_list as target to do training.</p>
<div class="highlight"><pre><span></span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">target</span><span class="o">=</span> <span class="n">opt_op_list</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>

</li>
</ol>
<h3>Optimizer Python interface:</h3>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Optimizer Base class.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">create_optimization_pass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters_and_grads</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Add optimization operators to update gradients to variables.</span>

<span class="sd">        Args:</span>
<span class="sd">          parameters_and_grads: a list of (variable, gradient) pair to update.</span>

<span class="sd">        Returns:</span>
<span class="sd">          optmization_op_list: a list of optimization operator that will update parameter using gradient.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">minimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">parameter_list</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Add operations to minimize `loss` by updating `parameter_list`.</span>

<span class="sd">        This method combines interface `append_backward()` and</span>
<span class="sd">        `create_optimization_pass()` into one.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">params_grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_backward_pass</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">parameter_list</span><span class="p">)</span>
        <span class="n">update_ops</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_optimization_pass</span><span class="p">(</span><span class="n">params_grads</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">update_ops</span>
</pre></div>

<p>Users can inherit the Optimizer above to create their own Optimizer with some special logic, such as AdagradOptimizer.</p>
{% endverbatim %}