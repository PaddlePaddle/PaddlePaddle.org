<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="distributed-training-with-nccl2-and-rdma">
<span id="distributed-training-with-nccl2-and-rdma"></span><h1>Distributed Training with NCCL2 and RDMA<a class="headerlink" href="#distributed-training-with-nccl2-and-rdma" title="永久链接至标题">¶</a></h1>
<p>When doing distributed multi-GPU training, network bandwith often becomes the
bottle neck. We introduce a way to use NCCL2 to do such training job to
achieve best performace.</p>
<div class="section" id="prepare-hardwares-with-rdma-and-multiple-gpus">
<span id="prepare-hardwares-with-rdma-and-multiple-gpus"></span><h2>Prepare Hardwares with RDMA and Multiple GPUs<a class="headerlink" href="#prepare-hardwares-with-rdma-and-multiple-gpus" title="永久链接至标题">¶</a></h2>
<p>I’m using two Linux servers each of them is installed with 8 GPUs and
one 100Gb RDMA card.
Base environment is:</p>
<ul class="simple">
<li>OS: CentOS 7.4</li>
<li>RDMA device: “Mellanox Technologies MT27700 Family [ConnectX-4]“</li>
<li>Kernel version: <code class="docutils literal"><span class="pre">4.4.88-1.el7.elrepo.x86_64</span></code></li>
<li>Docker version: <code class="docutils literal"><span class="pre">1.12.6</span></code></li>
<li>Docker storage driver: <code class="docutils literal"><span class="pre">overlay2</span></code></li>
<li>IP addresses: 192.168.16.30,192.168.16.34</li>
</ul>
<p>In general, the steps including:</p>
<ol class="simple">
<li>Install GPU drivers</li>
<li>Install RDMA drivers</li>
<li>Install “InfiniBand Support”</li>
<li>Use docker to run tests and make sure GPUs and RDMA can work inside
the container.</li>
</ol>
<p>I’ll ommit section “Install GPU drivers” because we can find it easily
somewhere else.</p>
<div class="section" id="install-rdma-drivers">
<span id="install-rdma-drivers"></span><h3>Install RDMA drivers<a class="headerlink" href="#install-rdma-drivers" title="永久链接至标题">¶</a></h3>
<p>For my case, I’ve got two machines with device
“Mellanox Technologies MT27700 Family [ConnectX-4]” installed. The OS was
“CentOS 7.4” and I updated the kernel to version 4.4 so that docker can
work with latest overlay2 filesystem.</p>
<p><strong><em>NOTE: before you start, make sure you have a way to get a console
of the server other than ssh because we may need to re-configure the
network device.</em></strong></p>
<ol class="simple">
<li>Go to http://www.mellanox.com/page/products_dyn?product_family=26,
download <code class="docutils literal"><span class="pre">MLNX_OFED</span></code> software in the bottom of the page, and upload it
onto the server.</li>
<li>Run <code class="docutils literal"><span class="pre">./mlnxofedinstall</span> <span class="pre">--add-kernel-support</span></code> in the software package.</li>
<li>Run <code class="docutils literal"><span class="pre">/etc/init.d/openibd</span> <span class="pre">restart</span></code> to make everything work, note that
this operation may cause the network goes down if you are using this
RDMA device as default network device and use ssh to login the server.</li>
<li>Re-configure the network interface, for example:
<code class="docutils literal"><span class="pre">ifconfig</span> <span class="pre">eth2</span> <span class="pre">192.168.16.30/20</span> <span class="pre">up</span></code>, then add routes if needed:
<code class="docutils literal"><span class="pre">ip</span> <span class="pre">route</span> <span class="pre">add</span> <span class="pre">default</span> <span class="pre">via</span> <span class="pre">192.168.16.1</span> <span class="pre">dev</span> <span class="pre">eth2</span></code>.</li>
<li>Do the same thing on the other node.</li>
<li>Use <code class="docutils literal"><span class="pre">ping</span></code> to test if the two nodes have typical ICMP connection.</li>
<li>Use either <code class="docutils literal"><span class="pre">udaddy</span></code> or <code class="docutils literal"><span class="pre">ib_write_bw</span></code> to test the network connection is
ready and have the desired bandwith.</li>
</ol>
</div>
<div class="section" id="prepare-docker-image-to-run-rdma-programs">
<span id="prepare-docker-image-to-run-rdma-programs"></span><h3>Prepare Docker Image to Run RDMA Programs<a class="headerlink" href="#prepare-docker-image-to-run-rdma-programs" title="永久链接至标题">¶</a></h3>
<ol class="simple">
<li>Build a docker image using cuda base image like: <code class="docutils literal"><span class="pre">nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04</span></code> and install paddlepaddle whl
package in it.</li>
<li>Start a docker container and mount GPU driver libs into it (you can
skip this step if you are using nvidia-docker).</li>
<li>Mount RDMA dirvers and libs into the docker image (see below section),
also <code class="docutils literal"><span class="pre">udaddy</span></code> and <code class="docutils literal"><span class="pre">ib_write_bw</span></code> if needed.</li>
<li>Mount GPU devices and RDMA devices into the container using <code class="docutils literal"><span class="pre">--device</span></code>
or just use privileged mode <code class="docutils literal"><span class="pre">--privileged</span></code>.</li>
<li>Start the container using host network mode: <code class="docutils literal"><span class="pre">--net=host</span></code></li>
</ol>
</div>
<div class="section" id="rdma-library-files-needed">
<span id="rdma-library-files-needed"></span><h3>RDMA Library Files Needed<a class="headerlink" href="#rdma-library-files-needed" title="永久链接至标题">¶</a></h3>
<p>Usually, <code class="docutils literal"><span class="pre">MLNX_OFED</span></code> install latest supported libs under
<code class="docutils literal"><span class="pre">/usr/lib64/mlnx_ofed/valgrind</span></code>. Other libs also needed to run RDMA programs
is listed below. These libs must be mounted into the docker container.</p>
<ul class="simple">
<li>Libs under <code class="docutils literal"><span class="pre">/usr/lib64/mlnx_ofed/valgrind</span></code><ul>
<li>libibcm.so</li>
<li>libibverbs.so</li>
<li>libmlx4.so</li>
<li>libmlx5.so</li>
<li>libmlx5-rdmav2.so</li>
<li>librdmacm.so</li>
</ul>
</li>
<li>Other libs:<ul>
<li>libnl-3.so.200</li>
<li>libnl-route-3.so.200</li>
<li>libnuma.so.1</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="start-to-run-the-training-job">
<span id="start-to-run-the-training-job"></span><h2>Start to Run the Training Job<a class="headerlink" href="#start-to-run-the-training-job" title="永久链接至标题">¶</a></h2>
<p>Setting NCCL environment variables to turn NCCL switches on and off:</p>
<p>| Env Name | Description |
| — | — |
| NCCL_SOCKET_IFNAME | The RDMA device, e.g. eth2 |
| NCCL_P2P_DISABLE | Set to 1 to disable P2P transfer between GPUs |
| NCCL_IB_DISABLE | Set to 1 to disable using RDMA |
| NCCL_IB_CUDA_SUPPORT | Set to 1 to enable GPU Direct if supported |
| NCCL_DEBUG | Set debug level: VERSION, WARN, INFO |</p>
<p>My two servers are: <code class="docutils literal"><span class="pre">192.168.16.30,192.168.16.34</span></code>, On node 1, Run :</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">PADDLE_TRAINER_ID</span><span class="o">=</span><span class="m">0</span> <span class="nv">PADDLE_PORT</span><span class="o">=</span><span class="m">48372</span> <span class="nv">PADDLE_WORKERS</span><span class="o">=</span><span class="m">192</span>.168.16.30,192.168.16.34 <span class="nv">POD_IP</span><span class="o">=</span><span class="m">192</span>.168.16.30 stdbuf -oL python vgg16.py
</pre></div>
</div>
<p>On node 2, Run:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">PADDLE_TRAINER_ID</span><span class="o">=</span><span class="m">1</span> <span class="nv">PADDLE_PORT</span><span class="o">=</span><span class="m">48372</span> <span class="nv">PADDLE_WORKERS</span><span class="o">=</span><span class="m">192</span>.168.16.30,192.168.16.34 <span class="nv">POD_IP</span><span class="o">=</span><span class="m">192</span>.168.16.34 stdbuf -oL python vgg16.py
</pre></div>
</div>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>