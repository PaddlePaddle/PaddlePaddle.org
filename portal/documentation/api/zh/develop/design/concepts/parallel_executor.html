<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="parallelexecutor">
<span id="parallelexecutor"></span><h1>ParallelExecutor<a class="headerlink" href="#parallelexecutor" title="永久链接至标题">¶</a></h1>
<div class="section" id="background">
<span id="background"></span><h2>Background<a class="headerlink" href="#background" title="永久链接至标题">¶</a></h2>
<p>Neural network models are defined as a <code class="docutils literal"><span class="pre">ProgramDesc</span></code> in Fluid. The <code class="docutils literal"><span class="pre">ProgramDesc</span></code> can be executed by an interpreter(i.e. the <code class="docutils literal"><span class="pre">executor</span></code> concept in Fluid). The instructions or operators in a <code class="docutils literal"><span class="pre">Program</span></code> will be executed, and the results will be fetched in Python side.</p>
<p>The executor is a very naive interpreter. It runs operators one by one. We can use <code class="docutils literal"><span class="pre">Parallel.Do</span></code> to support data parallelism, however, lacking device information in <code class="docutils literal"><span class="pre">ProgramDesc</span></code>; it is not possible to optimize the performance of <code class="docutils literal"><span class="pre">Parallel.Do</span></code>.</p>
<p>We want a <code class="docutils literal"><span class="pre">ProgramDesc</span></code> can be run on different nodes. It is better not to contain device information in <code class="docutils literal"><span class="pre">ProgramDesc</span></code>. However, we can write a high-performance interpreter, which can hold an alternative intermediate representation of <code class="docutils literal"><span class="pre">ProgramDesc</span></code>, to take full usage of Multi-GPUs.</p>
<p>ParallelExecutor is an interpreter of <code class="docutils literal"><span class="pre">ProgramDesc</span></code> which will <a class="reference external" href="https://en.wikipedia.org/wiki/Out-of-order_execution">out-of-order execute</a> <code class="docutils literal"><span class="pre">Program</span></code> in data parallelism mode and maximise the utility of Multi-GPUs.</p>
</div>
<div class="section" id="overview-of-multigpus-logic">
<span id="overview-of-multigpus-logic"></span><h2>Overview of MultiGPUs logic<a class="headerlink" href="#overview-of-multigpus-logic" title="永久链接至标题">¶</a></h2>
<p>The ParallelExecutor takes the startup program and main program as inputs. The parameters will be initialised on <code class="docutils literal"><span class="pre">GPU0</span></code> by startup program and will broadcast to multi-GPUs. The main program will be duplicated into multi-GPUs. The gradient will be merged during each iteration, and each device will optimize parameters independently. Since the gradients on each device will be merged before parameter optimization, the parameters will be the same on each device and it does not need to be broadcast the parameters.</p>
<p><img alt="alt" src="../../_images/parallel_executor_overview.png"/></p>
<p>There are several optimizations for this logic.</p>
<ol class="simple">
<li>We use an alternate representation in ParallelExecutor. It because the device information is critical for performance optimization.</li>
<li>The execution is out-of-order, i.e., an operator will be executed whenever the inputs of the operator are ready.<ul>
<li>GPU is a high-performance device; only one CPU thread cannot fulfil one GPU. So there is a thread pool to execute operators.</li>
<li>Out-of-order also helps transpilers to generate <code class="docutils literal"><span class="pre">ProgramDesc</span></code>. It is no need to concern about the best order of performance when implementing a transpiler.</li>
</ul>
</li>
<li>The streams of computation, merge gradients and fetch data are different.</li>
</ol>
<p>The performance of <code class="docutils literal"><span class="pre">ResNeXt152</span></code> on <code class="docutils literal"><span class="pre">TitanX</span></code> which <code class="docutils literal"><span class="pre">batch_size=12</span></code> is shown below.</p>
<p>| Number of GPUs | 1 | 2 | 3 | 4|
| — | — | — | — | — |
| Image/Sec | 17.9906 | 25.771 | 36.911 | 48.8428 |
| Speed Up | N/A | 1.43247029 | 2.05168255 | 2.71490667 |</p>
</div>
<div class="section" id="static-single-assignment-graph">
<span id="static-single-assignment-graph"></span><h2>Static single assignment Graph<a class="headerlink" href="#static-single-assignment-graph" title="永久链接至标题">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Static_single_assignment_form">Static single assignment form</a>(<code class="docutils literal"><span class="pre">SSA</span></code> for short) is a common form for compiler optimization. To implement concurrent execution, we uses an <code class="docutils literal"><span class="pre">SSA</span></code> graph as an intermedia representation of <code class="docutils literal"><span class="pre">ProgramDesc</span></code>.</p>
<p>The <code class="docutils literal"><span class="pre">Program</span></code> is a directed acyclic graph, since a variable can be assigned multiple times. We enforce a variable will be assigned once, by adding version number to varaibles. We parsing the <code class="docutils literal"><span class="pre">Program</span></code> into a <code class="docutils literal"><span class="pre">SSA</span></code> graph. Also, ProgramExecutor duplicate <code class="docutils literal"><span class="pre">Program</span></code> into multi-devices. We also add a device number to varaibles and insert <code class="docutils literal"><span class="pre">NCCLAllReduce</span></code> into Graph.</p>
<p>The data structure of <code class="docutils literal"><span class="pre">SSA</span></code> graph is:</p>
<div class="highlight-c++"><div class="highlight"><pre><span></span><span class="k">struct</span> <span class="n">VarHandleBase</span> <span class="p">{</span>
  <span class="n">OpHandleBase</span><span class="o">*</span> <span class="n">generated_op_</span><span class="p">;</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="n">OpHandleBase</span><span class="o">*&gt;</span> <span class="n">pending_ops_</span><span class="p">;</span>
  
  <span class="n">string</span> <span class="n">name</span><span class="p">;</span>
  <span class="n">Place</span> <span class="n">place</span><span class="p">;</span>
  <span class="kt">size_t</span> <span class="n">version</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="n">OpHandleBase</span> <span class="p">{</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="n">OpHandleBase</span><span class="o">*&gt;</span> <span class="n">inputs_</span><span class="p">;</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="n">OpHnadleBase</span><span class="o">*&gt;</span> <span class="n">outputs_</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="n">SSAGraph</span> <span class="p">{</span>
  <span class="c1">// vars on each devices. </span>
  <span class="c1">//   * the vars in each map in vector is on different device.</span>
  <span class="c1">//   * the map is mapping a variable name to variable handles</span>
  <span class="c1">//   with different versions</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">string</span><span class="p">,</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">VarHandleBase</span><span class="o">&gt;&gt;&gt;</span> <span class="n">vars_</span><span class="p">;</span>
  
  <span class="c1">// All ops</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="n">OpHandleBase</span><span class="o">&gt;</span> <span class="n">ops_</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The variable handles are the wrapper of <code class="docutils literal"><span class="pre">Variables</span></code>. The operator handles are the wrapper of <code class="docutils literal"><span class="pre">OperatorBase</span></code>. Some <code class="docutils literal"><span class="pre">OpHandle</span></code> is not an <code class="docutils literal"><span class="pre">OperatorBase</span></code>, such as <code class="docutils literal"><span class="pre">NCCLAllReduceOpHandle</span></code>, because <code class="docutils literal"><span class="pre">AllReduceOpHandle</span></code> will use new device contexts.</p>
<p>When the <code class="docutils literal"><span class="pre">ProgramDesc</span></code> converted into an <code class="docutils literal"><span class="pre">SSA</span></code> Graph, the <a class="reference external" href="https://en.wikipedia.org/wiki/Hazard_(computer_architecture)">data hazard</a> problem is also need to be taken care. The dummy variables, which represent the dependency between operators, will be manually inserted into SSA graph to resolve the <a class="reference external" href="https://en.wikipedia.org/wiki/Hazard_(computer_architecture)">data hazard</a> problem.</p>
</div>
<div class="section" id="execute-ssa-graph">
<span id="execute-ssa-graph"></span><h2>Execute SSA Graph<a class="headerlink" href="#execute-ssa-graph" title="永久链接至标题">¶</a></h2>
<p>The SSA graph can be out-of-order executed by an approximate <a class="reference external" href="https://en.wikipedia.org/wiki/Topological_sorting">topological sorting</a> algorithm. The algorithm is</p>
<ol class="simple">
<li>Maintaining a map of an operator and its needed input number.</li>
<li>If a variable is not generated by an operator, i.e., <code class="docutils literal"><span class="pre">var.generated_op</span> <span class="pre">==</span> <span class="pre">nullptr</span></code>, decrease the needed input number of its pending operators.</li>
<li>If there is an operator which needed input number is decreased to zero, just run this operator.</li>
<li>After run this operator, just mark the variables are generated and repeat step 2 until all variables are generated.</li>
</ol>
<p>Running an operator can be asynchronized. There is a thread pool to execute an <code class="docutils literal"><span class="pre">SSA</span></code> graph.</p>
</div>
<div class="section" id="synchronize-gpu-kernels">
<span id="synchronize-gpu-kernels"></span><h2>Synchronize GPU Kernels<a class="headerlink" href="#synchronize-gpu-kernels" title="永久链接至标题">¶</a></h2>
<p>The GPU is a non-blocking device. The different streams need be synchronized when switching streams. In current implementation, the synchronization based on the following algorithm:</p>
<ol class="simple">
<li><code class="docutils literal"><span class="pre">OpHandle</span></code> will record <code class="docutils literal"><span class="pre">DeviceContext</span></code> that it is used.</li>
<li>In <code class="docutils literal"><span class="pre">OpHandle::Run</span></code>, if the <code class="docutils literal"><span class="pre">DeviceContext</span></code> of current operator is different from <code class="docutils literal"><span class="pre">DeviceContext</span></code> of any input variable, just wait the generate operator of this input variable.</li>
</ol>
<p>The <code class="docutils literal"><span class="pre">wait</span></code> are implemented by two strategies:</p>
<ol class="simple">
<li>Invoke <code class="docutils literal"><span class="pre">DeviceContext-&gt;Wait()</span></code>, It will wait all operators on this device contexts complete.</li>
<li>Uses <code class="docutils literal"><span class="pre">cudaStreamWaitEvent</span></code> to sending a event to the stream. It is a non-blocking call. The wait operators will be executed in GPU.</li>
</ol>
<p>Generally, the <code class="docutils literal"><span class="pre">cudaStreamWaitEvent</span></code> will have a better perforamnce. However, <code class="docutils literal"><span class="pre">DeviceContext-&gt;Wait()</span></code> strategy is easier to debug. The strategy can be changed in runtime.</p>
</div>
<div class="section" id="what-s-next">
<span id="what-s-next"></span><h2>What’s next?<a class="headerlink" href="#what-s-next" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li>Merging gradient of dense parameters has been done. However, the merging of sparse parameters has not been done.</li>
<li>The CPU version of Parallel Executor has not been implemented. The out-of-order logic will make CPU compuatation faster, too.</li>
<li>A better strategy to merge gradients can be introduced. We can shrink the gradients from <code class="docutils literal"><span class="pre">float32</span></code> to <code class="docutils literal"><span class="pre">int8</span></code> or <code class="docutils literal"><span class="pre">int4</span></code> while merging. It will significantly speed up multi-GPUs training without much loss of precision.</li>
<li>Combine multi-Nodes implementation. By the benifit of out-of-order, sending and recving operator can be an blocking operator, and the transpiler does not need to concern about the best position of operator.</li>
</ul>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>