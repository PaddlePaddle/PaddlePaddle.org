<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="design-doc-parameter-server">
<span id="design-doc-parameter-server"></span><h1>Design Doc: Parameter Server<a class="headerlink" href="#design-doc-parameter-server" title="永久链接至标题">¶</a></h1>
<div class="section" id="abstract">
<span id="abstract"></span><h2>Abstract<a class="headerlink" href="#abstract" title="永久链接至标题">¶</a></h2>
<p>We propose an approach to implement the parameter server. In this
approach, there is no fundamental difference between the trainer and
the parameter server: they both run subgraphs, but subgraphs of
different purposes.</p>
</div>
<div class="section" id="background">
<span id="background"></span><h2>Background<a class="headerlink" href="#background" title="永久链接至标题">¶</a></h2>
<p>The previous implementations of the parameter server do not run a
fluid sub-program. Parameter initialization, optimizer computation, network
communication and checkpointing are implemented twice on both the
trainer as well as the parameter server.</p>
<p>It would be great if we can write code once and use them on both: the
trainer and the parameter server, since this reduces code duplication and
improves extensibility. Given that after the current refactoring, we are
representing everything as a computation graph on the
trainer. Representing everything as a computation graph on the parameter
server becomes a natural extension.</p>
</div>
<div class="section" id="design">
<span id="design"></span><h2>Design<a class="headerlink" href="#design" title="永久链接至标题">¶</a></h2>
<div class="section" id="distributed-transpiler">
<span id="distributed-transpiler"></span><h3>Distributed Transpiler<a class="headerlink" href="#distributed-transpiler" title="永久链接至标题">¶</a></h3>
<p>The <em>Distributed Transpiler</em> converts the user-defined fluid program
into sub-programs to be scheduled on different nodes with the following
steps:</p>
<ol class="simple">
<li>OP placement: the OPs will be placed on different nodes according
to a heuristic that minimizes the estimated total computation
time. Currently we will use a simple heuristic that puts parameter
variable on parameter server workers and everything else on trainer
workers.</li>
<li>Add communication OPs to enable the communication between nodes.</li>
</ol>
<p>We will need these OPs: <em>Send</em>, <em>Recv</em>, <em>Enqueue</em>, <em>Dequeue</em>.</p>
<p>Below is an example of converting the user defined graph to the
subgraphs for the trainer and the parameter server:</p>
<p><img src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/local-graph.png" width="300"/></p>
<p>After converting:</p>
<p><img src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/dist-graph.png" width="700"/></p>
<ol class="simple">
<li>The parameter variable W and its optimizer program are placed on the parameter server.</li>
<li>Operators are added to the program.<ul>
<li><em>Send</em> sends data to the connected <em>Recv</em> operator.  The
scheduler on the receive node will only schedule <em>Recv</em> operator
to run when the <em>Send</em> operator has ran (the <em>Send</em> OP will mark
the <em>Recv</em> OP runnable automatically).</li>
<li><em>Enqueue</em> enqueues the input variable, it can block until space
become available in the queue.</li>
<li><em>Dequeue</em> outputs configurable numbers of tensors from the
queue. It will block until the queue has the required number of
tensors.</li>
</ul>
</li>
</ol>
</div>
<div class="section" id="sparse-update">
<span id="sparse-update"></span><h3>Sparse Update<a class="headerlink" href="#sparse-update" title="永久链接至标题">¶</a></h3>
<p>For embedding layers, the gradient may have many rows containing only 0 when training,
if the gradient uses a dense tensor to do parameter optimization,
it could spend unnecessary memory, slow down the calculations and waste
the bandwidth while doing distributed training.
In Fluid, we introduce <a class="reference internal" href="../modules/selected_rows.html"><span class="doc">SelectedRows</span></a> to represent a list of rows containing
non-zero gradient data. So when we do parameter optimization both locally and remotely,
we only need to send those non-zero rows to the optimizer operators:</p>
<p><img src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/sparse_update.png" width="700"/></p>
</div>
<div class="section" id="benefits">
<span id="benefits"></span><h3>Benefits<a class="headerlink" href="#benefits" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li>Model parallelism becomes easier to implement: it is an extension to
the trainer - parameter server approach. We can have several “Transpilers”
to achieve different goals.</li>
<li>User-defined optimizer is easier to add - user can now express it as
a sub-program.</li>
<li>No more duplication logic inside the trainer and the parameter
server mentioned in the background section.</li>
</ul>
</div>
<div class="section" id="challenges">
<span id="challenges"></span><h3>Challenges<a class="headerlink" href="#challenges" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li>It is important to balance the parameter shards on multiple
parameter servers. If a single parameter is very big (for example: some
word-embedding, fully connected, softmax layer), we need to
automatically partition the single parameter onto different
parameter servers when possible (only element-wise optimizer depends
on the parameter variable).</li>
<li>In the “Async SGD” figure, the “W” variable on the parameter server
could be read and written concurrently. See
<a class="reference external" href="https://github.com/PaddlePaddle/Paddle/pull/6394">here</a> for more
details about concurrent program in Fluid.</li>
</ul>
</div>
<div class="section" id="discussion">
<span id="discussion"></span><h3>Discussion<a class="headerlink" href="#discussion" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li>Can the Enqueue OP be implemented under our current tensor design
(put the input tensor into the queue tensor)?</li>
<li><em>Dequeue</em> OP will have variable numbers of output (depending on the
<code class="docutils literal"><span class="pre">min_count</span></code> attribute), does our current design support it? (similar
question for the <em>Add</em> OP)</li>
</ul>
</div>
<div class="section" id="references">
<span id="references"></span><h3>References<a class="headerlink" href="#references" title="永久链接至标题">¶</a></h3>
<p>[1] <a class="reference external" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45166.pdf">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</a></p>
</div>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>