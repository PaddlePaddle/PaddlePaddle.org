<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="design-doc-asynchronous-update-with-distributed-training">
<span id="design-doc-asynchronous-update-with-distributed-training"></span><h1>Design Doc: Asynchronous Update With Distributed Training<a class="headerlink" href="#design-doc-asynchronous-update-with-distributed-training" title="永久链接至标题">¶</a></h1>
<div class="section" id="background">
<span id="background"></span><h2>Background<a class="headerlink" href="#background" title="永久链接至标题">¶</a></h2>
<p>For the typical synchronous distributed training, some significant steps are as follows:</p>
<ol class="simple">
<li>A trainer process will compute the gradients and <strong>send</strong> them to the parameter server (PS) nodes.</li>
<li>After the PS node received gradients came from all the Trainers, It will aggregate the
gradient variables for the same parameter into one gradient variable and then apply the aggregated
gradient to the respective parameter, finally using an optimize algorithms(SGD, Monument...)
to update the parameters.</li>
<li>The Trainer would wait for the PS finished the optimize stage, and GET the parameters from PS,
so all the Trainers would get the same parameters.</li>
</ol>
<p>In Synchronous Distributed Training, there is a <strong>barrier</strong> on each PS to wait until all trainers processes
have completed running current mini-batch. After that, all trainers can continue to run the next
mini-batch. So, we can find that the overall performance of Synchronous Distributed Training depends
on the slowest node.</p>
<p>In Asynchronous Distributed Training, we don’t need to wait for a global mini-bach, the optimizer on
the PS will run immediately when the gradient is uploaded to the PS from one trainer. This mode would
train such models that achieve scaling, better throughput. In this design doc, we will introduce how to
implement the Asynchronous Distributed Training base on PaddlePaddle Fluid.</p>
</div>
<div class="section" id="design">
<span id="design"></span><h2>Design<a class="headerlink" href="#design" title="永久链接至标题">¶</a></h2>
<p><img src="./src/async_update.png" width="600"/></p>
<p>As the figure above, we describe a global view of the asynchronous update process and use
the parameter <code class="docutils literal"><span class="pre">w1</span></code> as an example to introduce the steps:</p>
<ol class="simple">
<li>For each gradient variables, they may distribute on different GPU card and aggregate
them while they are all calculated.</li>
<li>Split the gradient variable into multiple blocks according to the number of PS
instances and then send them.</li>
<li>PS would run an <code class="docutils literal"><span class="pre">Optimize</span> <span class="pre">Block</span></code> using a specified optimize algorithm to update
the specified parameter.</li>
<li>The trainer will fetch the latest parameter from PS before running forward Op which depends
on the specified parameter.</li>
<li>Broadcast the received variable into multiple GPU cards and continue to run the next
mini-batch.</li>
</ol>
<div class="section" id="trainer">
<span id="trainer"></span><h3>Trainer<a class="headerlink" href="#trainer" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li>For the multiple devices distributed training, we need to aggregate the gradient
variables which placed on different devices firstly and then schedule a <code class="docutils literal"><span class="pre">SendVars</span></code> Operator to
send the gradient variables to the multiple PS instances.</li>
<li>Schedule <code class="docutils literal"><span class="pre">FetchVars</span></code> operator to fetch the latest parameter from PS before running
the forward ops.</li>
<li>There could be a large number of gradient variables to be sent, so we need to use another
thread pool(IO Threadpool) whose a number of the schedulable threads is larger than the
computing thread pool to avoid competitive the thread resources with computing.</li>
</ul>
</div>
<div class="section" id="parameter-server">
<span id="parameter-server"></span><h3>Parameter Server<a class="headerlink" href="#parameter-server" title="永久链接至标题">¶</a></h3>
<p><img src="./src/async_pserver.png" width="750"/></p>
<ul class="simple">
<li>There should be multiple trainer instances want to optimize the same parameter at
the same time, to avoid the racing, we need one <code class="docutils literal"><span class="pre">BlockingQueue</span></code> for each gradient
variable to process them one by one.</li>
<li>We need a <code class="docutils literal"><span class="pre">Map</span></code> structure to map a gradient variable name to the <code class="docutils literal"><span class="pre">OptimizeBlock</span></code> which
can optimize the respective parameter.</li>
</ul>
</div>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>