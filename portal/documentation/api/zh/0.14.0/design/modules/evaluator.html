<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="evaluator-design">
<span id="evaluator-design"></span><h1>Evaluator Design<a class="headerlink" href="#evaluator-design" title="永久链接至标题">¶</a></h1>
<div class="section" id="problem-statement">
<span id="problem-statement"></span><h2>Problem Statement<a class="headerlink" href="#problem-statement" title="永久链接至标题">¶</a></h2>
<p>During training or inference, we provide an evaluation function to measure the model performance, for example, accuracy, precision, etc. In the operator based framework design, the data passes through the network pipeline batch by batch. As a result, inside the operator, we only calculate the metrics for one minibatch. Thus, we need to provide a mechanism to calculate the metrics for each N pass/batch the user wants.</p>
</div>
<div class="section" id="evaluator-design">
<span id="id1"></span><h2>Evaluator Design<a class="headerlink" href="#evaluator-design" title="永久链接至标题">¶</a></h2>
<p>Currently, every operation is expressed in the graph. We divide the evaluator process into three steps.</p>
<ol class="simple">
<li>Initialize the metric state and add it into the block.</li>
<li>Calculate the concerned metrics for every mini-batch. The single evaluator operator is only responsible for calculating the necessary statistics for one mini-batch. For example, the accuracy operator only calculates the accuracy for a minibatch data if run once.</li>
</ol>
<ol class="simple">
<li>Merge the mini-batch statistics to form the evaluation result for multiple mini-batches. When it comes to distributed training/Multi-GPU training, aggregate the value from different devices.</li>
</ol>
</div>
<div class="section" id="implementation">
<span id="implementation"></span><h2>Implementation<a class="headerlink" href="#implementation" title="永久链接至标题">¶</a></h2>
<p>This design is shown in the Python API.
Each metric operator needs to caculate the metric statistic and return the batch-aware states. Python side is responsible for accumulating the states for each pass.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Evaluator</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Evaluator Base class.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
       <span class="sd">"""</span>
<span class="sd">       Different evaluator may has different metric states. E.g, Accuracy need two variables, total and right sample counts.</span>
<span class="sd">       Auc need four variables, `true_positives`,</span>
<span class="sd">         `true_negatives`, `false_positives` and `false_negatives`. So every evaluator should create its needed variables and append to main_program</span>

<span class="sd">       The initialization of Evaluator should be responsible for:</span>
<span class="sd">       create metric states and append to the main_program</span>
<span class="sd">       """</span>
       <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">_update_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
       <span class="sd">"""</span>
<span class="sd">       Add mini-batch evaluator caculate operators to the main_program.</span>
<span class="sd">       Add increment operator to accumulate the metric states.</span>
<span class="sd">       """</span>


    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">executor</span><span class="p">,</span> <span class="n">reset_program</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
      <span class="sd">"""</span>
<span class="sd">      Reset metric states at the begin of each pass/user specified batch number.</span>
<span class="sd">      Execute the reset_program to reset the states.</span>
<span class="sd">      """</span>


    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">executor</span><span class="p">,</span> <span class="n">eval_program</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
      <span class="sd">"""</span>
<span class="sd">      Merge the mini-batch statistics to form the evaluation result for multiple mini-batches.</span>
<span class="sd">      Execute the eval_program and return the result.</span>
<span class="sd">      """</span>
      <span class="k">return</span> <span class="n">eval_result</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>