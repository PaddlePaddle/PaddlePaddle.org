<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="optimizer-design">
<span id="optimizer-design"></span><h1>Optimizer Design<a class="headerlink" href="#optimizer-design" title="永久链接至标题">¶</a></h1>
<div class="section" id="the-problem">
<span id="the-problem"></span><h2>The Problem<a class="headerlink" href="#the-problem" title="永久链接至标题">¶</a></h2>
<p>A PaddlePaddle program, or a block, is a sequence of operators operating variables.  A training program needs to do three kinds of works:</p>
<ol class="simple">
<li>the forward pass, which computes intermediate results and the cost(s),</li>
<li>the backward pass, which derives gradients from intermediate results and costs, and</li>
<li>the optimization pass, which update model parameters to optimize the cost(s).</li>
</ol>
<p>These works rely on three kinds of operators:</p>
<ol class="simple">
<li>forward operators,</li>
<li>gradient operators, and</li>
<li>optimization operators.</li>
</ol>
<p>It’s true that users should be able to create all these operators manually by calling some low-level API, but it would be much more convenient if they could only describe the forward pass and let PaddlePaddle create the backward and optimization operators automatically.</p>
<p>In this design, we propose a high-level API that automatically derives the optimisation pass and operators from the forward pass.</p>
</div>
<div class="section" id="high-level-python-api-to-describe-the-training-process">
<span id="high-level-python-api-to-describe-the-training-process"></span><h2>High-level Python API to describe the training process<a class="headerlink" href="#high-level-python-api-to-describe-the-training-process" title="永久链接至标题">¶</a></h2>
<ol>
<li><p class="first">User write code to describe the network:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">images</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="s2">"images"</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="s2">"labels"</span><span class="p">)</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">"w1"</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">"b1"</span><span class="p">)</span>
<span class="n">hidden</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="n">w1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b1</span><span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
<p>The above code snippet will create forward operators in <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/design/block.md">Block</a>.</p>
</li>
</ol>
<ol>
<li><p class="first">Users create a certain kind of Optimizer with some argument.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdagradOptimizer</span><span class="p">(</span><span class="n">learing_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">Users use the optimizer to <code class="docutils literal"><span class="pre">minimize</span></code> a certain <code class="docutils literal"><span class="pre">cost</span></code> through updating parameters in parameter_list.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">opt_op_list</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">parameter_list</span><span class="o">=</span><span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">])</span>
</pre></div>
</div>
<p>The above code snippet will create gradient and optimization operators in Block. The return value of <code class="docutils literal"><span class="pre">minimize()</span></code> is list of optimization operators that will be run by session.</p>
</li>
<li><p class="first">Users use Session/Executor to run this opt_op_list as target to do training.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">target</span><span class="o">=</span> <span class="n">opt_op_list</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<div class="section" id="optimizer-python-interface">
<span id="optimizer-python-interface"></span><h3>Optimizer Python interface:<a class="headerlink" href="#optimizer-python-interface" title="永久链接至标题">¶</a></h3>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""Optimizer Base class.</span>

<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">create_optimization_pass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters_and_grads</span><span class="p">):</span>
        <span class="sd">"""Add optimization operators to update gradients to variables.</span>

<span class="sd">        Args:</span>
<span class="sd">          parameters_and_grads: a list of (variable, gradient) pair to update.</span>

<span class="sd">        Returns:</span>
<span class="sd">          optmization_op_list: a list of optimization operator that will update parameter using gradient.</span>
<span class="sd">        """</span>
        <span class="k">return</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">minimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">parameter_list</span><span class="p">):</span>
        <span class="sd">"""Add operations to minimize `loss` by updating `parameter_list`.</span>

<span class="sd">        This method combines interface `append_backward()` and</span>
<span class="sd">        `create_optimization_pass()` into one.</span>
<span class="sd">        """</span>
        <span class="n">params_grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_backward_pass</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">parameter_list</span><span class="p">)</span>
        <span class="n">update_ops</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_optimization_pass</span><span class="p">(</span><span class="n">params_grads</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">update_ops</span>

</pre></div>
</div>
<p>Users can inherit the Optimizer above to create their own Optimizer with some special logic, such as AdagradOptimizer.</p>
</div>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>