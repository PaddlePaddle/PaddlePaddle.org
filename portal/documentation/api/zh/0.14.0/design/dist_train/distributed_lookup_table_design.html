<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="design-doc-distributed-lookup-table-operator">
<span id="design-doc-distributed-lookup-table-operator"></span><h1>Design Doc: Distributed Lookup Table Operator<a class="headerlink" href="#design-doc-distributed-lookup-table-operator" title="永久链接至标题">¶</a></h1>
<p>A lookup table operator in PaddlePaddle where the table could be out
of the memory of a computer.</p>
<div class="section" id="background">
<span id="background"></span><h2>Background<a class="headerlink" href="#background" title="永久链接至标题">¶</a></h2>
<p>A lookup table operator is well-used in deep learning for learning the
representation, or the
<a class="reference external" href="http://www.cs.toronto.edu/~fritz/absps/ieee-lre.pdf"><em>embedding</em></a>, of
symbols.</p>
<div class="section" id="the-forward-algorithm">
<span id="the-forward-algorithm"></span><h3>The Forward Algorithm<a class="headerlink" href="#the-forward-algorithm" title="永久链接至标题">¶</a></h3>
<p>The forward algorithm of the lookup table is a multiplication of the
input vector x and the lookup table matrix W:</p>
<p>$$y = x * W$$</p>
<p>When x is a sparse vector of symbols, the above multiplication
simplifies into looking up rows in W that correspond to symbols in x,
denoted by W(x).  Please be aware that W could be huge and out of the
memory, so we’d need a distributed storage service, which supports the
lookup of rows.</p>
<p>The following figure illustrates the multiplication of x with two
non-zero elements, or say, two symbols, and a lookup table W:</p>
<p><img alt="lookup table" src="../../_images/lookup_table.png"/></p>
</div>
<div class="section" id="the-backward-algorithm">
<span id="the-backward-algorithm"></span><h3>The Backward Algorithm<a class="headerlink" href="#the-backward-algorithm" title="永久链接至标题">¶</a></h3>
<p>The backward algorithm computes W’(x) using W(x).  W’(x) has the same
scale of size as W(x) and is much smaller than W.</p>
<p>To optimize W given W’, we can do simple SGD update:</p>
<p>$$W = f(W’) = \lambda * W’$$</p>
<p>or some more sophisticated algorithms that rely on both W’ and W:</p>
<p>$$W = f(W, W’)$$</p>
<p>The following figure illustrates the backward pass of the lookup
operator: <img alt="lookup table training" src="../../_images/lookup_table_training.png"/></p>
</div>
</div>
<div class="section" id="distributed-storage-service">
<span id="distributed-storage-service"></span><h2>Distributed Storage Service<a class="headerlink" href="#distributed-storage-service" title="永久链接至标题">¶</a></h2>
<p>The forward algorithm requires a distributed storage service for W.
The backward algorithm prefers that the storage system can apply the
optimization algorithm on W.  The following two sections describe two
solutions – the former doesn’t require that the storage service can
do optimization, the latter does.</p>
<div class="section" id="storage-service-doesn-t-optimize">
<span id="storage-service-doesn-t-optimize"></span><h3>Storage Service Doesn’t Optimize<a class="headerlink" href="#storage-service-doesn-t-optimize" title="永久链接至标题">¶</a></h3>
<p>In this design, we use highly-optimized distributed storage, e.g.,
memcached, as the storage service, and we run the optimization
algorithm on parameter servers of PaddlePaddle.  The following figure
illustrates the training process.</p>
<!--
Note: please update the following URL when update this digraph.
<img src='https://g.gravizo.com/svg?
digraph G {
  rankdir="LR";
  subgraph cluster1 {
  P1 [label="pserver 1"];
  P2 [label="pserver 2"];
  T1 [label="trainer 1"];
  T2 [label="trainer 2"];
  T3 [label="trainer 3"];
  }
  KV [label="memcached"];
  T1 -> P1;
  T1 -> P2;
  T2 -> P1;
  T2 -> P2;
  T3 -> P1;
  T3 -> P2;
  P1 -> KV [color=gray, weight=0.1];
  KV -> P1 [color=gray, weight=0.1];
  P2 -> KV [color=gray, weight=0.1];
  KV -> P2 [color=gray, weight=0.1];
  KV -> T1 [color=gray, weight=0.1];
  KV -> T2 [color=gray, weight=0.1];
  KV -> T3 [color=gray, weight=0.1];
}
)
'/>
--><p><img src="https://g.gravizo.com/svg?%20digraph%20G%20{%20rankdir=%22LR%22;%20subgraph%20cluster1%20{%20P1%20[label=%22pserver%201%22];%20P2%20[label=%22pserver%202%22];%20T1%20[label=%22trainer%201%22];%20T2%20[label=%22trainer%202%22];%20T3%20[label=%22trainer%203%22];%20}%20KV%20[label=%22memcached%22];%20T1%20-%3E%20P1;%20T1%20-%3E%20P2;%20T2%20-%3E%20P1;%20T2%20-%3E%20P2;%20T3%20-%3E%20P1;%20T3%20-%3E%20P2;%20P1%20-%3E%20KV%20[color=gray,%20weight=0.1];%20KV%20-%3E%20P1%20[color=gray,%20weight=0.1];%20P2%20-%3E%20KV%20[color=gray,%20weight=0.1];%20KV%20-%3E%20P2%20[color=gray,%20weight=0.1];%20KV%20-%3E%20T1%20[color=gray,%20weight=0.1];%20KV%20-%3E%20T2%20[color=gray,%20weight=0.1];%20KV%20-%3E%20T3%20[color=gray,%20weight=0.1];%20}"/></p>
<p>Each trainer runs the forward and backward passes using their local
data:</p>
<ol class="simple">
<li>In the forward pass, when a trainer runs the forward algorithm of a
lookup operator, it retrieves W(x) from the storage service.</li>
<li>The trainer computes W’(x) in the backward pass using W(x).</li>
</ol>
<p>During the global update process:</p>
<ol class="simple">
<li>Each trainer uploads its W’(x) to parameter servers.</li>
<li>The parameter server runs the optimization algorithm, e.g., the
Adam optimization algorithm, which requires that<ol>
<li>The parameter server retrieves W(x) from memcached, and</li>
<li>The parameter server pushes $\Delta W(x)=f(W(x), lambda \sum_j
W’(x))$ to memcached, where $f$ denotes the optimization
algorithm.</li>
</ol>
</li>
</ol>
</div>
<div class="section" id="storage-service-does-optimize">
<span id="storage-service-does-optimize"></span><h3>Storage Service Does Optimize<a class="headerlink" href="#storage-service-does-optimize" title="永久链接至标题">¶</a></h3>
<p>This design is very similar to the above one, except that the
optimization algorithm $f$ runs on the storage service.</p>
<ul class="simple">
<li>Pro: parameter servers do not retrieve W(x) from the storage
service, thus saves half network communication.</li>
<li>Con: the storage service needs to be able to run the optimization
algorithm.</li>
</ul>
</div>
</div>
<div class="section" id="conclusion">
<span id="conclusion"></span><h2>Conclusion<a class="headerlink" href="#conclusion" title="永久链接至标题">¶</a></h2>
<p>Let us do the “storage service does not optimize” solution first, as a
baseline at least, because it is easier to use a well-optimized
distributed storage service like memcached.  We can do the “storage
service does optimize” solution later or at the same time, which, if
implemented carefully, should have better performance than the former.</p>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>