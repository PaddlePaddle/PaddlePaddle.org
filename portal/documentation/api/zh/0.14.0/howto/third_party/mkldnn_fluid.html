<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="design-doc-add-mkldnn-kernel-in-fluid-operator">
<span id="design-doc-add-mkldnn-kernel-in-fluid-operator"></span><h1>Design Doc: Add MKLDNN Kernel in Fluid Operator<a class="headerlink" href="#design-doc-add-mkldnn-kernel-in-fluid-operator" title="永久链接至标题">¶</a></h1>
<div class="section" id="principles">
<span id="principles"></span><h2>Principles<a class="headerlink" href="#principles" title="永久链接至标题">¶</a></h2>
<p>First of all, we should follow some basical principles like:</p>
<ol class="simple">
<li><a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/howto/dev/new_op_en.md">How to write a new operator</a>. We are trying to add a new kind of kernel into operators, so basically we should follow this doc.</li>
<li><a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/design/support_new_device.md">Supporting new Device/Library</a>. Since MKLDNN is a new library to fluid, we should add <code class="docutils literal"><span class="pre">MKLDNNDeviceContext</span></code> and maybe <code class="docutils literal"><span class="pre">mkldnn_helper.h</span></code>, just like <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/platform/cudnn_helper.h">cudnn_helper.h</a>.</li>
<li><a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/design/switch_kernel.md">Switch Kernel</a>. Another important point is that we should ensure the data synchronization between different kernel types, which is this <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/issues/6549">topic</a>. So basically we should override <code class="docutils literal"><span class="pre">GetExpectedKernelType</span></code> and <code class="docutils literal"><span class="pre">trans</span></code> functions to support switching kernels.</li>
<li><a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/design/operator_kernel_type.md">The Keys of Operator Kernel Type</a>. Kernel Type is a pivotal conception which can record the <code class="docutils literal"><span class="pre">Place</span></code>, <code class="docutils literal"><span class="pre">Library</span></code>, <code class="docutils literal"><span class="pre">DataType</span></code> and <code class="docutils literal"><span class="pre">Layout</span></code>.</li>
</ol>
</div>
<div class="section" id="sulution">
<span id="sulution"></span><h2>Sulution<a class="headerlink" href="#sulution" title="永久链接至标题">¶</a></h2>
<p>In general, there are four parts we should follow to run a MKL-DNN primitive.</p>
<ul class="simple">
<li>Create a primitive descriptor that describe this operator</li>
<li>Create a primitive itself by primitive descriptor and the engine</li>
<li>Create all memory buffers that primitive needed</li>
<li>Launch a stream to execute the primitive created
More details can refer to <a class="reference external" href="http://01org.github.io/mkl-dnn">here</a>.</li>
</ul>
<p>It’s better to avoid reinitialization of primitives and memory handles in the first three stages in every iteration. So we plan to create a map to record all the <code class="docutils literal"><span class="pre">primitive</span></code> and <code class="docutils literal"><span class="pre">memory</span></code>, which should not take too much memories as discussed <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/issues/6822">here</a>.</p>
<p>It’s assumed that following three conditions should be satisfied.</p>
<ol class="simple">
<li>there is a unique key for each operator instance. May be the actual name of <code class="docutils literal"><span class="pre">Output</span> <span class="pre">Tensor</span></code>.</li>
<li>the <code class="docutils literal"><span class="pre">Input</span> <span class="pre">Tensor</span></code> inside <code class="docutils literal"><span class="pre">Compute</span></code> function is the one after converted.</li>
<li>we can get the phase(eg. <code class="docutils literal"><span class="pre">is_test</span></code>) inside <code class="docutils literal"><span class="pre">Compute</span></code> function, otherwise we need to expose this attribue to user.</li>
</ol>
<div class="section" id="compute">
<span id="compute"></span><h3>Compute<a class="headerlink" href="#compute" title="永久链接至标题">¶</a></h3>
<p>The algorithm of <code class="docutils literal"><span class="pre">Compute</span></code> would be described as follow, let’s take conv like an example.</p>
<div class="highlight-c++"><div class="highlight"><pre><span></span>  <span class="n">PADDLE_ENFORCE</span><span class="p">(</span><span class="n">platform</span><span class="o">::</span><span class="n">is_cpu_place</span><span class="p">(</span><span class="n">ctx</span><span class="p">.</span><span class="n">GetPlace</span><span class="p">()),</span> <span class="s">"It must use CPUPlace."</span><span class="p">);</span>
  <span class="n">PADDLE_ENFORCE</span><span class="p">(</span><span class="n">platform</span><span class="o">::</span><span class="n">is_mkldnn_library</span><span class="p">(</span><span class="n">ctx</span><span class="p">.</span><span class="n">GetLibrary</span><span class="p">()),</span> <span class="s">"It must use MKLDNN Library."</span><span class="p">);</span>

  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">dev_ctx</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="k">template</span> <span class="n">device_context</span><span class="o">&lt;</span><span class="n">platform</span><span class="o">::</span><span class="n">MKLDNNDeviceContext</span><span class="o">&gt;</span><span class="p">();</span>

  <span class="c1">// find primitive by unique key from mkldnn context</span>
  <span class="c1">// the op_key should be a unique name of this op instance</span>
  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">p</span> <span class="o">=</span> <span class="n">dev_ctx</span><span class="p">.</span><span class="n">findPrimitive</span><span class="p">(</span><span class="n">op_key</span> <span class="o">+</span> <span class="s">"_fwd"</span><span class="p">);</span>

  <span class="c1">// assuming the input tensor inside this compute function is the one after converted</span>
  <span class="c1">// this point should be guarantee by another mechanism</span>
  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">i</span> <span class="o">=</span> <span class="n">dev_ctx</span><span class="p">.</span><span class="n">findMemory</span><span class="p">(</span><span class="n">op_key</span> <span class="o">+</span> <span class="s">"_input"</span><span class="p">);</span>
  
  <span class="k">if</span> <span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="k">nullptr</span> <span class="o">||</span> <span class="n">i</span> <span class="o">==</span> <span class="k">nullptr</span> <span class="o">||</span> <span class="n">inputSizeChanged</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>  <span class="p">{</span>
    <span class="k">auto</span> <span class="n">fwd_primitive_desc</span> <span class="o">=</span> <span class="n">createPrimitiveDesc</span><span class="p">(</span><span class="n">ctx</span><span class="p">);</span>
    <span class="k">auto</span><span class="o">*</span> <span class="n">input</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">Input</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"Input"</span><span class="p">);</span>
    <span class="k">auto</span><span class="o">*</span> <span class="n">filter</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">Input</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"Filter"</span><span class="p">);</span>
    <span class="k">auto</span><span class="o">*</span> <span class="n">output</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">Output</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"Output"</span><span class="p">);</span>
    <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mkldnn</span><span class="o">::</span><span class="n">memory</span><span class="o">&gt;</span> <span class="n">in</span><span class="p">(</span><span class="k">new</span> <span class="n">mkldnn</span><span class="o">::</span><span class="n">memory</span><span class="p">(</span><span class="n">fwd_primitive_desc</span><span class="o">-&gt;</span><span class="n">src_primitive_desc</span><span class="p">(),</span> <span class="n">input</span><span class="o">-&gt;</span><span class="n">data</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">()));</span>
    <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mkldnn</span><span class="o">::</span><span class="n">memory</span><span class="o">&gt;</span> <span class="n">wgt</span><span class="p">(</span><span class="k">new</span> <span class="n">mkldnn</span><span class="o">::</span><span class="n">memory</span><span class="p">(</span><span class="n">fwd_primitive_desc</span><span class="o">-&gt;</span><span class="n">weights_primitive_desc</span><span class="p">(),</span> <span class="n">filter</span><span class="o">-&gt;</span><span class="n">data</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">()));</span>
    <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mkldnn</span><span class="o">::</span><span class="n">memory</span><span class="o">&gt;</span> <span class="n">out</span><span class="p">(</span><span class="k">new</span> <span class="n">mkldnn</span><span class="o">::</span><span class="n">memory</span><span class="p">(</span><span class="n">fwd_primitive_desc</span><span class="o">-&gt;</span><span class="n">dst_primitive_desc</span><span class="p">(),</span> <span class="n">output</span><span class="o">-&gt;</span><span class="n">mutable_data</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">ctx</span><span class="p">.</span><span class="n">GetPlace</span><span class="p">())));</span>
    <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mkldnn</span><span class="o">::</span><span class="n">conv_fwd</span><span class="o">&gt;</span> <span class="n">fwd_primitive</span><span class="p">(</span><span class="k">new</span> <span class="n">mkldnn</span><span class="o">::</span><span class="n">conv_fwd</span><span class="p">(</span><span class="o">*</span><span class="n">fwd_primitive_desc</span><span class="p">,</span> <span class="o">*</span><span class="n">in</span><span class="p">,</span> <span class="o">*</span><span class="n">wgt</span><span class="p">,</span> <span class="o">*</span><span class="n">out</span><span class="p">));</span>

    <span class="n">dev_ctx</span><span class="p">.</span><span class="n">addMemory</span><span class="p">(</span><span class="n">op_key</span><span class="o">+</span><span class="s">"_input"</span><span class="p">,</span> <span class="n">in</span><span class="p">);</span>
    <span class="n">dev_ctx</span><span class="p">.</span><span class="n">addMemory</span><span class="p">(</span><span class="n">op_key</span><span class="o">+</span><span class="s">"_output"</span><span class="p">,</span> <span class="n">out</span><span class="p">);</span>
    <span class="n">dev_ctx</span><span class="p">.</span><span class="n">addMemory</span><span class="p">(</span><span class="n">op_key</span><span class="o">+</span><span class="s">"_filer"</span><span class="p">,</span> <span class="n">wgt</span><span class="p">);</span>
    <span class="n">dev_ctx</span><span class="p">.</span><span class="n">addPrimitive</span><span class="p">(</span><span class="n">op_key</span><span class="o">+</span><span class="s">"_fwd"</span><span class="p">,</span> <span class="n">fwd_primitive</span><span class="p">);</span>
    <span class="n">dev_ctx</span><span class="p">.</span><span class="n">addPrimitiveDesc</span><span class="p">(</span><span class="n">op_key</span><span class="o">+</span><span class="s">"_fwd_PD"</span><span class="p">,</span> <span class="n">fwd_primitive_desc</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="n">p</span> <span class="o">=</span> <span class="n">dev_ctx</span><span class="p">.</span><span class="n">findPrimitive</span><span class="p">(</span><span class="n">op_key</span> <span class="o">+</span> <span class="s">"_fwd"</span><span class="p">);</span>

  <span class="n">PADDLE_ENFORCE</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s">"Should have forward Primitive"</span><span class="p">);</span>
  <span class="n">PADDLE_ENFORCE</span><span class="p">(</span><span class="n">dev_ctx</span><span class="p">.</span><span class="n">findMemory</span><span class="p">(</span><span class="n">op_unique_key</span><span class="o">+</span><span class="s">"_input"</span><span class="p">),</span> <span class="s">"Should have input memory"</span><span class="p">);</span>
  <span class="n">PADDLE_ENFORCE</span><span class="p">(</span><span class="n">dev_ctx</span><span class="p">.</span><span class="n">findMemory</span><span class="p">(</span><span class="n">op_unique_key</span><span class="o">+</span><span class="s">"_output"</span><span class="p">),</span> <span class="s">"Should have output memory"</span><span class="p">);</span>
  <span class="n">PADDLE_ENFORCE</span><span class="p">(</span><span class="n">dev_ctx</span><span class="p">.</span><span class="n">findMemory</span><span class="p">(</span><span class="n">op_unique_key</span><span class="o">+</span><span class="s">"_filter"</span><span class="p">),</span> <span class="s">"Should have filter memory"</span><span class="p">);</span>
  <span class="n">PADDLE_ENFORCE</span><span class="p">(</span><span class="n">dev_ctx</span><span class="p">.</span><span class="n">findPrimitiveDesc</span><span class="p">(</span><span class="n">op_unique_key</span><span class="o">+</span><span class="s">"_fwd_PD"</span><span class="p">),</span> <span class="s">"Should have forward PrimitiveDesc"</span><span class="p">);</span>
  <span class="n">dev_ctx</span><span class="p">.</span><span class="n">submit</span><span class="p">(</span><span class="n">p</span><span class="p">);</span>
  <span class="n">dev_ctx</span><span class="p">.</span><span class="n">execute</span><span class="p">();</span>  <span class="c1">// the convert primitive should have already contained.</span>
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">createPrimitiveDesc</span></code> returns the primitive descripotor of this operator, would be like this:</p>
<div class="highlight-c++"><div class="highlight"><pre><span></span>  <span class="k">auto</span><span class="o">*</span> <span class="n">input</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">Input</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"Input"</span><span class="p">);</span>
  <span class="k">auto</span><span class="o">*</span> <span class="n">filter</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">Input</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"Filter"</span><span class="p">);</span>
  <span class="k">auto</span><span class="o">*</span> <span class="n">output</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">Output</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"Output"</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">strides</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">Attr</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="s">"strides"</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">paddings</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">Attr</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="s">"paddings"</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">dilations</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">Attr</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="s">"dilations"</span><span class="p">);</span>
  <span class="kt">int</span> <span class="n">groups</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">Attr</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"groups"</span><span class="p">);</span>
  <span class="n">algorithm</span> <span class="n">algo</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">algorithm</span><span class="o">&gt;</span><span class="p">(</span><span class="n">ctx</span><span class="p">.</span><span class="n">Attr</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"convolution_algorithm_option"</span><span class="p">));</span>
  <span class="n">prop_kind</span> <span class="n">pk</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">Attr</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"is_test"</span><span class="p">)</span> <span class="o">?</span> <span class="n">prop_kind</span><span class="o">::</span><span class="nl">forward_inference</span> <span class="p">:</span> <span class="n">prop_kind</span><span class="o">::</span><span class="n">forward_training</span><span class="p">;</span>
    
  <span class="k">auto</span> <span class="n">fwd_desc</span> <span class="o">=</span> <span class="n">mkldnn</span><span class="o">::</span><span class="n">conv_fwd</span><span class="o">::</span><span class="n">desc</span><span class="p">(</span><span class="cm">/* all the setting above*/</span><span class="p">);</span>
  <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mkldnn</span><span class="o">::</span><span class="n">conv_fwd</span><span class="o">::</span><span class="n">primitive_desc</span><span class="o">&gt;</span> <span class="n">fwd_primitive_desc</span><span class="p">(</span><span class="k">new</span> <span class="n">mkldnn</span><span class="o">::</span><span class="n">conv_fwd</span><span class="o">::</span><span class="n">primitive_desc</span><span class="p">(</span><span class="n">fwd_desc</span><span class="p">,</span> <span class="n">ctx</span><span class="p">.</span><span class="n">getEngine</span><span class="p">()));</span>

  <span class="k">return</span> <span class="n">fwd_primitive_desc</span><span class="p">;</span>
  <span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="mkldnndevicecontext">
<span id="mkldnndevicecontext"></span><h3>MKLDNNDeviceContext<a class="headerlink" href="#mkldnndevicecontext" title="永久链接至标题">¶</a></h3>
<p><code class="docutils literal"><span class="pre">MKLDNNDeviceContext</span></code>, which is very straightforward, should contain some base information like: <code class="docutils literal"><span class="pre">stream</span></code>, <code class="docutils literal"><span class="pre">engine</span></code> and the map needed.</p>
</div>
<div class="section" id="mkldnn-helper">
<span id="mkldnn-helper"></span><h3>mkldnn_helper<a class="headerlink" href="#mkldnn-helper" title="永久链接至标题">¶</a></h3>
<p>Some functions would be put in <code class="docutils literal"><span class="pre">paddle/platform/mkldnn_helper.h</span></code>.</p>
<ul class="simple">
<li>create MKLDNN memories</li>
<li>create MKLDNN primitives</li>
<li>error check function</li>
<li>etc</li>
</ul>
</div>
<div class="section" id="kernel-switch">
<span id="kernel-switch"></span><h3>Kernel Switch<a class="headerlink" href="#kernel-switch" title="永久链接至标题">¶</a></h3>
<p>We should <code class="docutils literal"><span class="pre">reorder</span></code> the different Layout from other device or to other device. <code class="docutils literal"><span class="pre">GetExpectedKernelType</span></code> and <code class="docutils literal"><span class="pre">trans</span></code> functions can help us to implement it.</p>
<p><code class="docutils literal"><span class="pre">GetExpectedKernelType</span></code> should get the context, and this operator can return the best <code class="docutils literal"><span class="pre">KernelType</span></code>.
<code class="docutils literal"><span class="pre">trans</span></code> would be like this:</p>
<div class="highlight-c++"><div class="highlight"><pre><span></span><span class="kt">void</span> <span class="nf">trans</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">NoNeedTrans</span><span class="p">())</span> <span class="p">{</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="c1">// find reorder primitive by op_key from context</span>
  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">dev_ctx</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="k">template</span> <span class="n">device_context</span><span class="o">&lt;</span><span class="n">platform</span><span class="o">::</span><span class="n">MKLDNNDeviceContext</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">p</span> <span class="o">=</span> <span class="n">dev_ctx</span><span class="p">.</span><span class="n">findPrimitive</span><span class="p">(</span><span class="n">op_key</span> <span class="o">+</span> <span class="s">"_reorder_input"</span><span class="p">);</span>
  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">i</span> <span class="o">=</span> <span class="n">dev_ctx</span><span class="p">.</span><span class="n">findMemory</span><span class="p">(</span><span class="n">op_key</span> <span class="o">+</span> <span class="s">"_src_input"</span><span class="p">);</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="k">nullptr</span> <span class="o">||</span> <span class="n">i</span> <span class="o">==</span> <span class="k">nullptr</span> <span class="o">||</span> <span class="n">changeSized</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">input</span><span class="p">))</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">prim</span> <span class="o">=</span> <span class="n">createPrimitiveDesc</span><span class="p">(</span><span class="n">ctx</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">src</span> <span class="o">=</span> <span class="n">createMemory</span><span class="p">(</span><span class="n">memoryDesc</span><span class="p">(</span><span class="n">input</span><span class="o">-&gt;</span><span class="n">dims</span><span class="p">(),</span> <span class="n">actual_layout</span><span class="p">),</span> <span class="n">input</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">newbuffer</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">::</span><span class="n">memory</span><span class="o">::</span><span class="n">Alloc</span><span class="p">(</span><span class="n">ctx</span><span class="p">.</span><span class="n">GetPlace</span><span class="p">(),</span> <span class="n">input</span><span class="o">-&gt;</span><span class="n">size_in_bytes</span><span class="p">());</span>
    <span class="k">auto</span> <span class="n">dst</span> <span class="o">=</span> <span class="n">createMemory</span><span class="p">(</span><span class="n">p</span><span class="o">-&gt;</span><span class="n">expected_desc</span><span class="p">(),</span> <span class="n">newbuffer</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">reorder_primitive</span><span class="p">(</span><span class="k">new</span> <span class="n">mkldnn</span><span class="o">::</span><span class="n">reorder</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">));</span>

    <span class="n">dev_ctx</span><span class="p">.</span><span class="n">addMemory</span><span class="p">(</span><span class="n">op_key</span><span class="o">+</span><span class="s">"_src_input"</span><span class="p">,</span> <span class="n">src</span><span class="p">);</span>
    <span class="n">dev_ctx</span><span class="p">.</span><span class="n">addMemory</span><span class="p">(</span><span class="n">op_key</span><span class="o">+</span><span class="s">"_input"</span><span class="p">,</span> <span class="n">dst</span><span class="p">);</span>
    <span class="n">dev_ctx</span><span class="p">.</span><span class="n">addPrimitive</span><span class="p">(</span><span class="n">op_key</span><span class="o">+</span><span class="s">"_reorder_input"</span><span class="p">,</span> <span class="n">reorder_primitive</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="n">p</span> <span class="o">=</span> <span class="n">dev_ctx</span><span class="p">.</span><span class="n">findPrimitive</span><span class="p">(</span><span class="n">op_key</span> <span class="o">+</span> <span class="s">"_reorder_input"</span><span class="p">);</span>
  <span class="n">PADDLE_ENFORCE</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s">"Should have Reorder Primitive"</span><span class="p">);</span>
  <span class="n">dev_ctx</span><span class="p">.</span><span class="n">submit</span><span class="p">(</span><span class="n">p</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">isMKLDNNKernel</span><span class="p">())</span> <span class="p">{</span>
    <span class="c1">// execute immediately only if this is not mkldnn kernel function.</span>
    <span class="c1">// otherwise, it can be executed with the operator primitive in Compute</span>
    <span class="n">dev_ctx</span><span class="p">.</span><span class="n">stream</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="c1">// after submit, the input tensor in ExecutionContext should be changed as the converted one</span>
  <span class="c1">// there should be another mechanism to ensure this</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="unit-test">
<span id="unit-test"></span><h3>Unit Test<a class="headerlink" href="#unit-test" title="永久链接至标题">¶</a></h3>
<p>All the functions should be tested corresponding.
TBD</p>
</div>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>