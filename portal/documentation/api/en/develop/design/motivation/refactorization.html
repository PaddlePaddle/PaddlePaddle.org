<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="design-doc-refactorization-overview">
<span id="design-doc-refactorization-overview"></span><h1>Design Doc: Refactorization Overview<a class="headerlink" href="#design-doc-refactorization-overview" title="永久链接至标题">¶</a></h1>
<p>The goals of refactoring include:</p>
<ol class="simple">
<li>Making it easy for external contributors to write new elementary computation operations.</li>
<li>Making the codebase clean and readable.</li>
<li>Designing a new computation representation – a computation graph of operators and variables.</li>
<li>Implementing auto-scalability and auto fault recoverable distributed computing with the help of computation graphs.</li>
</ol>
<div class="section" id="computation-graphs">
<span id="computation-graphs"></span><h2>Computation Graphs<a class="headerlink" href="#computation-graphs" title="永久链接至标题">¶</a></h2>
<ol class="simple">
<li>PaddlePaddle represents the computation, training and inference of Deep Learning models, by computation graphs.</li>
<li>Please refer to <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/others/graph.md">computation graphs</a> for a concrete example.</li>
<li>Users write Python programs to describe the graphs and run them (locally or remotely).</li>
<li>A graph is composed of <em>variables</em> and <em>operators</em>.</li>
<li>The description of graphs must be serializable/deserializable, so that:<ol>
<li>It can be sent to the cloud for distributed execution, and</li>
<li>It can be sent to clients for mobile or enterprise deployment.</li>
</ol>
</li>
<li>The Python program does two things<ol>
<li><em>Compilation</em> runs a Python program to generate a protobuf message representation of the graph and send it to<ol>
<li>the C++ library <code class="docutils literal"><span class="pre">libpaddle.so</span></code> for local execution,</li>
<li>the master process of a distributed training job for training, or</li>
<li>the server process of a Kubernetes serving job for distributed serving.</li>
</ol>
</li>
<li><em>Execution</em> executes the graph by constructing instances of class <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/variable.h#L24"><code class="docutils literal"><span class="pre">Variable</span></code></a> and <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/operator.h#L70"><code class="docutils literal"><span class="pre">OperatorBase</span></code></a>, according to the protobuf message.</li>
</ol>
</li>
</ol>
</div>
<div class="section" id="description-and-realization-of-computation-graph">
<span id="description-and-realization-of-computation-graph"></span><h2>Description and Realization of Computation Graph<a class="headerlink" href="#description-and-realization-of-computation-graph" title="永久链接至标题">¶</a></h2>
<p>At compile time, the Python program generates a protobuf message representation of the graph, or a description of the graph.</p>
<p>At runtime, the C++ program realizes the graph and runs it.</p>
<table>
<thead>
<tr>
<th></th>
<th>Representation (protobuf messages)</th>
<th>Realization (C++ class objects) </th>
</tr>
</thead>
<tbody>
<tr>
<td>Data</td>
<td>
<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/framework.proto#L107">VarDesc</a></td>
<td>
<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/variable.h#L24">Variable</a></td>
</tr>
<tr>
<td>Operation </td>
<td>
<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/framework.proto#L35">OpDesc</a></td>
<td>
<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/operator.h#L64">Operator</a></td>
</tr>
<tr>
<td>Block </td>
<td>BlockDesc </td>
<td>Block </td></tr></tbody>
</table><p>The word <em>graph</em> is interchangeable with <em>block</em> in this document.  A graph consists of computation steps and local variables similar to a C++/Java program block, or a pair of parentheses(<code class="docutils literal"><span class="pre">{</span></code> and <code class="docutils literal"><span class="pre">}</span></code>).</p>
</div>
<div class="section" id="compilation-and-execution">
<span id="compilation-and-execution"></span><h2>Compilation and Execution<a class="headerlink" href="#compilation-and-execution" title="永久链接至标题">¶</a></h2>
<ol class="simple">
<li>Run a Python program to describe the graph.  In particular, the Python application program does the following:<ol>
<li>Create <code class="docutils literal"><span class="pre">VarDesc</span></code> to represent local/intermediate variables,</li>
<li>Create operators and set attributes,</li>
<li>Validate attribute values,</li>
<li>Infer the type and the shape of variables,</li>
<li>Plan memory-reuse for variables,</li>
<li>Generate the backward graph</li>
<li>Add optimization operators to the computation graph.</li>
<li>Optionally, split the graph for distributed training.</li>
</ol>
</li>
<li>The invocation of <code class="docutils literal"><span class="pre">train</span></code> or <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/inference.py#L108"><code class="docutils literal"><span class="pre">infer</span></code></a> methods in the Python program does the following:<ol>
<li>Create a new Scope instance in the <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/concepts/scope.md">scope hierarchy</a> for each run of a block,<ol>
<li>realize local variables defined in the BlockDesc message in the new scope,</li>
<li>a scope is similar to the stack frame in programming languages,</li>
</ol>
</li>
<li>Create an instance of class <code class="docutils literal"><span class="pre">Block</span></code>, in which,<ol>
<li>realize operators in the BlockDesc message,</li>
</ol>
</li>
<li>Run the Block by calling<ol>
<li><code class="docutils literal"><span class="pre">Block::Eval(vector&lt;Variable&gt;*</span> <span class="pre">targets)</span></code> for forward and backward computations, or</li>
<li><code class="docutils literal"><span class="pre">Block::Eval(vector&lt;Operator&gt;*</span> <span class="pre">targets)</span></code> for optimization.</li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
<div class="section" id="intermediate-representation-ir">
<span id="intermediate-representation-ir"></span><h2>Intermediate Representation (IR)<a class="headerlink" href="#intermediate-representation-ir" title="永久链接至标题">¶</a></h2>
<div class="highlight-text"><div class="highlight"><pre><span></span>Compile Time -&gt; IR -&gt; Runtime
</pre></div>
</div>
<div class="section" id="benefits-of-ir">
<span id="benefits-of-ir"></span><h3>Benefits of IR<a class="headerlink" href="#benefits-of-ir" title="永久链接至标题">¶</a></h3>
<ul>
<li><p class="first">Optimization</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>Compile Time -&gt; IR -&gt; Optimized IR -&gt; Runtime
</pre></div>
</div>
</li>
<li><p class="first">Automatically send partitioned IR to different nodes.</p>
<ul>
<li><p class="first">Automatic Data Parallelism</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>Compile Time
|-&gt; Single GPU IR
    |-&gt; [trainer-IR-0, trainer-IR-1, pserver-IR]
        |-&gt; Node-0 (runs trainer-IR-0)
        |-&gt; Node-1 (runs trainer-IR-1)
        |-&gt; Node-2 (runs pserver-IR)
</pre></div>
</div>
</li>
<li><p class="first">Automatic Model Parallelism (planned for future)</p>
</li>
</ul>
</li>
</ul>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="operator-opwithkernel-opkernel">
<span id="operator-opwithkernel-opkernel"></span><h2>Operator/OpWithKernel/OpKernel<a class="headerlink" href="#operator-opwithkernel-opkernel" title="永久链接至标题">¶</a></h2>
<p><img alt="class_diagram" src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/op_op_with_kern_class_diagram.dot"/></p>
</div>
<hr class="docutils"/>
<div class="section" id="operator">
<span id="operator"></span><h2>Operator<a class="headerlink" href="#operator" title="永久链接至标题">¶</a></h2>
<p><img alt="class_diagram" src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/op.dot"/></p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">Operator</span></code> is the fundamental building block of the user interface.<ul>
<li>Operator stores input/output variable names and attributes.</li>
<li>The <code class="docutils literal"><span class="pre">InferShape</span></code> interface is used to infer the shape of the output variables based on the shapes of the input variables.</li>
<li>Use <code class="docutils literal"><span class="pre">Run</span></code> to compute the <code class="docutils literal"><span class="pre">output</span></code> variables from the <code class="docutils literal"><span class="pre">input</span></code> variables.</li>
</ul>
</li>
</ul>
</div>
<hr class="docutils"/>
<div class="section" id="opwithkernel-kernel">
<span id="opwithkernel-kernel"></span><h2>OpWithKernel/Kernel<a class="headerlink" href="#opwithkernel-kernel" title="永久链接至标题">¶</a></h2>
<p><img alt="class_diagram" src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/op_with_kernel.dot"/></p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">OpWithKernel</span></code> inherits <code class="docutils literal"><span class="pre">Operator</span></code>.</li>
<li><code class="docutils literal"><span class="pre">OpWithKernel</span></code> contains a Kernel map.<ul>
<li><code class="docutils literal"><span class="pre">OpWithKernel::Run</span></code> get device’s kernel, and invoke <code class="docutils literal"><span class="pre">OpKernel::Compute</span></code>.</li>
<li><code class="docutils literal"><span class="pre">OpKernelKey</span></code> is the map key. Only device place now, but may be data type later.</li>
</ul>
</li>
</ul>
</div>
<hr class="docutils"/>
<div class="section" id="why-separate-kernel-and-operator">
<span id="why-separate-kernel-and-operator"></span><h2>Why separate Kernel and Operator<a class="headerlink" href="#why-separate-kernel-and-operator" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li>Separate GPU and CPU code.<ul>
<li>Make Paddle capable of running without GPU.</li>
</ul>
</li>
<li>Make one operator (which is a user interface) and create many implementations.<ul>
<li>For example, same multiplication op can have different implementations kernels such as FP16 kernel, FP32 kernel, MKL, eigen kernel.</li>
</ul>
</li>
</ul>
</div>
<hr class="docutils"/>
<div class="section" id="libraries-for-kernel-development">
<span id="libraries-for-kernel-development"></span><h2>Libraries for Kernel development<a class="headerlink" href="#libraries-for-kernel-development" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">Eigen::Tensor</span></code> contains basic math and element-wise functions.<ul>
<li>Note that <code class="docutils literal"><span class="pre">Eigen::Tensor</span></code> has broadcast implementation.</li>
<li>Limit the number of <code class="docutils literal"><span class="pre">tensor.device(dev)</span> <span class="pre">=</span></code> in your code.</li>
</ul>
</li>
<li><code class="docutils literal"><span class="pre">thrust::transform</span></code> and <code class="docutils literal"><span class="pre">std::transform</span></code>.<ul>
<li><code class="docutils literal"><span class="pre">thrust</span></code> has the same API as C++ standard library. Using <code class="docutils literal"><span class="pre">transform</span></code>, one can quickly implement customized element-wise kernels.</li>
<li><code class="docutils literal"><span class="pre">thrust</span></code>, in addition, supports more complex APIs, like <code class="docutils literal"><span class="pre">scan</span></code>, <code class="docutils literal"><span class="pre">reduce</span></code>, <code class="docutils literal"><span class="pre">reduce_by_key</span></code>.</li>
</ul>
</li>
<li>Hand-writing <code class="docutils literal"><span class="pre">GPUKernel</span></code> and <code class="docutils literal"><span class="pre">CPU</span></code> code<ul>
<li>Do not write in header (<code class="docutils literal"><span class="pre">.h</span></code>) files. CPU Kernel should be in cpp source (<code class="docutils literal"><span class="pre">.cc</span></code>) and GPU kernels should be in cuda (<code class="docutils literal"><span class="pre">.cu</span></code>) files. (GCC cannot compile GPU code.)</li>
</ul>
</li>
</ul>
</div>
<hr class="docutils"/>
<div class="section" id="operator-registration">
<span id="operator-registration"></span><h2>Operator Registration<a class="headerlink" href="#operator-registration" title="永久链接至标题">¶</a></h2>
<div class="section" id="why-is-registration-necessary">
<span id="why-is-registration-necessary"></span><h3>Why is registration necessary?<a class="headerlink" href="#why-is-registration-necessary" title="永久链接至标题">¶</a></h3>
<p>We need a method to build mappings between Op type names and Op classes.</p>
</div>
<div class="section" id="how-is-registration-implemented">
<span id="how-is-registration-implemented"></span><h3>How is registration implemented?<a class="headerlink" href="#how-is-registration-implemented" title="永久链接至标题">¶</a></h3>
<p>Maintaining a map, whose key is the type name and the value is the corresponding Op constructor.</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="the-registry-map">
<span id="the-registry-map"></span><h2>The Registry Map<a class="headerlink" href="#the-registry-map" title="永久链接至标题">¶</a></h2>
<div class="section" id="opinfomap">
<span id="opinfomap"></span><h3><code class="docutils literal"><span class="pre">OpInfoMap</span></code><a class="headerlink" href="#opinfomap" title="永久链接至标题">¶</a></h3>
<p><code class="docutils literal"><span class="pre">op_type(string)</span></code> -&gt; <code class="docutils literal"><span class="pre">OpInfo</span></code></p>
<p><code class="docutils literal"><span class="pre">OpInfo</span></code>:</p>
<ul class="simple">
<li><strong><code class="docutils literal"><span class="pre">creator</span></code></strong>: The Op constructor.</li>
<li><strong><code class="docutils literal"><span class="pre">grad_op_type</span></code></strong>: The type of the gradient Op.</li>
<li><strong><code class="docutils literal"><span class="pre">proto</span></code></strong>: The Op’s Protobuf, including inputs, outputs and required attributes.</li>
<li><strong><code class="docutils literal"><span class="pre">checker</span></code></strong>: Used to check attributes.</li>
</ul>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="related-concepts">
<span id="related-concepts"></span><h2>Related Concepts<a class="headerlink" href="#related-concepts" title="永久链接至标题">¶</a></h2>
<div class="section" id="op-maker">
<span id="op-maker"></span><h3>Op_Maker<a class="headerlink" href="#op-maker" title="永久链接至标题">¶</a></h3>
<p>It’s constructor takes <code class="docutils literal"><span class="pre">proto</span></code> and <code class="docutils literal"><span class="pre">checker</span></code>. They are completed during Op_Maker’s construction. (<a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/operators/scale_op.cc#L37">ScaleOpMaker</a>)</p>
</div>
<div class="section" id="register-macros">
<span id="register-macros"></span><h3>Register Macros<a class="headerlink" href="#register-macros" title="永久链接至标题">¶</a></h3>
<div class="highlight-cpp"><div class="highlight"><pre><span></span><span class="n">REGISTER_OP</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">op_class</span><span class="p">,</span> <span class="n">op_maker_class</span><span class="p">,</span> <span class="n">grad_op_type</span><span class="p">,</span> <span class="n">grad_op_class</span><span class="p">)</span>
<span class="n">REGISTER_OP_WITHOUT_GRADIENT</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">op_class</span><span class="p">,</span> <span class="n">op_maker_class</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="registration-process">
<span id="registration-process"></span><h2>Registration Process<a class="headerlink" href="#registration-process" title="永久链接至标题">¶</a></h2>
<ol class="simple">
<li>Write an Op class and its gradient Op class, if required.</li>
<li>Write an Op maker class. In the constructor of this class, describe the inputs, outputs and attributes of the operator.</li>
<li>Invoke the macro <code class="docutils literal"><span class="pre">REGISTER_OP</span></code>. This macro will<ol>
<li>Call maker class to complete <code class="docutils literal"><span class="pre">proto</span></code> and <code class="docutils literal"><span class="pre">checker</span></code></li>
<li>Using the completed <code class="docutils literal"><span class="pre">proto</span></code> and <code class="docutils literal"><span class="pre">checker</span></code>, it will add a new key-value pair to the <code class="docutils literal"><span class="pre">OpInfoMap</span></code></li>
</ol>
</li>
</ol>
</div>
<hr class="docutils"/>
<div class="section" id="backward-module-1-2">
<span id="backward-module-1-2"></span><h2>Backward Module (1/2)<a class="headerlink" href="#backward-module-1-2" title="永久链接至标题">¶</a></h2>
<div class="section" id="create-backward-operator">
<span id="create-backward-operator"></span><h3>Create Backward Operator<a class="headerlink" href="#create-backward-operator" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li>Mapping from forward Op to backward Op
<img alt="backward" src="https://gist.githubusercontent.com/dzhwinter/a6fbd4623ee76c459f7f94591fd1abf0/raw/61026ab6e518e66bde66a889bc42557a1fccff33/backward.png"/></li>
</ul>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="backward-module-2-2">
<span id="backward-module-2-2"></span><h2>Backward Module (2/2)<a class="headerlink" href="#backward-module-2-2" title="永久链接至标题">¶</a></h2>
<div class="section" id="build-backward-network">
<span id="build-backward-network"></span><h3>Build Backward Network<a class="headerlink" href="#build-backward-network" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><strong>Input</strong>: a graph of forward operators</li>
<li><strong>Output</strong>: a graph of backward operators</li>
<li><strong>Corner cases in construction</strong><ul>
<li>Shared Variables =&gt; insert an <code class="docutils literal"><span class="pre">Add</span></code> operator to combine gradients</li>
<li>No Gradient =&gt; insert a <code class="docutils literal"><span class="pre">fill_zero_grad</span></code> operator</li>
<li>Recursive NetOp =&gt; call <code class="docutils literal"><span class="pre">Backward</span></code> recursively</li>
<li>RNN Op =&gt; recursively call <code class="docutils literal"><span class="pre">Backward</span></code> on stepnet</li>
<li>RNN Op =&gt; recursively call <code class="docutils literal"><span class="pre">Backward</span></code> on stepnet</li>
</ul>
</li>
</ul>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="scope-variable-tensor">
<span id="scope-variable-tensor"></span><h2>Scope, Variable, Tensor<a class="headerlink" href="#scope-variable-tensor" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">Tensor</span></code> is an n-dimension array with type.<ul>
<li>Only dims and data pointers are stored in <code class="docutils literal"><span class="pre">Tensor</span></code>.</li>
<li>All operations on <code class="docutils literal"><span class="pre">Tensor</span></code> are written in <code class="docutils literal"><span class="pre">Operator</span></code> or global functions.</li>
<li>Variable length Tensor design <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/concepts/lod_tensor.md">LoDTensor</a></li>
</ul>
</li>
<li><code class="docutils literal"><span class="pre">Variable</span></code> instances are the inputs and the outputs of an operator, not just <code class="docutils literal"><span class="pre">Tensor</span></code>.<ul>
<li><code class="docutils literal"><span class="pre">step_scopes</span></code> in RNN is a variable and not a tensor.</li>
</ul>
</li>
<li><code class="docutils literal"><span class="pre">Scope</span></code> is where variables are stored.<ul>
<li>map&lt;string <code class="docutils literal"><span class="pre">var</span> <span class="pre">name</span></code>, Variable&gt;</li>
<li><code class="docutils literal"><span class="pre">Scope</span></code> has a hierarchical structure. The local scope can get variables from its parent scope.</li>
</ul>
</li>
</ul>
</div>
<hr class="docutils"/>
<div class="section" id="block-in-design">
<span id="block-in-design"></span><h2>Block (in design)<a class="headerlink" href="#block-in-design" title="永久链接至标题">¶</a></h2>
<div class="section" id="the-difference-between-original-rnnop-and-block">
<span id="the-difference-between-original-rnnop-and-block"></span><h3>the difference between original RNNOp and Block<a class="headerlink" href="#the-difference-between-original-rnnop-and-block" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li>As an operator is more intuitive than <code class="docutils literal"><span class="pre">RNNOp</span></code>,</li>
<li>Offers a new interface <code class="docutils literal"><span class="pre">Eval(targets)</span></code> to deduce the minimal block to <code class="docutils literal"><span class="pre">Run</span></code>,</li>
<li>Fits the compile-time/ runtime separation design paradigm.<ul>
<li>During the compilation, <code class="docutils literal"><span class="pre">SymbolTable</span></code> stores <code class="docutils literal"><span class="pre">VarDesc</span></code>s and <code class="docutils literal"><span class="pre">OpDesc</span></code>s and serialize to a <code class="docutils literal"><span class="pre">BlockDesc</span></code></li>
<li>When graph executes, a Block with <code class="docutils literal"><span class="pre">BlockDesc</span></code> is passed. It then creates <code class="docutils literal"><span class="pre">Op</span></code> and <code class="docutils literal"><span class="pre">Var</span></code> instances and then invokes <code class="docutils literal"><span class="pre">Run</span></code>.</li>
</ul>
</li>
</ul>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="milestone">
<span id="milestone"></span><h2>Milestone<a class="headerlink" href="#milestone" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li>Take Paddle/books as the main line, the requirement of the models motivates framework refactoring,</li>
<li>Model migration<ul>
<li>Framework development gives <strong>priority support</strong> to model migration, for example,<ul>
<li>the MNIST demo needs a Python interface,</li>
<li>the RNN models require the framework to support <code class="docutils literal"><span class="pre">LoDTensor</span></code>.</li>
</ul>
</li>
<li>Determine some timelines,</li>
<li>Frequently used Ops need to be migrated first,</li>
<li>Different models can be migrated in parallel.</li>
</ul>
</li>
<li>Improve the framework at the same time</li>
<li>Accept imperfection, concentrate on solving the specific problem at the right price.</li>
</ul>
</div>
<hr class="docutils"/>
<div class="section" id="control-the-migration-quality">
<span id="control-the-migration-quality"></span><h2>Control the migration quality<a class="headerlink" href="#control-the-migration-quality" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li>Compare the performance of migrated models with old ones.</li>
<li>Follow the google C++ style guide.</li>
<li>Build the automatic workflow of generating Python/C++ documentations.<ul>
<li>The documentation of layers and ops should be written inside the code.</li>
<li>Take the documentation quality into account when submitting pull requests.</li>
<li>Preview the documentations, read and improve them from a user’s perspective.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>