<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="design-doc-float16">
<span id="design-doc-float16"></span><h1>Design Doc: float16<a class="headerlink" href="#design-doc-float16" title="永久链接至标题">¶</a></h1>
<div class="section" id="why-float16">
<span id="why-float16"></span><h2>Why float16<a class="headerlink" href="#why-float16" title="永久链接至标题">¶</a></h2>
<p>Half precision (float16) is a binary floating-point format that occupies 16 bits in memory. float16 is half the size of traditional 32-bit single precision format (float) and has lower precision and smaller range.</p>
<p>When high precision computation is not required (which is usually the case at least in the deep learning inference stage), using float16 data type could potentially</p>
<ul class="simple">
<li>reduce storage space, memory bandwidth, and power usages;</li>
<li>increase the chance of data fitting into a smaller cache of lower latency;</li>
<li>provide arithmetic speed up if supported by hardware.</li>
</ul>
</div>
<div class="section" id="survey-of-current-float16-support">
<span id="survey-of-current-float16-support"></span><h2>Survey of current float16 support<a class="headerlink" href="#survey-of-current-float16-support" title="永久链接至标题">¶</a></h2>
<p>A brief survey of float16 support on different compilers, hardwares, and libraries can be found below. Interested readers can refer to <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/issues/4853">link1</a> and <a class="reference external" href="https://github.com/Xreki/Xreki.github.io/blob/master/multi_data_types_in_dl_framework/ppt/float16_and_quantized_type.md">link2</a> for more info.</p>
<p>The goal of float16 is to serve as a key for the executor to find and run the correct version of compute method specialized for float16 in operator kernels. It should be compatible with various natively supported float16 implementations including <code class="docutils literal"><span class="pre">__half</span></code> for cuda, <code class="docutils literal"><span class="pre">float16_t</span></code> for ARM, and <code class="docutils literal"><span class="pre">Eigen::half</span></code> for Eigen to make writing customized float16 kernels easier.</p>
<div class="section" id="compiler">
<span id="compiler"></span><h3>Compiler<a class="headerlink" href="#compiler" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li>nvcc supports <code class="docutils literal"><span class="pre">__half</span></code> data type after CUDA 7.5.</li>
<li><code class="docutils literal"><span class="pre">__fp16</span></code> or <code class="docutils literal"><span class="pre">float16_t</span></code> is supported as storage type for gcc &gt;= 6.1 and clang &gt;= 3.4.</li>
<li><code class="docutils literal"><span class="pre">__fp16</span></code> or <code class="docutils literal"><span class="pre">float16_t</span></code> is supported as arithmetic type for gcc &gt;= 7.1 and clang &gt;= 3.9.</li>
</ul>
</div>
<div class="section" id="hardware">
<span id="hardware"></span><h3>Hardware<a class="headerlink" href="#hardware" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">__half</span></code> is supported on GPU with compute capability &gt;= 5.3.</li>
<li><code class="docutils literal"><span class="pre">__fp16</span></code> is supported as storage type for ARMv7-A, ARMv8-A, and above.</li>
<li><code class="docutils literal"><span class="pre">__fp16</span></code> is supported as arithmetic type after ARMv8.2-A (currently, the only microarchitecture implementing ARMv8.2-A is ARM Cortex-A75, which is announced in May 2017. There seems to be no application processors currently available on market that adopts this architecture. It is reported that Qualcomm Snapdragon 845 uses Cortex-A75 design and will be available in mobile devices in early 2018).</li>
</ul>
</div>
<div class="section" id="libraries">
<span id="libraries"></span><h3>Libraries<a class="headerlink" href="#libraries" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/RLovelett/eigen">Eigen</a> &gt;= 3.3 supports float16 calculation on both GPU and CPU using the <code class="docutils literal"><span class="pre">Eigen::half</span></code> class. It is mostly useful for Nvidia GPUs because of the overloaded arithmetic operators using cuda intrinsics. It falls back to using software emulation on CPU for calculation and there is no special treatment to ARM processors.</li>
<li><a class="reference external" href="https://github.com/ARM-software/ComputeLibrary">ARM compute library</a> &gt;= 17.02.01 supports NEON FP16 kernels (requires ARMv8.2-A CPU).</li>
</ul>
</div>
<div class="section" id="cuda-version-issue">
<span id="cuda-version-issue"></span><h3>CUDA version issue<a class="headerlink" href="#cuda-version-issue" title="永久链接至标题">¶</a></h3>
<p>There are currently three versions of CUDA that supports <code class="docutils literal"><span class="pre">__half</span></code> data type, namely, CUDA 7.5, 8.0, and 9.0.
CUDA 7.5 and 8.0 define <code class="docutils literal"><span class="pre">__half</span></code> as a simple struct that has a <code class="docutils literal"><span class="pre">uint16_t</span></code> data (see <a class="reference external" href="https://github.com/ptillet/isaac/blob/9212ab5a3ddbe48f30ef373f9c1fb546804c7a8c/include/isaac/external/CUDA/cuda_fp16.h"><code class="docutils literal"><span class="pre">cuda_fp16.h</span></code></a>) as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">typedef</span> <span class="n">struct</span> <span class="n">__align__</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
   <span class="n">unsigned</span> <span class="n">short</span> <span class="n">x</span><span class="p">;</span>
<span class="p">}</span> <span class="n">__half</span><span class="p">;</span>

<span class="n">typedef</span> <span class="n">__half</span> <span class="n">half</span><span class="p">;</span>
</pre></div>
</div>
<p>This struct does not define any overloaded arithmetic operators. So you have to directly use <code class="docutils literal"><span class="pre">__hadd</span></code> instead of <code class="docutils literal"><span class="pre">+</span></code> to correctly add two half types:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">__global__</span> <span class="n">void</span> <span class="n">Add</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">half</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">;</span>
  <span class="n">c</span> <span class="o">=</span> <span class="n">__hadd</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">);</span> <span class="o">//</span> <span class="n">correct</span>
  <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">;</span> <span class="o">//</span> <span class="n">compiler</span> <span class="n">error</span><span class="p">:</span> <span class="n">no</span> <span class="n">operator</span> <span class="s2">"+"</span> <span class="n">matches</span> <span class="n">these</span> <span class="n">operands</span>
<span class="p">}</span>
</pre></div>
</div>
<p>CUDA 9.0 provides a major update to the half data type. The related code can be found in the updated <a class="reference external" href="https://github.com/ptillet/isaac/blob/master/include/isaac/external/CUDA/cuda_fp16.h"><code class="docutils literal"><span class="pre">cuda_fp16.h</span></code></a> and the newly added <a class="reference external" href="https://github.com/ptillet/isaac/blob/master/include/isaac/external/CUDA/cuda_fp16.hpp"><code class="docutils literal"><span class="pre">cuda_fp16.hpp</span></code></a>.</p>
<p>Essentially, CUDA 9.0 renames the original <code class="docutils literal"><span class="pre">__half</span></code> type in 7.5 and 8.0 as <code class="docutils literal"><span class="pre">__half_raw</span></code>, and defines a new <code class="docutils literal"><span class="pre">__half</span></code> class type that has constructors, conversion operators, and also provides overloaded arithmetic operators such as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">typedef</span> <span class="n">struct</span> <span class="n">__CUDA_ALIGN__</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">unsigned</span> <span class="n">short</span> <span class="n">x</span><span class="p">;</span>
<span class="p">}</span> <span class="n">__half_raw</span><span class="p">;</span>


<span class="n">struct</span> <span class="n">__CUDA_ALIGN__</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="n">__half</span> <span class="p">{</span>
<span class="n">protected</span><span class="p">:</span>
    <span class="n">unsigned</span> <span class="n">short</span> <span class="n">__x</span><span class="p">;</span>
<span class="n">public</span><span class="p">:</span>
    <span class="o">//</span> <span class="n">constructors</span> <span class="ow">and</span> <span class="n">conversion</span> <span class="n">operators</span> <span class="n">from</span><span class="o">/</span><span class="n">to</span> 
    <span class="o">//</span> <span class="n">__half_raw</span> <span class="ow">and</span> <span class="n">other</span> <span class="n">built</span><span class="o">-</span><span class="ow">in</span> <span class="n">data</span> <span class="n">types</span>
<span class="p">}</span>

<span class="n">typedef</span> <span class="n">__half</span> <span class="n">half</span><span class="p">;</span>

<span class="n">__device__</span> <span class="n">__forceinline__</span> 
<span class="n">__half</span> <span class="n">operator</span><span class="o">+</span><span class="p">(</span><span class="n">const</span> <span class="n">__half</span> <span class="o">&amp;</span><span class="n">lh</span><span class="p">,</span> <span class="n">const</span> <span class="n">__half</span> <span class="o">&amp;</span><span class="n">rh</span><span class="p">)</span> <span class="p">{</span> 
    <span class="k">return</span> <span class="n">__hadd</span><span class="p">(</span><span class="n">lh</span><span class="p">,</span> <span class="n">rh</span><span class="p">);</span> 
<span class="p">}</span>

<span class="o">//</span> <span class="n">Other</span> <span class="n">overloaded</span> <span class="n">operators</span>
</pre></div>
</div>
<p>This new design makes <code class="docutils literal"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">b</span></code> work correctly for CUDA half data type.</p>
</div>
</div>
<div class="section" id="implementation">
<span id="implementation"></span><h2>Implementation<a class="headerlink" href="#implementation" title="永久链接至标题">¶</a></h2>
<p>The float16 class holds a 16-bit <code class="docutils literal"><span class="pre">uint16_t</span></code> data internally.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">struct</span> <span class="n">float16</span> <span class="p">{</span>
  <span class="n">uint16_t</span> <span class="n">x</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>float16 supports the following features:</p>
<ul class="simple">
<li>constructors / assignment operators that take input from primitive data types including bool, integers of various length, float, and double.</li>
<li>constructors / assignment operators that take input from <code class="docutils literal"><span class="pre">__half</span></code> on cuda, <code class="docutils literal"><span class="pre">float16_t</span></code> on ARM, and <code class="docutils literal"><span class="pre">Eigen::half</span></code> on Eigen.</li>
<li>conversion operators to primitive data types and half precision data types on cuda, ARM and Eigen.</li>
<li>overloaded arithmetic operators for cuda, arm, and non-arm cpu, respectively. These operators will take advantage of the cuda and ARM intrinsics on the corresponding hardware.</li>
</ul>
<p>To support the above features, two fundamental conversion functions are provided:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">float16</span> <span class="n">float_to_half_rn</span><span class="p">(</span><span class="nb">float</span> <span class="n">f</span><span class="p">);</span>  <span class="o">//</span> <span class="n">convert</span> <span class="n">to</span> <span class="n">half</span> <span class="n">precision</span> <span class="ow">in</span> <span class="nb">round</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">nearest</span><span class="o">-</span><span class="n">even</span> <span class="n">mode</span>
<span class="nb">float</span> <span class="n">half_to_float</span><span class="p">(</span><span class="n">float16</span> <span class="n">h</span><span class="p">);</span>
</pre></div>
</div>
<p>which provides one-to-one conversion between float32 and float16. These twos functions will do different conversion routines based on the current hardware. CUDA/ARM instrinsics will be used when the corresonding hardware is available. If the hardware or compiler level does not support float32 to float16 conversion, software emulation will be performed to do the conversion.</p>
</div>
<div class="section" id="float16-inference">
<span id="float16-inference"></span><h2>float16 inference<a class="headerlink" href="#float16-inference" title="永久链接至标题">¶</a></h2>
<p>In Fluid, a neural network is represented as a protobuf message called <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/concepts/program.md">ProgramDesc</a>, whose Python wrapper is a <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/modules/python_api.md#program">Program</a>. The basic structure of a program is some nested <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/modules/python_api.md#block">blocks</a>, where each block consists of some <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/modules/python_api.md#variable">variable</a> definitions and a sequence of <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/modules/python_api.md#operator">operators</a>. An <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/concepts/executor.md">executor</a> will run a given program desc by executing the sequence of operators in the entrance block of the program one by one.</p>
<div class="section" id="operator-level-requirement">
<span id="operator-level-requirement"></span><h3>Operator level requirement<a class="headerlink" href="#operator-level-requirement" title="永久链接至标题">¶</a></h3>
<p>Each operator has many kernels for different data types, devices, and library types. The operator will select the appropriate kernel to run based on, among other things, the data type of the input variables. By default, every Fluid operator has a float data type kernel that takes float variables as input and generates float output.</p>
<p>This means that if we provide float input to the first operator in a program, then each opeartor will use float kernel to compute float output and send it as input to the next operator to trigger the float kernel. Overall, the program will run in float mode and give us a final output of float data type.</p>
<p>The same principle applies if we want a program to run in float16 mode. We provide input variable of float16 data type to the first operator, and then one by one, each operator in the program will run the float16 kernel (provided that each operator in this program has float16 kernels registered) until we finally obtain a float16 output variable.</p>
<p>So the preliminary requirement for float16 inference is to add float16 kernel to operators that are needed in a specific kind of program. For example, float16 inference on an image classification neural network like Vgg or Resnet, typically requires the following operators to have float16 kernels: convolution, pooling, multiplication, addition, batch norm, dropout, relu, and softmax. Please refer to <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/dev/new_op_en.md">new_op_en</a> for details of how to add new kernels to an operator.</p>
</div>
<div class="section" id="variable-level-requirement">
<span id="variable-level-requirement"></span><h3>Variable level requirement<a class="headerlink" href="#variable-level-requirement" title="永久链接至标题">¶</a></h3>
<p>Operators including convolution and multiplication (used in fully-connected layers) takes as input not only the variables generated by the preceding operators but also <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/modules/python_api.md#parameter">parameter</a> variables, which contains the trained weights to apply to the input data. These weights are obtained in the Fluid training process and are by default of float data type.</p>
<p>When these operators are running in float16 mode, the float16 kernel requires those parameter variables to contain weights of Fluid float16 data type. Thus, we need a convenient way to convert the original float weights to float16 weights.</p>
<p>In Fluid, we use tensor to hold actual data for a variable on the c++ end. <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/pybind/tensor_py.h">Pybind</a> is used to bind c++ tensors of certain data type with numpy array of the correponding numpy data type on the Python end. Each common c++ built-in data type has a corresponding numpy data type of the same name. However, since there is no built-in float16 type in c++, we cannot directly bind numpy float16 data type with the Fluid float16 class. Since both Fluid float16 and numpy float16 use uint16 as the internal data storage type, we use c++ built-in type <code class="docutils literal"><span class="pre">uint16_t</span></code> and the corresponding numpy uint16 data type to bridge the gap via <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/pybind/tensor_py.h">Pybind</a>.</p>
<p>The following code demonstrates how to do the tensor conversion.</p>
<div class="highlight-Python"><div class="highlight"><pre><span></span><span class="c1"># var is the variable of float weights</span>
<span class="c1"># tensor is a numpy array of data copied from the tensor data in var </span>
<span class="c1"># fp16_var is the variable that will contain float16 weights converted from var  </span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">get_tensor</span><span class="p">())</span>
<span class="n">fp16_tensor</span> <span class="o">=</span> <span class="n">fp16_var</span><span class="o">.</span><span class="n">get_tensor</span><span class="p">()</span>

<span class="c1"># After the original tensor data is converted to numpy float16 data type, </span>
<span class="c1"># view(numpy.uint16) is used so that the internal memory of the numpy array </span>
<span class="c1"># will be reinterpreted to be of uint16 data type, which is binded to </span>
<span class="c1"># Fluid float16 class via pybind with the help of uint16_t built-in c++ type</span>
<span class="n">fp16_tensor</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">uint16</span><span class="p">),</span> <span class="n">GPUPlace</span><span class="p">)</span>  
</pre></div>
</div>
</div>
<div class="section" id="consistent-api-requirement">
<span id="consistent-api-requirement"></span><h3>Consistent API requirement<a class="headerlink" href="#consistent-api-requirement" title="永久链接至标题">¶</a></h3>
<p>The basic inference in float16 mode requires users to feed input and obtain output both of float16 data type. However, in this way, the inference APIs are not consistent between float16 mode and float mode, and users may find it confusing and diffcult to use float16 inference since they need to do extra steps to provide float16 input data and convert float16 output data back to float. To have consistent API for different inference modes, we need to transpile the program desc in some way so that we can run float16 inference by feeding and fetching variables of float data type.</p>
<p>This problem can be solved by introducing a type-casting operator which takes an input variable of certain data type, cast it to another specified data type, and put the casted data into the output variable. Insert cast operator where needed can make a program internally run in float16 mode.</p>
</div>
<div class="section" id="float16-transpiler">
<span id="float16-transpiler"></span><h3>float16 transpiler<a class="headerlink" href="#float16-transpiler" title="永久链接至标题">¶</a></h3>
<p>Put all the above requirements in mind, we designed a float16 inference transpiler that can tranpile a float32 mode inference program desc to a float16 mode one.</p>
<p>Given a float inference program and the corresponding variables of float32 weights in the <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/concepts/scope.md">scope</a>,
this transpiler mainly does the following modifications:</p>
<ol class="simple">
<li>Insert cast operators at the beginning of the program so that the input float data will be converted to float16 data type before feeding to subsequent operators to invoke the float16 kernel.</li>
<li>Insert cast operators at the end of the program so that the output float16 data will be converted back to float data type before users obtain the result.</li>
<li>For each parameter variable of float weights, create in the scope a corresponding variable of float16 weights which are converted from the corresponding float weights and add this new float16 variable to the program.</li>
<li>Update the operator information in the program so that each relevant operator use the newly created float16 variable instead of its float counterpart.</li>
</ol>
<p>Below is an example of usage:</p>
<div class="highlight-Python"><div class="highlight"><pre><span></span><span class="c1"># Get the float inference program</span>
<span class="p">[</span><span class="n">float_inference_program</span><span class="p">,</span> <span class="n">feed_target_names</span><span class="p">,</span>
 <span class="n">fetch_targets</span><span class="p">]</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">load_inference_model</span><span class="p">(</span><span class="n">save_dirname</span><span class="p">,</span> <span class="n">exe</span><span class="p">)</span>

<span class="c1"># Prepare the float input data</span>
<span class="n">tensor_img</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Running inference_program in float mode</span>
<span class="n">float_results</span> <span class="o">=</span> <span class="n">exe</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">float_inference_program</span><span class="p">,</span>
                        <span class="n">feed</span><span class="o">=</span><span class="p">{</span><span class="n">feed_target_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">tensor_img</span><span class="p">},</span>
                        <span class="n">fetch_list</span><span class="o">=</span><span class="n">fetch_targets</span><span class="p">)</span>

<span class="c1"># Use float16 transpiler to speedup</span>
<span class="n">float16_inference_program</span> <span class="o">=</span> <span class="n">float_inference_program</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">InferenceTranspiler</span><span class="p">()</span>
<span class="n">t</span><span class="o">.</span><span class="n">float16_transpile</span><span class="p">(</span><span class="n">float16_inference_program</span><span class="p">,</span> <span class="n">GPUPlace</span><span class="p">)</span>

<span class="c1"># Running </span>
<span class="n">float16_results</span> <span class="o">=</span> <span class="n">exe</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">float16_inference_program</span><span class="p">,</span>
                          <span class="n">feed</span><span class="o">=</span><span class="p">{</span><span class="n">feed_target_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">tensor_img</span><span class="p">},</span>
                          <span class="n">fetch_list</span><span class="o">=</span><span class="n">fetch_targets</span><span class="p">)</span>
</pre></div>
</div>
<p>As we can see from the example above, users can simply use the <code class="docutils literal"><span class="pre">float16_transpile</span></code> method provided by the infernece transpiler class on an existing float inference program to run inference in float16 mode.</p>
</div>
<div class="section" id="speedup-on-gpu">
<span id="speedup-on-gpu"></span><h3>Speedup on GPU<a class="headerlink" href="#speedup-on-gpu" title="永久链接至标题">¶</a></h3>
<p>Currently, Fluid inference in float16 mode is only supported on Nvidia GPU device. There is no motivation to support float16 inference on non-ARM CPUs because float16 is not natively supported there and float16 calculation will only be slower than its float counterpart.</p>
<p>Nvidia started to support its native float16 data type (which has the same internal memory representation as Fluid float16 class) on CUDA 7.5. Moreover, float16 speedups on common computational intensive tasks including GEMM (general matrix-matrix multiplication) and convolution are supported since cublas 7.5 and cuDNN 5.0.</p>
<p>Recently, the introduction of <a class="reference external" href="https://devblogs.nvidia.com/programming-tensor-cores-cuda-9/">tensor core</a> in volta architecture GPUs and the support of tensor core calculation in CUDA 9.0 and cuDNN 7.0 make float16 truly superior to float in certain deep learning applications. Please refer to this <a class="reference external" href="https://github.com/kexinzhao/Paddle_benchmark/blob/master/float16_benchmark.md">benchmark report</a> for more details.</p>
</div>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>