<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="batch-normalization">
<span id="batch-normalization"></span><h1>Batch Normalization<a class="headerlink" href="#batch-normalization" title="永久链接至标题">¶</a></h1>
<div class="section" id="what-is-batch-normalization">
<span id="what-is-batch-normalization"></span><h2>What is batch normalization<a class="headerlink" href="#what-is-batch-normalization" title="永久链接至标题">¶</a></h2>
<p>Batch normalization is a frequently-used method in deep network training. It adjusts the mean and variance of a layer’s output, and make the data distribution easier for next layer’s training.</p>
<p>The principle of batch normalization can be summarized into a simple function:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">E</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="o">/</span> <span class="n">STD</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">bias</span>
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">x</span></code> is a batch of output data of a certain layer. <code class="docutils literal"><span class="pre">E[x]</span></code> and <code class="docutils literal"><span class="pre">STD[x]</span></code> is the mean and standard deviation of <code class="docutils literal"><span class="pre">x</span></code>, respectively。 <code class="docutils literal"><span class="pre">scale</span></code> and <code class="docutils literal"><span class="pre">bias</span></code> are two trainable parameters. The training of batch normalization layer equals to the learning of best values of <code class="docutils literal"><span class="pre">scale</span></code> and <code class="docutils literal"><span class="pre">bias</span></code>.</p>
<p>In our design, we use a single operator(<code class="docutils literal"><span class="pre">batch_norm_op</span></code>) to implement the whole batch normalization in C++, and wrap it as a layer in Python.</p>
</div>
<div class="section" id="differences-with-normal-operators">
<span id="differences-with-normal-operators"></span><h2>Differences with normal operators<a class="headerlink" href="#differences-with-normal-operators" title="永久链接至标题">¶</a></h2>
<p><code class="docutils literal"><span class="pre">batch_norm_op</span></code> is a single operator. However, there are a few differences between <code class="docutils literal"><span class="pre">BatchNormOp</span></code> and normal operators, which we shall take into consideration in our design.</p>
<ol class="simple">
<li><code class="docutils literal"><span class="pre">batch_norm_op</span></code> shall behave differently in training and inferencing. For example, during inferencing, there is no batch data and it’s impossible to compute <code class="docutils literal"><span class="pre">E[x]</span></code> and <code class="docutils literal"><span class="pre">STD[x]</span></code>, so we have to use an <code class="docutils literal"><span class="pre">estimated_mean</span></code> and an <code class="docutils literal"><span class="pre">estimated_variance</span></code> instead of them. These require our framework to be able to inform operators current running type (training/inferencing), then operators can switch their behaviors.</li>
<li><code class="docutils literal"><span class="pre">batch_norm_op</span></code> shall have the ability to maintain <code class="docutils literal"><span class="pre">estimated_mean</span></code> and <code class="docutils literal"><span class="pre">estimated_variance</span></code> across mini-batch. In each mini-batch, <code class="docutils literal"><span class="pre">estimated_mean</span></code> is iterated by the following equations:</li>
</ol>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">batch_id</span> <span class="o">==</span> <span class="mi">0</span>
  <span class="n">estimated_mean</span> <span class="o">=</span> <span class="n">E</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
<span class="k">else</span>
  <span class="n">estimated_mean</span> <span class="o">=</span> <span class="n">estimated_mean</span> <span class="o">*</span> <span class="n">momentum</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">momentum_</span><span class="p">)</span> <span class="o">*</span> <span class="n">E</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
</pre></div>
</div>
<p>The iterating of <code class="docutils literal"><span class="pre">estimated_variance</span></code> is similar. <code class="docutils literal"><span class="pre">momentum</span></code> is an attribute, which controls estimated_mean updating speed.</p>
</div>
<div class="section" id="implementation">
<span id="implementation"></span><h2>Implementation<a class="headerlink" href="#implementation" title="永久链接至标题">¶</a></h2>
<p>Batch normalization is designed as a single operator is C++, and then wrapped as a layer in Python.</p>
<div class="section" id="c">
<span id="c"></span><h3>C++<a class="headerlink" href="#c" title="永久链接至标题">¶</a></h3>
<p>As most C++ operators do, <code class="docutils literal"><span class="pre">batch_norm_op</span></code> is defined by inputs, outputs, attributes and compute kernels.</p>
<div class="section" id="inputs">
<span id="inputs"></span><h4>Inputs<a class="headerlink" href="#inputs" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">x</span></code>: The inputs data, which is generated by the previous layer.</li>
<li><code class="docutils literal"><span class="pre">estimated_mean</span></code>: The estimated mean of all previous data batches. It is updated in each forward propagation and will be used in inferencing to take the role of <code class="docutils literal"><span class="pre">E[x]</span></code>.</li>
<li><code class="docutils literal"><span class="pre">estimated_var</span></code>: The estimated standard deviation of all previous data batches. It is updated in each forward propagation and will be used in inferencing to take the role of <code class="docutils literal"><span class="pre">STD[x]</span></code>.</li>
<li><code class="docutils literal"><span class="pre">scale</span></code>: trainable parameter ‘scale’</li>
<li><code class="docutils literal"><span class="pre">bias</span></code>: trainable parameter ‘bias’</li>
</ul>
</div>
<div class="section" id="outputs">
<span id="outputs"></span><h4>Outputs<a class="headerlink" href="#outputs" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">y</span></code>: The output data.</li>
<li><code class="docutils literal"><span class="pre">batch_mean</span></code>: The mean value of batch data.</li>
<li><code class="docutils literal"><span class="pre">batch_var</span></code>: The standard deviation value of batch data.</li>
<li><code class="docutils literal"><span class="pre">saved_mean</span></code>: Updated <code class="docutils literal"><span class="pre">estimated_mean</span></code> with current batch data. It’s supposed to share the memory with input <code class="docutils literal"><span class="pre">estimated_mean</span></code>.</li>
<li><code class="docutils literal"><span class="pre">saved_var</span></code>: Updated <code class="docutils literal"><span class="pre">estimated_var</span></code> with current batch data. It’s supposed to share the memory with input <code class="docutils literal"><span class="pre">estimated_var</span></code>.</li>
</ul>
</div>
<div class="section" id="attributes">
<span id="attributes"></span><h4>Attributes<a class="headerlink" href="#attributes" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">is_infer</span></code>: <em>bool</em>. If true, run <code class="docutils literal"><span class="pre">batch_norm_op</span></code> in inferencing mode.</li>
<li><code class="docutils literal"><span class="pre">use_global_est</span></code>: <em>bool</em>. If true, use <code class="docutils literal"><span class="pre">saved_mean</span></code> and <code class="docutils literal"><span class="pre">saved_var</span></code> instead of <code class="docutils literal"><span class="pre">E[x]</span></code> and <code class="docutils literal"><span class="pre">STD[x]</span></code> in trainning.</li>
<li><code class="docutils literal"><span class="pre">epsilon</span></code>: <em>float</em>. The epsilon value to avoid division by zero.</li>
<li><code class="docutils literal"><span class="pre">momentum</span></code>: <em>float</em>. Factor used in <code class="docutils literal"><span class="pre">estimated_mean</span></code> and <code class="docutils literal"><span class="pre">estimated_var</span></code> updating. The usage is shown above.</li>
</ul>
</div>
<div class="section" id="kernels">
<span id="kernels"></span><h4>Kernels<a class="headerlink" href="#kernels" title="永久链接至标题">¶</a></h4>
<p>The following graph showes the training computational process of <code class="docutils literal"><span class="pre">batch_norm_op</span></code>:</p>
<p><img src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/batch_norm_op_kernel.png" width="800"/></p>
<p>cudnn provides APIs to finish the whole series of computation, we can use them in our GPU kernel.</p>
</div>
</div>
<div class="section" id="python">
<span id="python"></span><h3>Python<a class="headerlink" href="#python" title="永久链接至标题">¶</a></h3>
<p><code class="docutils literal"><span class="pre">batch_norm_op</span></code> is warpped as a layer in Python:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">batch_norm_layer</span><span class="p">(</span><span class="n">net</span><span class="p">,</span>
                     <span class="nb">input</span><span class="p">,</span>
                     <span class="n">output</span><span class="p">,</span>
                     <span class="n">scale</span><span class="p">,</span>
                     <span class="n">bias</span><span class="p">,</span>
                     <span class="n">use_global_est</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
                     <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
                     <span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">):</span>
    <span class="n">mean_cache</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">new_var</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s1">'estimated_mean'</span><span class="p">,</span> <span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
    <span class="n">var_cache</span> <span class="o">=</span> <span class="n">scop</span><span class="o">.</span><span class="n">new_var</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s1">'estimated_var'</span><span class="p">,</span> <span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
    <span class="n">batch_mean</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">new_var</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s1">'batch_mean'</span><span class="p">)</span>
    <span class="n">batch_var</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">new_var</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s1">'batch_var'</span><span class="p">)</span>
    <span class="n">batch_norm_op</span> <span class="o">=</span> <span class="n">Operator</span><span class="p">(</span><span class="s1">'batch_norm_op'</span><span class="p">,</span>
                             <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="p">,</span>
                             <span class="n">estimated_mean</span> <span class="o">=</span> <span class="n">mean_cache</span><span class="p">,</span>
                             <span class="n">estimated_mean</span> <span class="o">=</span> <span class="n">var_cache</span><span class="p">,</span>
                             <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="p">,</span>
                             <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="p">,</span>
                             <span class="n">y</span> <span class="o">=</span> <span class="n">output</span><span class="p">,</span>
                             <span class="n">batch_mean</span> <span class="o">=</span> <span class="n">batch_mean</span><span class="p">,</span>
                             <span class="n">batch_var</span> <span class="o">=</span> <span class="n">batch_var</span><span class="p">,</span>
                             <span class="n">saved_mean</span> <span class="o">=</span> <span class="n">mean_cache</span><span class="p">,</span>
                             <span class="n">saved_var</span> <span class="o">=</span> <span class="n">var_cache</span><span class="p">,</span>
                             <span class="n">is_infer</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
                             <span class="n">use_global_est</span> <span class="o">=</span> <span class="n">use_global_est</span><span class="p">,</span>
                             <span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span><span class="p">,</span>
                             <span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">append_op</span><span class="p">(</span><span class="n">batch_norm_op</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>Because Python API has not been finally decided, the code above can be regarded as pseudo code. There are a few key points we shall note:</p>
<ol class="simple">
<li><code class="docutils literal"><span class="pre">estimated_mean</span></code> and <code class="docutils literal"><span class="pre">estimated_var</span></code> are assigned the same variables with <code class="docutils literal"><span class="pre">saved_mean</span></code> and <code class="docutils literal"><span class="pre">saved_var</span></code> respectively. So they share same the memories. The output mean and variance values(<code class="docutils literal"><span class="pre">saved_mean</span></code> and <code class="docutils literal"><span class="pre">saved_var</span></code>) of a certain batch will be the inputs(<code class="docutils literal"><span class="pre">estimated_mean</span></code> and <code class="docutils literal"><span class="pre">estimated_var</span></code>) of the next batch.</li>
<li><code class="docutils literal"><span class="pre">is_infer</span></code> decided whether <code class="docutils literal"><span class="pre">batch_norm_op</span></code> will run in training mode or inferencing mode. However, a network may contains both training and inferencing parts. And user may switch <code class="docutils literal"><span class="pre">batch_norm_op</span></code>‘s running mode in Python <code class="docutils literal"><span class="pre">for</span></code> loop like this:</li>
</ol>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">pass_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">PASS_NUM</span><span class="p">):</span>
    <span class="c1"># ...</span>
    <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># run training model</span>
    <span class="k">if</span> <span class="n">pass_id</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">net</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">test_image</span><span class="p">)</span>    <span class="c1"># run inferencing model</span>
    <span class="c1"># ...</span>
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">is_infer</span></code> is an attribute. Once an operator is created, its attributes can not be changed. It suggests us that we shall maintain two <code class="docutils literal"><span class="pre">batch_norm_op</span></code> in the model, one’s <code class="docutils literal"><span class="pre">is_infer</span></code> is <code class="docutils literal"><span class="pre">True</span></code>(we call it <code class="docutils literal"><span class="pre">infer_batch_norm_op</span></code>) and the other one’s is <code class="docutils literal"><span class="pre">False</span></code>(we call it <code class="docutils literal"><span class="pre">train_batch_norm_op</span></code>). They share all parameters and variables, but be placed in two different branches. That is to say, if a network contains a <code class="docutils literal"><span class="pre">batch_norm_op</span></code>, it will fork into two branches, one go through <code class="docutils literal"><span class="pre">train_batch_norm_op</span></code> and the other one go through <code class="docutils literal"><span class="pre">infer_batch_norm_op</span></code>:</p>
<div align="center">
<img src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/batch_norm_fork.png" width="500"/>
</div><p>Just like what is shown in the above graph, the net forks before <code class="docutils literal"><span class="pre">batch_norm_op</span></code> and will never merge again. All the operators after <code class="docutils literal"><span class="pre">batch_norm_op</span></code> will duplicate.</p>
<p>When the net runs in training mode, the end of the left branch will be set as the running target, so the dependency tracking process will ignore right branch automatically. When the net runs in inferencing mode, the process is reversed.</p>
<p>How to set a target is related to Python API design, so I will leave it here waiting for more discussions.</p>
</div>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>