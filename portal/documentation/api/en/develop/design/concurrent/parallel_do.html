<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="design-doc-parallel-do-in-paddlepaddle">
<span id="design-doc-parallel-do-in-paddlepaddle"></span><h1>Design Doc: Parallel_Do in PaddlePaddle<a class="headerlink" href="#design-doc-parallel-do-in-paddlepaddle" title="永久链接至标题">¶</a></h1>
<p>In PaddlePaddle, we use parallel_do primitive to represent multithread data parallel processing.</p>
<div class="section" id="design-overview">
<span id="design-overview"></span><h2>Design overview<a class="headerlink" href="#design-overview" title="永久链接至标题">¶</a></h2>
<p>The definition of a parallel_do op looks like the following</p>
<div class="highlight-c++"><div class="highlight"><pre><span></span><span class="n">AddInput</span><span class="p">(</span><span class="n">kInputs</span><span class="p">,</span> <span class="s">"Inputs needed to be split onto different devices"</span><span class="p">).</span><span class="n">AsDuplicable</span><span class="p">();</span>
<span class="n">AddInput</span><span class="p">(</span><span class="n">kParameters</span><span class="p">,</span> <span class="s">"Parameters are duplicated over different devices"</span><span class="p">)</span>
    <span class="p">.</span><span class="n">AsDuplicable</span><span class="p">();</span>
<span class="n">AddInput</span><span class="p">(</span><span class="n">kPlaces</span><span class="p">,</span> <span class="s">"Devices used for parallel processing"</span><span class="p">);</span>
<span class="n">AddOutput</span><span class="p">(</span><span class="n">kOutputs</span><span class="p">,</span> <span class="s">"Outputs needed to be merged from different devices"</span><span class="p">).</span><span class="n">AsDuplicable</span><span class="p">();</span>
<span class="n">AddOutput</span><span class="p">(</span><span class="n">kParallelScopes</span><span class="p">,</span>
          <span class="s">"Scopes for all local variables in forward pass. One scope for each device"</span><span class="p">);</span>
<span class="n">AddAttr</span><span class="o">&lt;</span><span class="n">framework</span><span class="o">::</span><span class="n">BlockDesc</span> <span class="o">*&gt;</span><span class="p">(</span><span class="n">kParallelBlock</span><span class="p">,</span>
                                <span class="s">"List of operaters to be executed in parallel"</span><span class="p">);</span>
</pre></div>
</div>
<p>A vanilla implementation of parallel_do can be shown as the following (<code class="docutils literal"><span class="pre">|</span></code> means single thread and
<code class="docutils literal"><span class="pre">||||</span></code> means multiple threads)</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="n">the</span> <span class="n">forward</span> <span class="k">pass</span>
  <span class="o">|</span>      <span class="n">Split</span> <span class="nb">input</span> <span class="n">onto</span> <span class="n">different</span> <span class="n">devices</span>
  <span class="o">|</span>      <span class="n">Copy</span> <span class="n">parameter</span> <span class="n">onto</span> <span class="n">different</span> <span class="n">devices</span>
  <span class="o">||||</span>   <span class="n">Compute</span> <span class="n">forward</span> <span class="k">pass</span> <span class="ow">in</span> <span class="n">parallel</span>
  <span class="o">|</span>      <span class="n">Merge</span> <span class="n">output</span> <span class="kn">from</span> <span class="nn">different</span> <span class="n">devices</span>

<span class="n">In</span> <span class="n">the</span> <span class="n">backward</span> <span class="k">pass</span>
  <span class="o">|</span>      <span class="n">Split</span> <span class="n">output</span><span class="nd">@grad</span> <span class="n">onto</span> <span class="n">different</span> <span class="n">devices</span>
  <span class="o">||||</span>   <span class="n">Compute</span> <span class="n">backward</span> <span class="k">pass</span> <span class="ow">in</span> <span class="n">parallel</span>
  <span class="o">|</span>      <span class="n">accumulate</span> <span class="n">param</span><span class="nd">@grad</span> <span class="kn">from</span> <span class="nn">different</span> <span class="n">devices</span> <span class="n">to</span> <span class="n">the</span> <span class="n">first</span> <span class="n">device</span>
  <span class="o">|</span>      <span class="n">Merge</span> <span class="nb">input</span><span class="nd">@grad</span> <span class="kn">from</span> <span class="nn">different</span> <span class="n">devices</span>
  <span class="o">|</span>      <span class="n">Copy</span> <span class="n">param</span><span class="nd">@grad</span> <span class="n">to</span> <span class="n">the</span> <span class="n">place</span> <span class="n">of</span> <span class="n">parallel_do_op</span>
</pre></div>
</div>
<p>This implementation allows to write mixed device program like this</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span> <span class="n">parameter</span><span class="o">=</span><span class="n">true</span><span class="p">)</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span><span class="mi">15</span><span class="p">],</span> <span class="n">parameter</span><span class="o">=</span><span class="n">true</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">data</span><span class="p">()</span>

<span class="n">gpu_places</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">get_place</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># parallel processing on multiple GPUs</span>
<span class="n">pd</span> <span class="o">=</span> <span class="n">ParallelDo</span><span class="p">(</span><span class="n">gpu_places</span><span class="p">)</span>
<span class="k">with</span> <span class="n">pd</span><span class="o">.</span><span class="n">do</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">data</span><span class="p">):</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">fc</span><span class="p">(</span><span class="n">fc</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">W1</span><span class="p">),</span> <span class="n">W2</span><span class="p">))</span>
    <span class="n">write_output</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">pd</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
<p>And the programDesc are like the following</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># start_program will be run by executor(CPUPlace), all w1, w2 will be allocated on CPU</span>
<span class="n">start_program</span>
<span class="p">{</span>
  <span class="nb">vars</span><span class="p">:</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span>
  <span class="n">ops</span><span class="p">:</span> <span class="n">init</span><span class="p">(</span><span class="n">w1</span><span class="p">),</span> <span class="n">init</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">main_program</span>
<span class="p">{</span>
<span class="n">block0</span> <span class="p">{</span>
  <span class="nb">vars</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span> <span class="n">places</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">w1_grad</span><span class="p">,</span> <span class="n">w2_grad</span><span class="p">,</span>
  <span class="n">ops</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span> <span class="n">get_place</span><span class="p">,</span> <span class="n">parallel_do</span><span class="p">(</span><span class="n">block1</span><span class="p">),</span>
       <span class="n">parallel_do_grad</span><span class="p">(</span><span class="n">block2</span><span class="p">),</span>
       <span class="n">sgd</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">w2_grad</span><span class="p">),</span>
       <span class="n">sgd</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w1_grad</span><span class="p">)</span>
<span class="p">}</span>
<span class="n">block1</span> <span class="p">{</span> <span class="c1"># the forward pass</span>
  <span class="n">parent_block</span><span class="p">:</span> <span class="mi">0</span>
  <span class="nb">vars</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span> <span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">,</span> <span class="n">loss</span>
  <span class="n">ops</span><span class="p">:</span> <span class="n">fc</span><span class="p">,</span> <span class="n">fc</span><span class="p">,</span> <span class="n">softmax</span>
<span class="p">}</span>
<span class="n">block2</span> <span class="p">{</span> <span class="c1"># the backward pass</span>
  <span class="n">parent_block</span><span class="p">:</span> <span class="mi">1</span>
  <span class="nb">vars</span><span class="p">:</span> <span class="n">data_grad</span><span class="p">,</span> <span class="n">h1_grad</span><span class="p">,</span> <span class="n">h2_grad</span><span class="p">,</span> <span class="n">loss_gard</span><span class="p">,</span> <span class="n">local_w1_grad</span><span class="p">,</span> <span class="n">local_w2_grad</span>
  <span class="n">ops</span><span class="p">:</span> <span class="n">softmax_grad</span><span class="p">,</span>
       <span class="n">fc_grad</span>
       <span class="n">fc_grad</span>
<span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="performance-imporvement">
<span id="performance-imporvement"></span><h2>Performance Imporvement<a class="headerlink" href="#performance-imporvement" title="永久链接至标题">¶</a></h2>
<p>There are serial places we can make this parallel_do faster.</p>
<div class="section" id="forward-split-input-onto-different-devices">
<span id="forward-split-input-onto-different-devices"></span><h3>forward: split input onto different devices<a class="headerlink" href="#forward-split-input-onto-different-devices" title="永久链接至标题">¶</a></h3>
<p>If the input of the parallel_do is independent from any prior opeartors, we can avoid this step by
prefetching the input onto different devices in a seperate background thread. And the python code
looks like this.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>pd = ParallelDo(gpu_places)
with pd.do():
    feature = get_data_from_prefetch_queue(gpu_places)
    prediction = my_net(feature)
    write_output(activation)
</pre></div>
</div>
</div>
<div class="section" id="forward-copy-parameter-to-onto-different-devices">
<span id="forward-copy-parameter-to-onto-different-devices"></span><h3>forward: Copy parameter to onto different devices<a class="headerlink" href="#forward-copy-parameter-to-onto-different-devices" title="永久链接至标题">¶</a></h3>
<p>We can avoid this step by making each device have a copy of the parameter. This requires:</p>
<ol class="simple">
<li><code class="docutils literal"><span class="pre">fluid.default_start_up_program()</span></code> to be run on all devices</li>
<li>In the backward, allreduce param@grad at different devices, this requires<ol>
<li><code class="docutils literal"><span class="pre">backward.py</span></code> add <code class="docutils literal"><span class="pre">allreduce</span></code> operators at parallel_do_grad</li>
<li><code class="docutils literal"><span class="pre">allreduce</span></code> operators need to be called in async mode to achieve maximum throughput</li>
</ol>
</li>
<li>apply gradients related op(i.e. cliping, normalization, decay, sgd) on different devices in parallel</li>
</ol>
<p>By doing so, we also avoided “backward: accumulate param@grad from different devices to the first device”.
And the ProgramDesc looks like the following</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># w1, w2 will be allocated on all GPUs</span>
<span class="n">start_program</span>
<span class="p">{</span>
<span class="n">block0</span> <span class="p">{</span>
  <span class="n">parallel_do</span><span class="p">(</span><span class="n">block1</span><span class="p">)</span>
<span class="p">}</span>
<span class="n">block1</span> <span class="p">{</span>
  <span class="n">parent_block</span><span class="p">:</span> <span class="mi">0</span>
  <span class="nb">vars</span><span class="p">:</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span>
  <span class="n">ops</span><span class="p">:</span> <span class="n">init</span><span class="p">(</span><span class="n">w1</span><span class="p">),</span> <span class="n">init</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>
<span class="p">}</span>
<span class="p">}</span>

<span class="n">main_program</span>
<span class="p">{</span>
<span class="n">block0</span> <span class="p">{</span>
  <span class="nb">vars</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span> <span class="n">places</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span>
  <span class="n">ops</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span> <span class="n">get_place</span><span class="p">,</span> <span class="n">parallel_do</span><span class="p">(</span><span class="n">block1</span><span class="p">),</span>
       <span class="n">parallel_do_grad</span><span class="p">(</span><span class="n">block2</span><span class="p">),</span>      <span class="c1"># append_backward</span>
       <span class="n">parallel_do</span><span class="p">(</span><span class="n">block3</span><span class="p">)</span>            <span class="c1"># append_optimization</span>
       
<span class="p">}</span>
<span class="n">block1</span> <span class="p">{</span>
  <span class="n">parent_block</span><span class="p">:</span> <span class="mi">0</span>
  <span class="nb">vars</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span> <span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">,</span> <span class="n">loss</span>
  <span class="n">ops</span><span class="p">:</span> <span class="n">fc</span><span class="p">,</span> <span class="n">fc</span><span class="p">,</span> <span class="n">softmax</span>
<span class="p">}</span>
<span class="n">block2</span> <span class="p">{</span>
  <span class="n">parent_block</span><span class="p">:</span> <span class="mi">1</span>
  <span class="nb">vars</span><span class="p">:</span> <span class="n">data_grad</span><span class="p">,</span> <span class="n">h1_grad</span><span class="p">,</span> <span class="n">h2_grad</span><span class="p">,</span> <span class="n">loss_gard</span><span class="p">,</span> <span class="n">w1_grad</span><span class="p">,</span> <span class="n">w2_grad</span>
  <span class="n">ops</span><span class="p">:</span> <span class="n">softmax_grad</span><span class="p">,</span>
       <span class="n">fc_grad</span><span class="p">,</span> <span class="n">allreduce</span><span class="p">(</span><span class="n">places</span><span class="p">,</span> <span class="n">scopes</span><span class="p">,</span> <span class="n">w1_grad</span><span class="p">),</span>
       <span class="n">fc_grad</span><span class="p">,</span> <span class="n">allreduce</span><span class="p">(</span><span class="n">places</span><span class="p">,</span> <span class="n">scopes</span><span class="p">,</span> <span class="n">w2_grad</span><span class="p">)</span>
<span class="p">}</span>
<span class="n">block3</span> <span class="p">{</span>
  <span class="n">parent_block</span><span class="p">:</span> <span class="mi">0</span>
  <span class="nb">vars</span><span class="p">:</span> <span class="n">lr</span>
  <span class="n">ops</span><span class="p">:</span> <span class="n">sgd</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">w2_grad</span><span class="p">),</span>
       <span class="n">sgd</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w1_grad</span><span class="p">)</span>
<span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>