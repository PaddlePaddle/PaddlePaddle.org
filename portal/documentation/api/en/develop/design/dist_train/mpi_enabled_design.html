<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="mpi-enabled-paddlepaddle-design-doc">
<span id="mpi-enabled-paddlepaddle-design-doc"></span><h1>MPI-enabled PaddlePaddle Design doc<a class="headerlink" href="#mpi-enabled-paddlepaddle-design-doc" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="background">
<span id="background"></span><h1>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h1>
<p>When we do distribute multi GPU training, the communication overhead between servers become the major bottleneck, because of the following reasons:</p>
<ol class="simple">
<li>Must copy at least once from GPU to CPU memory so that the data can be ready to transfer. And for the pserver side, copy data from CPU to GPU introduce more overhead.</li>
<li>GPU-&gt;CPU data transfer is 10 times slower than data transfer between GPUs or between PCIe devices.</li>
<li>TCP connections can not make full use of RDMA 100Gb devices.</li>
</ol>
<p>We will use OpenMPI API to PaddlePaddle, which can bring two benefits to PaddlePaddle:</p>
<ol class="simple">
<li>Enable RDMA with PaddlePaddle, which bring high-performance low latency networks.</li>
<li>Enable GPUDriect with PaddlePaddle, which bring the highest throughput and lowest latency GPU read and write.</li>
</ol>
</div>
<div class="section" id="change-list">
<span id="change-list"></span><h1>Change list<a class="headerlink" href="#change-list" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li>Compile args: Need add compile args to enable MPI support.</li>
<li>Execute args:  Need add execute args to assign when and how to use MPI operations.</li>
<li>New ops:  Need new op  <code class="docutils literal"><span class="pre">mpi_send_op</span></code> and <code class="docutils literal"><span class="pre">mpi_listenandserve_op</span></code> to support MPI send and receive.</li>
<li>Transpiler optimized: Which can add   <code class="docutils literal"><span class="pre">mpi_send_op</span></code> and <code class="docutils literal"><span class="pre">mpi_listenandserve_op</span></code>  to the running graph.</li>
<li>MPI utils package: Need MPI utils package as the low-level API supported.</li>
</ul>
<div class="section" id="compile-args">
<span id="compile-args"></span><h2>Compile args<a class="headerlink" href="#compile-args" title="Permalink to this headline">¶</a></h2>
<p>Because MPI or CUDA need hardware supported, so we will add compile args to enable MPI support and control compiling.Add <code class="docutils literal"><span class="pre">WITH_MPI</span></code>  compile args to control MPI to use or not. If the  <code class="docutils literal"><span class="pre">WITH_MPI</span></code> is <code class="docutils literal"><span class="pre">ON</span></code>, compile system will find openMPI codes in configuration. We should prepare openMPI environment before compiling.</p>
</div>
<div class="section" id="execute-args">
<span id="execute-args"></span><h2>Execute args<a class="headerlink" href="#execute-args" title="Permalink to this headline">¶</a></h2>
<p>Launch the script using the <code class="docutils literal"><span class="pre">mpirun</span></code> launcher, For example: <code class="docutils literal"><span class="pre">mpirun</span> <span class="pre">-np</span> <span class="pre">3</span> <span class="pre">-hosts</span> <span class="pre">node1,node2,node3</span> <span class="pre">python</span> <span class="pre">train.py</span></code>. By doing this, We can number the actors (trainer/pserver/master) with o .. (n-1). The node’s number is the Rank of the calling process in a group of comm (integer),  The MPI processes identify each other using a Rank ID. We have to create a mapping between PaddlePaddle’s nodes and their Rank ID so that we can communicate with the correct destinations when using MPI operations.</p>
</div>
<div class="section" id="new-ops">
<span id="new-ops"></span><h2>New ops<a class="headerlink" href="#new-ops" title="Permalink to this headline">¶</a></h2>
<p>We won’t replace all the gRPC requests to MPI requests,  the standard gRPC library is used for all administrative operations and the MPI API will be used to transfer tensor or selectRows to Pservers. The base of this idea, we create two new operators to handle requests and receives,  the two operators are <code class="docutils literal"><span class="pre">mpi_send_op</span></code> and <code class="docutils literal"><span class="pre">mpi_listenandserve_op</span></code>. They are a little similar to <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/operators/send_op.cc">send_op</a> and <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/operators/listen_and_serv_op.cc">listen_and_serv_op</a>, also, We will build a new module to package MPI send and receive process.</p>
<div class="section" id="mpi-send-op">
<span id="mpi-send-op"></span><h3>mpi_send_op<a class="headerlink" href="#mpi-send-op" title="Permalink to this headline">¶</a></h3>
<p>Very similar with <code class="docutils literal"><span class="pre">send_op</span></code>, we will replace gRPC code which used to send gradient with <code class="docutils literal"><span class="pre">mpi_module</span></code>, at the same time, we will wrap it with <code class="docutils literal"><span class="pre">framework::Async</span></code>.</p>
</div>
<div class="section" id="mpi-listenandserve-op">
<span id="mpi-listenandserve-op"></span><h3>mpi_listenandserve_op<a class="headerlink" href="#mpi-listenandserve-op" title="Permalink to this headline">¶</a></h3>
<p>Very similar with <code class="docutils literal"><span class="pre">listen_and_serv_op</span></code>, we will replace gRPC code which used to receive gradient with <code class="docutils literal"><span class="pre">mpi_module</span></code>, at the same time, we will wrap it with <code class="docutils literal"><span class="pre">framework::Async</span></code>.</p>
</div>
</div>
<div class="section" id="transpiler-optimized">
<span id="transpiler-optimized"></span><h2>Transpiler optimized<a class="headerlink" href="#transpiler-optimized" title="Permalink to this headline">¶</a></h2>
<p><strong>We can get env <code class="docutils literal"><span class="pre">OMPI_COMM_WORLD_SIZE</span></code> and <code class="docutils literal"><span class="pre">OMPI_COMM_WORLD_RANK</span></code> to distinguish use MPI or not, If we use openMPI, the variable in env must exist.</strong>
if  confirm to use MPI, we will modify  <code class="docutils literal"><span class="pre">send_op</span></code> to <code class="docutils literal"><span class="pre">mpi_send_op</span></code> in distribute_transpiler, and modify <code class="docutils literal"><span class="pre">listenandserve_op</span></code> to <code class="docutils literal"><span class="pre">mpi_listenandserve_op</span></code> also.</p>
</div>
<div class="section" id="mpi-utils-package">
<span id="mpi-utils-package"></span><h2>MPI utils package<a class="headerlink" href="#mpi-utils-package" title="Permalink to this headline">¶</a></h2>
<p>In this package, We will write openMPI low-level API to use MPI.
The API included in this package are:</p>
<ul class="simple">
<li>MPI send and receive module, We will build a new module to package MPI send and receive process. MPI send and receive are different to gRPC, the MPI <a class="reference external" href="https://www.open-mpi.org/doc/v1.8/man3/MPI_Irecv.3.php">recvice</a> must know receive buffer size and receive buffer element. For this reason, We have to make communications twice, the first one is to send metadata about gradient through gRPC, the second one is the real communication through MPI which send gradient data to mpi_listenandserve_op.
The detailed flow is below:
<img alt="" src="https://github.com/seiriosPlus/Paddle/blob/mpi_enabled/doc/fluid/design/dist_train/src/mpi_module.png"/></li>
<li>MPI global configurations, which store the Rank ID and the mapping in global variables, for example:
gRPC client : MPI nodes :<code class="docutils literal"><span class="pre">127.0.0.1:32004</span> <span class="pre">:</span> <span class="pre">3</span></code></li>
</ul>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>