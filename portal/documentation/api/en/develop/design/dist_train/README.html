<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="distributed-training-overview-doc">
<span id="distributed-training-overview-doc"></span><h1>Distributed training overview doc<a class="headerlink" href="#distributed-training-overview-doc" title="永久链接至标题">¶</a></h1>
<p>Currently Paddle Fluid use parameter server architecture to support distributed training.</p>
<p>For synchronous and asynchronous training, the differences are mostly in the logic of parameter server. Now we have already support synchronous training.</p>
<div class="section" id="synchronous-training">
<span id="synchronous-training"></span><h2>Synchronous training<a class="headerlink" href="#synchronous-training" title="永久链接至标题">¶</a></h2>
<p>The training process of synchronous training is:</p>
<p><img alt="synchronous distributed training" src="../../_images/sync_distributed_training.png"/></p>
<ol class="simple">
<li>Pserver<ol>
<li>set <code class="docutils literal"><span class="pre">barrier_condition_</span></code> to 0 and waits for trainers to send gradient.</li>
</ol>
</li>
<li>Trainer<ol>
<li>Trainer read minibatch of data, run forward-backward with local parameter copy and get the gradients for parameters.</li>
<li>Trainer use split op to split all the gradient into blocks. The split method is determined at compile time.</li>
<li>Trainer use send_op to send all the split gradients to corresponding parameter server.</li>
<li>After trainer send all the gradients, it will send a <code class="docutils literal"><span class="pre">BATCH_BARRIER_MESSAGE</span></code> to all pservers.</li>
<li>Trainer call GetVariable to pserver and wait for <code class="docutils literal"><span class="pre">barrier_condition_</span></code> on pserver to be 1.</li>
</ol>
</li>
<li>Pserver<ol>
<li>Pserver will count the number of <code class="docutils literal"><span class="pre">BATCH_BARRIER_MESSAGE</span></code>.</li>
<li>When the count of <code class="docutils literal"><span class="pre">BATCH_BARRIER_MESSAGE</span></code> is equal to the number of Trainer. Pserver thinks it received all gradient from all trainers.</li>
<li>Pserver will run the optimization block to optimize the parameters.</li>
<li>After optimization, pserver set <code class="docutils literal"><span class="pre">barrier_condition_</span></code> to 1.</li>
<li>Pserver wait for <code class="docutils literal"><span class="pre">FETCH_BARRIER_MESSAGE</span></code>.</li>
</ol>
</li>
<li>Trainer.<ol>
<li>The trainer uses GetVariable to get all the parameters from pserver.</li>
<li>Trainer sends a <code class="docutils literal"><span class="pre">FETCH_BARRIER_MESSAGE</span></code> to each pserver.</li>
</ol>
</li>
<li>Pserver.<ol>
<li>when the number of <code class="docutils literal"><span class="pre">FETCH_BARRIER_MESSAGE</span></code> reach the number of all trainers. Pserver think all the parameters have been got. it will go back to 1. to set <code class="docutils literal"><span class="pre">barrier_condition_</span></code> to 0.</li>
</ol>
</li>
</ol>
</div>
<div class="section" id="asynchronous-training">
<span id="asynchronous-training"></span><h2>Asynchronous training<a class="headerlink" href="#asynchronous-training" title="永久链接至标题">¶</a></h2>
<p>In the above process. There are two barriers for all trainers to synchronize with each other. In asynchronous training, these two barriers are not needed. The trainer can just send gradients to pserver and then get parameters back.</p>
<p>The training process of asynchronous training can be:</p>
<p><img alt="asynchronous distributed training" src="../../_images/async_distributed_training.png"/></p>
<ol class="simple">
<li>Pserver:<ol>
<li>Each parameter has a queue to receive its gradient from trainers.</li>
<li>Each parameter has a thread to read data from the queue and run optimize block, using the gradient to optimize the parameter.</li>
<li>Using an independent thread to handle RPC call <code class="docutils literal"><span class="pre">GetVariable</span></code> for trainers to get parameters back.(Maybe here we should use a thread pool to speed up fetching the parameters.)</li>
</ol>
</li>
<li>Trainer:<ol>
<li>Trainer read a batch of data. Run forward and backward with local parameter copy and get the gradients for parameters.</li>
<li>Trainer split all gradients to blocks and then send these gradient blocks to pservers(pserver will put them into the queue).</li>
<li>Trainer gets all parameters back from pserver.</li>
</ol>
</li>
</ol>
</div>
<div class="section" id="note">
<span id="note"></span><h2>Note:<a class="headerlink" href="#note" title="永久链接至标题">¶</a></h2>
<p>There are also some conditions that need to consider. For exmaple:</p>
<ol class="simple">
<li>If trainer needs to wait for the pserver to apply it’s gradient and then get back the parameters back.</li>
<li>If we need a lock between parameter update and parameter fetch.</li>
<li>If one parameter must be on one server, or it can also be split and send to multiple parameter servers.</li>
</ol>
<p>The above architecture of asynchronous training can support different mode, we can have a detailed test in the future for these problems.</p>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>