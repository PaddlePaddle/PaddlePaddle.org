<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="design-doc-lod-level-of-detail-tensor">
<span id="design-doc-lod-level-of-detail-tensor"></span><h1>Design Doc: LoD (Level-of-Detail) Tensor<a class="headerlink" href="#design-doc-lod-level-of-detail-tensor" title="永久链接至标题">¶</a></h1>
<p>Like other deep learning systems, PaddlePaddle supports training models from sequence data.  Also, like other systems, PaddlePaddle represent a mini-batch of sequences as a Tensor.  What is different is that PaddlePaddle doesn’t require all sequences in a mini-batch to be of the same length. Thus no need for padding zeros.</p>
<table>
<thead>
<tr>
<th></th>
<th>TensorFlow</th>
<th>PaddlePaddle</th>
</tr>
</thead>
<tbody>
<tr>
<td>RNN </td>
<td>Support </td>
<td>Support </td>
</tr>
<tr>
<td>recursive RNN </td>
<td>Support </td>
<td>Support </td>
</tr>
<tr>
<td>padding zeros </td>
<td> Must </td>
<td>No need </td>
</tr>
<tr>
<td> blob data type </td>
<td> Tensor</td>
<td> LoDTensor </td>
</tr>
</tbody>
</table><p>PaddlePaddle achieves this flexibility by passing through a new data type, <em>LoD Tensor</em>, which is a Tensor attached with segmentation index known as <em>LoD</em>, between operators.  The LoD index doesn’t only segment a tensor, but also recursively segments sub-sequences.  This document presents the design of LoD and LoDTensor.</p>
<div class="section" id="the-challenge-variable-length-sequences">
<span id="the-challenge-variable-length-sequences"></span><h2>The Challenge: Variable-length Sequences<a class="headerlink" href="#the-challenge-variable-length-sequences" title="永久链接至标题">¶</a></h2>
<p>Most deep learning systems represent a mini-batch as a Tensor.  For example, a mini-batch of 10 images, each of size 32x32, is a 10x32x32 Tensor.  Another example is that each mini-batch contains N sentences, where each word is a D-dimensional one-hot vector.  Suppose that all sentences have the same length L, we can represent this mini-batch by a NxLxD tensor.</p>
<p>Both examples show that the elements of sequences are usually of the same size.  In the first example, all images are 32x32, and in the second one, all words are D-dimensional vectors.  It doesn’t make sense to allow variable-sized images, as that would require transformations like convolution to handle variable-sized Tensors.</p>
<p>The real challenge is that in most cases, sentences have variable lengths, and we will need an index data structure to segment the tensor into sequences.  Also, sequences might consist of sub-sequences.</p>
</div>
<div class="section" id="a-solution-the-lod-index">
<span id="a-solution-the-lod-index"></span><h2>A Solution: The LoD Index<a class="headerlink" href="#a-solution-the-lod-index" title="永久链接至标题">¶</a></h2>
<p>To understand our solution, it is best to look at some examples.</p>
<div class="section" id="a-mini-batch-of-sentences">
<span id="a-mini-batch-of-sentences"></span><h3>A Mini-Batch of Sentences<a class="headerlink" href="#a-mini-batch-of-sentences" title="永久链接至标题">¶</a></h3>
<p>Let’s imagine a mini-batch of 3 variable lengths sentences composed of 3, 1, and 2 words, respectively.  We can represent the mini-batch by a (3+1+2)xD tensor plus some index information:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">3</span>   <span class="mi">1</span> <span class="mi">2</span>
<span class="o">|||</span> <span class="o">|</span> <span class="o">||</span>
</pre></div>
</div>
<p>where each <code class="docutils literal"><span class="pre">|</span></code> represents a D-dimensional word vector.  The numbers, 3, 1, and 2, form a 1-level LoD.</p>
</div>
<div class="section" id="recursive-sequences">
<span id="recursive-sequences"></span><h3>Recursive Sequences<a class="headerlink" href="#recursive-sequences" title="永久链接至标题">¶</a></h3>
<p>Let check another example of a 2-level LoD Tensor.  Consider a mini-batch of three articles with 3, 1, and 2 sentences, and each sentence consists of a variable number of words:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">3</span>           <span class="mi">1</span>  <span class="mi">2</span>
<span class="mi">3</span>   <span class="mi">2</span>  <span class="mi">4</span>    <span class="mi">1</span>  <span class="mi">2</span>  <span class="mi">3</span>
<span class="o">|||</span> <span class="o">||</span> <span class="o">||||</span> <span class="o">|</span>  <span class="o">||</span> <span class="o">|||</span>
</pre></div>
</div>
</div>
<div class="section" id="a-mini-batch-of-videos">
<span id="a-mini-batch-of-videos"></span><h3>A Mini-Batch of Videos<a class="headerlink" href="#a-mini-batch-of-videos" title="永久链接至标题">¶</a></h3>
<p>LoD tensors generalize to the case where elements are higher dimensional objects, like images.  Suppose that a mini-batch contains videos of the same frame size 640x480.  Here is a mini-batch of 3 videos with 3, 1, and 2 frames, respectively.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">3</span>     <span class="mi">1</span>  <span class="mi">2</span>
<span class="n">口口口</span> <span class="n">口</span> <span class="n">口口</span>
</pre></div>
</div>
<p>The underlying tensor is of size (3+1+2)x640x480, and each <code class="docutils literal"><span class="pre">口</span></code> represents a 640x480 image.</p>
</div>
<div class="section" id="a-mini-batch-of-images">
<span id="a-mini-batch-of-images"></span><h3>A Mini-Batch of Images<a class="headerlink" href="#a-mini-batch-of-images" title="永久链接至标题">¶</a></h3>
<p>In traditional cases like a mini-batch with N fixed-sized images,  the LoD Tensor representation is as</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span>     <span class="mi">1</span>
<span class="n">口口口口</span> <span class="o">...</span> <span class="n">口</span>
</pre></div>
</div>
<p>In this case, we don’t lose any information by ignoring the many 1’s in the index and simply considering this LoD Tensor as a usual Tensor:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">口口口口</span> <span class="o">...</span> <span class="n">口</span>
</pre></div>
</div>
</div>
<div class="section" id="model-parameters">
<span id="model-parameters"></span><h3>Model Parameters<a class="headerlink" href="#model-parameters" title="永久链接至标题">¶</a></h3>
<p>A model parameter is just a usual Tensor, which, just like the above example, is a <strong>0-level LoD Tensor</strong>.</p>
</div>
</div>
<div class="section" id="the-lod-tensor">
<span id="the-lod-tensor"></span><h2>The LoD Tensor<a class="headerlink" href="#the-lod-tensor" title="永久链接至标题">¶</a></h2>
<p>Let us revisit above example of the 2-level LoD Tensor</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">3</span>           <span class="mi">1</span>  <span class="mi">2</span>
<span class="mi">3</span>   <span class="mi">2</span>  <span class="mi">4</span>    <span class="mi">1</span>  <span class="mi">2</span>  <span class="mi">3</span>
<span class="o">|||</span> <span class="o">||</span> <span class="o">||||</span> <span class="o">|</span>  <span class="o">||</span> <span class="o">|||</span>
</pre></div>
</div>
<p>It is indeed a tree, where leaves are elementary sequences identified by <strong>branches</strong>.</p>
<p>For example, the third sentence in above example is identified by branch &lt;0,2&gt;, where 0 indicates the first article with length 3, and 2 indicates the third sentence in this article with length 4.</p>
<div class="section" id="the-lod-index">
<span id="the-lod-index"></span><h3>The LoD Index<a class="headerlink" href="#the-lod-index" title="永久链接至标题">¶</a></h3>
<p>We can save the LoD index in the above example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">3</span>           <span class="mi">1</span>  <span class="mi">2</span>
<span class="mi">3</span>   <span class="mi">2</span>  <span class="mi">4</span>    <span class="mi">1</span>  <span class="mi">2</span>  <span class="mi">3</span>
</pre></div>
</div>
<p>in a not-full 2D matrix:</p>
<div class="highlight-c++"><div class="highlight"><pre><span></span><span class="k">typedef</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="o">&gt;</span> <span class="n">LoD</span><span class="p">;</span>
</pre></div>
</div>
<p>where</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">LoD.size()</span></code> is the number of levels, or the maximum length of branches,</li>
<li><code class="docutils literal"><span class="pre">LoD[i][j]</span></code> is the length of the j-th segment at the i-th level.</li>
</ul>
</div>
</div>
<div class="section" id="the-offset-representation">
<span id="the-offset-representation"></span><h2>The Offset Representation<a class="headerlink" href="#the-offset-representation" title="永久链接至标题">¶</a></h2>
<p>To quickly access elementary sequences, we adopt an offset representation – instead of saving the lengths, we save the beginning and ending elements of sequences.</p>
<p>In the above example, we accumulate the length of elementary sequences:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">3</span> <span class="mi">2</span> <span class="mi">4</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span>
</pre></div>
</div>
<p>into offsets</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">0</span>  <span class="mi">3</span>  <span class="mi">5</span>   <span class="mi">9</span>   <span class="mi">10</span>  <span class="mi">12</span>   <span class="mi">15</span>
   <span class="o">=</span>  <span class="o">=</span>   <span class="o">=</span>   <span class="o">=</span>   <span class="o">=</span>    <span class="o">=</span>
   <span class="mi">3</span>  <span class="mi">2</span><span class="o">+</span><span class="mi">3</span> <span class="mi">4</span><span class="o">+</span><span class="mi">5</span> <span class="mi">1</span><span class="o">+</span><span class="mi">9</span> <span class="mi">2</span><span class="o">+</span><span class="mi">10</span> <span class="mi">3</span><span class="o">+</span><span class="mi">12</span>
</pre></div>
</div>
<p>so we know that the first sentence is from word 0 to word 3, and the second sentence from word 3 to word 5.</p>
<p>Similarly, the lengths in the top level LoD</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">3</span> <span class="mi">1</span> <span class="mi">2</span>
</pre></div>
</div>
<p>are transformed into offsets of elements/words as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">0</span> <span class="mi">3</span> <span class="mi">4</span>   <span class="mi">6</span>
  <span class="o">=</span> <span class="o">=</span>   <span class="o">=</span>
  <span class="mi">3</span> <span class="mi">3</span><span class="o">+</span><span class="mi">1</span> <span class="mi">4</span><span class="o">+</span><span class="mi">2</span>
</pre></div>
</div>
</div>
<div class="section" id="slicing-of-lod-tensors">
<span id="slicing-of-lod-tensors"></span><h2>Slicing of LoD Tensors<a class="headerlink" href="#slicing-of-lod-tensors" title="永久链接至标题">¶</a></h2>
<p>When we use the above 2-level LoD Tensor as the input to a nested-RNN, we need to retrieve certain sequences.  Here we define the sequence identified by branch &lt;i,j,...&gt; as the <strong>&lt;i,j,...&gt;-slice</strong>.</p>
<p>For example, the &lt;2&gt;-slice of above example is</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">10</span>      <span class="mi">15</span>
<span class="mi">10</span>  <span class="mi">12</span>  <span class="mi">15</span>
  <span class="o">||</span> <span class="o">|||</span>
</pre></div>
</div>
<p>and the &lt;2,0&gt;-slice of above slice is</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">10</span>  <span class="mi">12</span>
  <span class="o">||</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>