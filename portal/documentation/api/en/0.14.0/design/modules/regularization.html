<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="regularization-in-paddlepaddle">
<span id="regularization-in-paddlepaddle"></span><h1>Regularization in PaddlePaddle<a class="headerlink" href="#regularization-in-paddlepaddle" title="永久链接至标题">¶</a></h1>
<div class="section" id="introduction-to-regularization">
<span id="introduction-to-regularization"></span><h2>Introduction to Regularization<a class="headerlink" href="#introduction-to-regularization" title="永久链接至标题">¶</a></h2>
<p>A central problem in machine learning is how to design an algorithm that will perform well not just on the training data, but also on new data. A frequently faced problem is the problem of <strong>overfitting</strong>, where the model does not make reliable predictions on new unseen data. <strong>Regularization</strong> is the process of introducing additional information in order to prevent overfitting. This is usually done by adding extra penalties to the loss function that restricts the parameter spaces that an optimization algorithm can explore.</p>
<div class="section" id="parameter-norm-penalties">
<span id="parameter-norm-penalties"></span><h3>Parameter Norm Penalties<a class="headerlink" href="#parameter-norm-penalties" title="永久链接至标题">¶</a></h3>
<p>Most common regularization approaches in deep learning are based on limiting the capacity of the models by adding a parameter norm penalty to the objective function <code class="docutils literal"><span class="pre">J</span></code>. This is given as follows:</p>
<p><img align="center" src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/loss_equation.png"/><br/></p>
<p>The parameter <code class="docutils literal"><span class="pre">alpha</span></code> is a hyperparameter that weights the relative contribution of the norm penalty term, <code class="docutils literal"><span class="pre">omega</span></code>, relative to the standard objective function <code class="docutils literal"><span class="pre">J</span></code>.</p>
<p>The most commonly used norm penalties are the L2 norm penalty and the L1 norm penalty. These are given as follows:</p>
<div class="section" id="l2-regularization">
<span id="l2-regularization"></span><h4>L2 Regularization:<a class="headerlink" href="#l2-regularization" title="永久链接至标题">¶</a></h4>
<p><img align="center" src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/l2_regularization.png"/><br/></p>
</div>
<div class="section" id="l1-regularization">
<span id="l1-regularization"></span><h4>L1 Regularization<a class="headerlink" href="#l1-regularization" title="永久链接至标题">¶</a></h4>
<p><img align="center" src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/l1_regularization.png"/><br/></p>
<p>A much more detailed mathematical background of regularization can be found <a class="reference external" href="http://www.deeplearningbook.org/contents/regularization.html">here</a>.</p>
</div>
</div>
</div>
<div class="section" id="regularization-survey">
<span id="regularization-survey"></span><h2>Regularization Survey<a class="headerlink" href="#regularization-survey" title="永久链接至标题">¶</a></h2>
<p>A detailed survey of regularization in various deep learning frameworks can be found <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/wiki/Regularization-Survey">here</a>.</p>
</div>
<div class="section" id="proposal-for-regularization-in-paddlepaddle">
<span id="proposal-for-regularization-in-paddlepaddle"></span><h2>Proposal for Regularization in PaddlePaddle<a class="headerlink" href="#proposal-for-regularization-in-paddlepaddle" title="永久链接至标题">¶</a></h2>
<div class="section" id="low-level-implementation">
<span id="low-level-implementation"></span><h3>Low-Level implementation<a class="headerlink" href="#low-level-implementation" title="永久链接至标题">¶</a></h3>
<p>In the new design, we propose to create new operations for regularization. For now, we can add 2 ops that correspond to the most frequently used regularizations:</p>
<ul class="simple">
<li>L2_regularization_op</li>
<li>L1_regularization_op</li>
</ul>
<p>These ops can be like any other ops with their own CPU/GPU implementations either using Eigen or separate CPU and GPU kernels. As the initial implementation, we can implement their kernels using Eigen following the abstraction pattern implemented for <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/operators/accuracy_op.h">Activation Ops</a>. This abstraction pattern can make it very easy to implement new regularization schemes other than L1 and L2 norm penalties.</p>
<p>The idea of building ops for regularization is in sync with the refactored Paddle philosophy of using operators to represent any computation unit. The way these ops will be added to the computation graph, will be decided by the <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/modules/python_api.md#layer-function">layer functions</a> in Python API.</p>
</div>
<div class="section" id="computation-graph">
<span id="computation-graph"></span><h3>Computation Graph<a class="headerlink" href="#computation-graph" title="永久链接至标题">¶</a></h3>
<p>Below is an example of a really simple feed forward neural network.</p>
<p><img align="center" src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/feed_forward.png"/><br/></p>
<p>The Python API will modify this computation graph to add regularization operators. The modified computation graph will look as follows:</p>
<p><img align="center" src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/feed_forward_regularized.png"/><br/></p>
</div>
<div class="section" id="python-api-implementation-for-regularization">
<span id="python-api-implementation-for-regularization"></span><h3>Python API implementation for Regularization<a class="headerlink" href="#python-api-implementation-for-regularization" title="永久链接至标题">¶</a></h3>
<p>Using the low level ops, <code class="docutils literal"><span class="pre">L2_regularization_op</span></code> and <code class="docutils literal"><span class="pre">L1_regularization_op</span></code>, any user can add regularization to their computation graphs. However, this will require a lot of lines of code and we should design Python APIs that support regularization. An example of such an API can be seen in <a class="reference external" href="https://keras.io/regularizers/">Keras</a>. As per the PaddlePaddle <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/modules/python_api.md">Python API design</a>, the layer functions are responsible for creating operators, operator parameters and variables. Since regularization is a property of parameters, it makes sense to create these in the layer functions.</p>
<div class="section" id="creation-of-regularization-ops">
<span id="creation-of-regularization-ops"></span><h4>Creation of Regularization ops<a class="headerlink" href="#creation-of-regularization-ops" title="永久链接至标题">¶</a></h4>
<p>There are two possibilities for creating the regularization ops:</p>
<ol class="simple">
<li>We create these ops immediately while building the computation graph.</li>
<li>We add these ops in a lazy manner, just before the backward, similar to the way the optimization ops are added.</li>
</ol>
<p>The proposal is to add these ops in a lazy manner just before the backward pass.</p>
</div>
<div class="section" id="storage-of-regularization-attributes">
<span id="storage-of-regularization-attributes"></span><h4>Storage of Regularization attributes<a class="headerlink" href="#storage-of-regularization-attributes" title="永久链接至标题">¶</a></h4>
<p>Since we want to create the regularization ops in a lazy manner, the regularization attributes (type of regularization and weight of regularization penalty) can be stored as attributes of the <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/framework/framework.py#L421"><code class="docutils literal"><span class="pre">Parameter</span></code></a> class. This is because regularization is a property of the parameters and storing regularization properties with Parameters also allows for shared parameters.</p>
</div>
<div class="section" id="high-level-api">
<span id="high-level-api"></span><h4>High-level API<a class="headerlink" href="#high-level-api" title="永久链接至标题">¶</a></h4>
<p>In PaddlePaddle Python API, users will primarily rely on <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/modules/python_api.md#layer-function">layer functions</a> to create neural network layers. Hence, we also need to provide regularization functionality in layer functions. The design of these APIs can be postponed for later right now. A good reference for these APIs can be found in <a class="reference external" href="https://keras.io/regularizers/">Keras</a> and also by looking at Tensorflow in <a class="reference external" href="https://www.tensorflow.org/api_guides/python/contrib.layers"><code class="docutils literal"><span class="pre">tf.contrib.layers</span></code></a>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="articleComments">
</div>
</div>