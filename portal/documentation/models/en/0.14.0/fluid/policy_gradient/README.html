{% verbatim %}
<p>﻿运行本目录下的程序示例需要使用PaddlePaddle的最新develop分枝。如果您的PaddlePaddle安装版本低于此要求，请按照<a href="http://www.paddlepaddle.org/docs/develop/documentation/zh/build_and_install/pip_install_cn.html">安装文档</a>中的说明更新PaddlePaddle安装版本。</p>
<hr/>
<h1>Policy Gradient RL by PaddlePaddle</h1>
<p>本文介绍了如何使用PaddlePaddle通过policy-based的强化学习方法来训练一个player（actor model）, 我们希望这个player可以完成简单的走阶梯任务。</p>
<p>内容分为:</p>
<ul>
<li>任务描述</li>
<li>模型</li>
<li>策略（目标函数）</li>
<li>算法（Gradient ascent）</li>
<li>PaddlePaddle实现</li>
</ul>
<h2>1. 任务描述</h2>
<p>假设有一个阶梯，连接A、B点，player从A点出发，每一步只能向前走一步或向后走一步，到达B点即为完成任务。我们希望训练一个聪明的player，它知道怎么最快的从A点到达B点。
我们在命令行以下边的形式模拟任务：
</p><div class="highlight"><pre><span></span>A - O - - - - - B
</pre></div>
一个‘-'代表一个阶梯，A点在行头，B点在行末，O代表player当前在的位置。
<h2>2. Policy Gradient</h2>
<h3>2.1 模型</h3>
<h4>inputyer</h4>
<p>模型的输入是player观察到的当前阶梯的状态<span class="markdown-equation" id="equation-0">$S$</span>, 要包含阶梯的长度和player当前的位置信息。
在命令行模拟的情况下，player的位置和阶梯长度连个变量足以表示当前的状态，但是我们为了便于将这个demo推广到更复杂的任务场景，我们这里用一个向量来表示游戏状态<span class="markdown-equation" id="equation-0">$S$</span>.
向量<span class="markdown-equation" id="equation-0">$S$</span>的长度为阶梯的长度，每一维代表一个阶梯，player所在的位置为1，其它位置为0.
下边是一个例子：
</p><div class="highlight"><pre><span></span>S = [0, 1, 0, 0]  // 阶梯长度为4，player在第二个阶梯上。
</pre></div>
<h4>hidden layer</h4>
<p>隐藏层采用两个全连接layer <code>FC_1</code>和<code>FC_2</code>, 其中<code>FC_1</code> 的size为10， <code>FC_2</code>的size为2.</p>
<h4>output layer</h4>
<p>我们使用softmax将<code>FC_2</code>的output映射为所有可能的动作（前进或后退）的概率分布（Probability of taking the action）,即为一个二维向量<code>act_probs</code>, 其中，<code>act_probs[0]</code> 为后退的概率，<code>act_probs[1]</code>为前进的概率。</p>
<h4>模型表示</h4>
<p>我将我们的player模型(actor)形式化表示如下：
<span class="markdown-equation" id="equation-3">$$a = \pi_\theta(s)$$</span>
其中<span class="markdown-equation" id="equation-4">$\theta$</span>表示模型的参数，<span class="markdown-equation" id="equation-5">$s$</span>是输入状态。</p>
<h3>2.2 策略（目标函数）</h3>
<p>我们怎么评估一个player(模型)的好坏呢？首先我们定义几个术语：
我们让<span class="markdown-equation" id="equation-6">$\pi_\theta(s)$</span>来玩一局游戏，<span class="markdown-equation" id="equation-7">$s_t$</span>表示第<span class="markdown-equation" id="equation-8">$t$</span>时刻的状态，<span class="markdown-equation" id="equation-9">$a_t$</span>表示在状态<span class="markdown-equation" id="equation-7">$s_t$</span>做出的动作，<span class="markdown-equation" id="equation-11">$r_t$</span>表示做过动作<span class="markdown-equation" id="equation-9">$a_t$</span>后得到的奖赏。
一局游戏的过程可以表示如下：
<span class="markdown-equation" id="equation-13">$$\tau = [s_1, a_1, r_1, s_2, a_2, r_2 ... s_T, a_T, r_T] \tag{1}$$</span></p>
<p>一局游戏的奖励表示如下：
<span class="markdown-equation" id="equation-14">$$R(\tau) = \sum_{t=1}^Tr_t$$</span></p>
<p>player玩一局游戏，可能会出现多种操作序列<span class="markdown-equation" id="equation-15">$\tau$</span> ,某个<span class="markdown-equation" id="equation-15">$\tau$</span>出现的概率是依赖于player model的<span class="markdown-equation" id="equation-4">$\theta$</span>, 记做：
<span class="markdown-equation" id="equation-18">$$P(\tau | \theta)$$</span>
那么，给定一个<span class="markdown-equation" id="equation-4">$\theta$</span>(player model), 玩一局游戏，期望得到的奖励是：
<span class="markdown-equation" id="equation-20">$$\overline {R}_\theta = \sum_\tau R(\tau)\sum_\tau R(\tau) P(\tau|\theta)$$</span>
大多数情况，我们无法穷举出所有的<span class="markdown-equation" id="equation-15">$\tau$</span>，所以我们就抽取N个<span class="markdown-equation" id="equation-15">$\tau$</span>来计算近似的期望：
<span class="markdown-equation" id="equation-23">$$\overline {R}_\theta = \sum_\tau R(\tau) P(\tau|\theta) \approx \frac{1}{N} \sum_{n=1}^N R(\tau^n)$$</span></p>
<p><span class="markdown-equation" id="equation-24">$\overline {R}_\theta$</span>就是我们需要的目标函数，它表示了一个参数为<span class="markdown-equation" id="equation-4">$\theta$</span>的player玩一局游戏得分的期望，这个期望越大，代表这个player能力越强。</p>
<h3>2.3 算法（Gradient ascent）</h3>
<p>我们的目标函数是<span class="markdown-equation" id="equation-24">$\overline {R}_\theta$</span>, 我们训练的任务就是, 我们训练的任务就是:
<span class="markdown-equation" id="equation-27">$$\theta^* = \arg\max_\theta  \overline {R}_\theta$$</span></p>
<p>为了找到理想的<span class="markdown-equation" id="equation-4">$\theta$</span>，我们使用Gradient ascent方法不断在<span class="markdown-equation" id="equation-24">$\overline {R}_\theta$</span>的梯度方向更新<span class="markdown-equation" id="equation-4">$\theta$</span>，可表示如下：
<span class="markdown-equation" id="equation-31">$$\theta' = \theta + \eta * \bigtriangledown \overline {R}_\theta$$</span></p>
<p><span class="markdown-equation" id="equation-32">$$ \bigtriangledown \overline {R}_\theta =  \sum_\tau R(\tau)  \bigtriangledown P(\tau|\theta)\\
= \sum_\tau R(\tau) P(\tau|\theta) \frac{\bigtriangledown P(\tau|\theta)}{P(\tau|\theta)} \\
=\sum_\tau R(\tau) P(\tau|\theta) {\bigtriangledown \log P(\tau|\theta)} $$</span></p>
<p><span class="markdown-equation" id="equation-33">$$P(\tau|\theta) = P(s_1)P(a_1|s_1,\theta)P(s_2, r_1|s_1,a_1)P(a_2|s_2,\theta)P(s_3,r_2|s_2,a_2)...P(a_t|s_t,\theta)P(s_{t+1}, r_t|s_t,a_t)\\
=P(s_1) \sum_{t=1}^T P(a_t|s_t,\theta)P(s_{t+1}, r_t|s_t,a_t)$$</span></p>
<p><span class="markdown-equation" id="equation-34">$$\log P(\tau|\theta) =  \log P(s_1)  + \sum_{t=1}^T [\log P(a_t|s_t,\theta) + \log P(s_{t+1}, r_t|s_t,a_t)]$$</span></p>
<p><span class="markdown-equation" id="equation-35">$$ \bigtriangledown \log P(\tau|\theta) =  \sum_{t=1}^T \bigtriangledown \log P(a_t|s_t,\theta)$$</span></p>
<p><span class="markdown-equation" id="equation-36">$$ \bigtriangledown \overline {R}_\theta =  \sum_\tau R(\tau) P(\tau|\theta) {\bigtriangledown \log P(\tau|\theta)} \\
\approx  \frac{1}{N} \sum_{n=1}^N R(\tau^n) {\bigtriangledown \log P(\tau|\theta)} \\
= \frac{1}{N} \sum_{n=1}^N R(\tau^n) {\sum_{t=1}^T \bigtriangledown \log P(a_t|s_t,\theta)} \\
= \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^T R(\tau^n) { \bigtriangledown \log P(a_t|s_t,\theta)} \tag{11}$$</span></p>
<h4>2.3.2 导数解释</h4>
<p>在使用深度学习框架进行训练求解时，一般用梯度下降方法，所以我们把Gradient ascent转为Gradient
descent, 重写等式<span class="markdown-equation" id="equation-37">$(5)(6)$</span>为：</p>
<p><span class="markdown-equation" id="equation-38">$$\theta^* = \arg\min_\theta  (-\overline {R}_\theta \tag{13}$$</span>
<span class="markdown-equation" id="equation-39">$$\theta' = \theta - \eta * \bigtriangledown (-\overline {R}_\theta)) \tag{14}$$</span></p>
<p>根据上一节的推导，<span class="markdown-equation" id="equation-40">$ (-\bigtriangledown \overline {R}_\theta) $</span>结果如下：</p>
<p><span class="markdown-equation" id="equation-41">$$ -\bigtriangledown \overline {R}_\theta
= \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^T R(\tau^n) { \bigtriangledown -\log P(a_t|s_t,\theta)} \tag{15}$$</span></p>
<p>根据等式(14), 我们的player的模型可以设计为：</p>
<p align="center">
<img hspace="10" src="images/PG_1.svg" width="620"/> <br/>
图 1
</p>
<p>用户的在一局游戏中的一次操作可以用元组<span class="markdown-equation" id="equation-42">$(s_t, a_t)$</span>, 就是在状态<span class="markdown-equation" id="equation-7">$s_t$</span>状态下做了动作<span class="markdown-equation" id="equation-9">$a_t$</span>, 我们通过图(1)中的前向网络计算出来cross entropy cost为<span class="markdown-equation" id="equation-45">$−\log P(a_t|s_t,\theta)$</span>， 恰好是等式(15)中我们需要微分的一项。
图1是我们需要的player模型，我用这个网络的前向计算可以预测任何状态下该做什么动作。但是怎么去训练学习这个网络呢？在等式(15)中还有一项<span class="markdown-equation" id="equation-46">$R(\tau^n)$</span>, 我做反向梯度传播的时候要加上这一项，所以我们需要在图1基础上再加上<span class="markdown-equation" id="equation-46">$R(\tau^n)$</span>， 如 图2 所示：</p>
<p align="center">
<img hspace="10" src="images/PG_2.svg" width="620"/> <br/>
图 2
</p>
<p>图2就是我们最终的网络结构。</p>
<h4>2.3.3 直观理解</h4>
<p>对于等式(15),我只看游戏中的一步操作，也就是这一项: <span class="markdown-equation" id="equation-48">$R(\tau^n) { \bigtriangledown -\log P(a_t|s_t,\theta)}$</span>,  我们可以简单的认为我们训练的目的是让 <span class="markdown-equation" id="equation-49">$R(\tau^n) {[ -\log P(a_t|s_t,\theta)]}$</span>尽可能的小，也就是<span class="markdown-equation" id="equation-50">$R(\tau^n) \log P(a_t|s_t,\theta)$</span>尽可能的大。</p>
<ul>
<li>如果我们当前游戏局的奖励<span class="markdown-equation" id="equation-46">$R(\tau^n)$</span>为正，那么我们希望当前操作的出现的概率<span class="markdown-equation" id="equation-52">$P(a_t|s_t,\theta)$</span>尽可能大。</li>
<li>如果我们当前游戏局的奖励<span class="markdown-equation" id="equation-46">$R(\tau^n)$</span>为负，那么我们希望当前操作的出现的概率<span class="markdown-equation" id="equation-52">$P(a_t|s_t,\theta)$</span>尽可能小。</li>
</ul>
<h4>2.3.4 一个问题</h4>
<p>一人犯错，诛连九族。一人得道，鸡犬升天。如果一局游戏得到奖励，我们希望帮助获得奖励的每一次操作都被重视；否则，导致惩罚的操作都要被冷落一次。
是不是很有道理的样子？但是，如果有些游戏场景只有奖励，没有惩罚，怎么办？也就是所有的<span class="markdown-equation" id="equation-46">$R(\tau^n)$</span>都为正。
针对不同的游戏场景，我们有不同的解决方案：</p>
<ol>
<li>每局游戏得分不一样：将每局的得分减去一个bias，结果就有正有负了。</li>
<li>每局游戏得分一样：把完成一局的时间作为计分因素，并减去一个bias.</li>
</ol>
<p>我们在第一章描述的游戏场景，需要用第二种 ，player每次到达终点都会收到1分的奖励，我们可以按完成任务所用的步数来定义奖励R.
更进一步，我们认为一局游戏中每步动作对结局的贡献是不同的，有聪明的动作，也有愚蠢的操作。直观的理解，一般是靠前的动作是愚蠢的，靠后的动作是聪明的。既然有了这个价值观，那么我们拿到1分的奖励，就不能平均分给每个动作了。
如图3所示，让所有动作按先后排队，从后往前衰减地给每个动作奖励，然后再每个动作的奖励再减去所有动作奖励的平均值：</p>
<p align="center">
<img hspace="10" src="images/PG_3.svg" width="620"/> <br/>
图 3
</p>
<h2>3. 训练效果</h2>
<p>demo运行训练效果如下，经过1000轮尝试，我们的player就学会了如何有效的完成任务了：</p>
<div class="highlight"><pre><span></span>---------O    epoch: 0; steps: 42
---------O    epoch: 1; steps: 77
---------O    epoch: 2; steps: 82
---------O    epoch: 3; steps: 64
---------O    epoch: 4; steps: 79
---------O    epoch: 501; steps: 19
---------O    epoch: 1001; steps: 9
---------O    epoch: 1501; steps: 9
---------O    epoch: 2001; steps: 11
---------O    epoch: 2501; steps: 9
---------O    epoch: 3001; steps: 9
---------O    epoch: 3002; steps: 9
---------O    epoch: 3003; steps: 9
---------O    epoch: 3004; steps: 9
---------O    epoch: 3005; steps: 9
---------O    epoch: 3006; steps: 9
---------O    epoch: 3007; steps: 9
---------O    epoch: 3008; steps: 9
---------O    epoch: 3009; steps: 9
---------O    epoch: 3010; steps: 11
---------O    epoch: 3011; steps: 9
---------O    epoch: 3012; steps: 9
---------O    epoch: 3013; steps: 9
---------O    epoch: 3014; steps: 9
</pre></div>
{% endverbatim %}