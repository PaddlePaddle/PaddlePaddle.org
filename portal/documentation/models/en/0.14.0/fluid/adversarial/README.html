{% verbatim %}
<p>The minimum PaddlePaddle version needed for the code sample in this directory is the lastest develop branch. If you are on a version of PaddlePaddle earlier than this, <a href="http://www.paddlepaddle.org/docs/develop/documentation/en/build_and_install/pip_install_en.html">please update your installation</a>.</p>
<hr/>
<h1>Advbox</h1>
<p>Advbox is a toolbox to generate adversarial examples that fool neural networks and Advbox can benchmark the robustness of machine learning models.</p>
<p>The Advbox is based on <a href="https://github.com/PaddlePaddle/Paddle">PaddlePaddle</a> Fluid and is under continual development, always welcoming contributions of the latest method of adversarial attacks and defenses.</p>
<h2>Overview</h2>
<p><a href="https://arxiv.org/abs/1312.6199">Szegedy et al.</a> discovered an intriguing properties of deep neural networks in the context of image classification for the first time. They showed that despite the state-of-the-art deep networks are surprisingly susceptible to adversarial attacks in the form of small perturbations to images that remain (almost) imperceptible to human vision system. These perturbations are found by optimizing the input to maximize the prediction error and the images modified by these perturbations are called as <code>adversarial examples</code>. The profound implications of these results triggered a wide interest of researchers in adversarial attacks and their defenses for deep learning in general.</p>
<p>Advbox is similar to <a href="https://github.com/bethgelab/foolbox">Foolbox</a> and <a href="https://github.com/tensorflow/cleverhans">CleverHans</a>. CleverHans only supports TensorFlow framework while foolbox interfaces with many popular machine learning frameworks such as PyTorch, Keras, TensorFlow, Theano, Lasagne and MXNet. However, these two great libraries don't support PaddlePaddle, an easy-to-use, efficient, flexible and scalable deep learning platform which is originally developed by Baidu scientists and engineers for the purpose of applying deep learning to many products at Baidu.</p>
<h2>Usage</h2>
<p>Advbox provides many stable reference implementations of modern methods to generate adversarial examples such as FGSM, DeepFool, JSMA. When you want to benchmark the robustness of your neural networks , you can use the advbox to generate some adversarial examples and benchmark the networks. Some tips of using Advbox:</p>
<ol>
<li>Train a model and save the parameters.</li>
<li>Load the parameters which has been trained，then reconstruct the model.</li>
<li>Use advbox to generate the adversarial samples.</li>
</ol>
<h4>Dependencies</h4>
<ul>
<li>PaddlePaddle: <a href="http://www.paddlepaddle.org/docs/develop/documentation/en/build_and_install/pip_install_en.html">the lastest develop branch</a></li>
<li>Python 2.x</li>
</ul>
<h4>Structure</h4>
<p>Network models, attack method's implements and the criterion that defines adversarial examples are three essential elements to generate adversarial examples. Misclassification is adopted as the adversarial criterion for briefness in Advbox.</p>
<p>The structure of Advbox module are as follows:</p>
<pre><code>.
├── advbox
|   ├── __init__.py
|   ├── attack
|        ├── __init__.py
|        ├── base.py
|        ├── deepfool.py
|        ├── gradient_method.py
|        ├── lbfgs.py
|        └── saliency.py
|   ├── models
|        ├── __init__.py
|        ├── base.py
|        └── paddle.py
|   └── adversary.py
├── tutorials
|   ├── __init__.py
|   ├── mnist_model.py
|   ├── mnist_tutorial_lbfgs.py
|   ├── mnist_tutorial_fgsm.py
|   ├── mnist_tutorial_bim.py
|   ├── mnist_tutorial_ilcm.py
|   ├── mnist_tutorial_mifgsm.py
|   ├── mnist_tutorial_jsma.py
|   └── mnist_tutorial_deepfool.py
└── README.md
</code></pre>
<p><strong>advbox.attack</strong></p>
<p>Advbox implements several popular adversarial attacks which search adversarial examples. Each attack method uses a distance measure(L1, L2, etc.) to quantify the size of adversarial perturbations. Advbox is easy to craft adversarial example as some attack methods could perform internal hyperparameter tuning to find the minimum perturbation.</p>
<p><strong>advbox.model</strong></p>
<p>Advbox implements interfaces to PaddlePaddle. Additionally, other deep learning framworks such as TensorFlow can also be defined and employed. The module is use to compute predictions and gradients for given inputs in a specific framework.</p>
<p><strong>advbox.adversary</strong></p>
<p>Adversary contains the original object, the target and the adversarial examples. It provides the misclassification as the criterion to accept a adversarial example.</p>
<h2>Tutorials</h2>
<p>The <code>./tutorials/</code> folder provides some tutorials to generate adversarial examples on the MNIST dataset. You can slightly modify the code to apply to other dataset. These attack methods are supported in Advbox:</p>
<ul>
<li><a href="https://arxiv.org/abs/1312.6199">L-BFGS</a></li>
<li><a href="https://arxiv.org/abs/1412.6572">FGSM</a></li>
<li><a href="https://arxiv.org/abs/1607.02533">BIM</a></li>
<li><a href="https://arxiv.org/abs/1607.02533">ILCM</a></li>
<li><a href="https://arxiv.org/pdf/1710.06081.pdf">MI-FGSM</a></li>
<li><a href="https://arxiv.org/pdf/1511.07528">JSMA</a></li>
<li><a href="https://arxiv.org/abs/1511.04599">DeepFool</a></li>
</ul>
<h2>Testing</h2>
<p>Benchmarks on a vanilla CNN model.</p>
<blockquote>
<p>MNIST</p>
</blockquote>
<table>
<thead>
<tr>
<th align="center">adversarial attacks</th>
<th align="center">fooling rate (non-targeted)</th>
<th align="center">fooling rate (targeted)</th>
<th align="center">max_epsilon</th>
<th align="center">iterations</th>
<th align="center">Strength</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">L-BFGS</td>
<td align="center">---</td>
<td align="center">89.2%</td>
<td align="center">---</td>
<td align="center">One shot</td>
<td align="center">***</td>
</tr>
<tr>
<td align="center">FGSM</td>
<td align="center">57.8%</td>
<td align="center">26.55%</td>
<td align="center">0.3</td>
<td align="center">One shot</td>
<td align="center">***</td>
</tr>
<tr>
<td align="center">BIM</td>
<td align="center">97.4%</td>
<td align="center">---</td>
<td align="center">0.1</td>
<td align="center">100</td>
<td align="center">****</td>
</tr>
<tr>
<td align="center">ILCM</td>
<td align="center">---</td>
<td align="center">100.0%</td>
<td align="center">0.1</td>
<td align="center">100</td>
<td align="center">****</td>
</tr>
<tr>
<td align="center">MI-FGSM</td>
<td align="center">94.4%</td>
<td align="center">100.0%</td>
<td align="center">0.1</td>
<td align="center">100</td>
<td align="center">****</td>
</tr>
<tr>
<td align="center">JSMA</td>
<td align="center">96.8%</td>
<td align="center">90.4%</td>
<td align="center">0.1</td>
<td align="center">2000</td>
<td align="center">***</td>
</tr>
<tr>
<td align="center">DeepFool</td>
<td align="center">97.7%</td>
<td align="center">51.3%</td>
<td align="center">---</td>
<td align="center">100</td>
<td align="center">****</td>
</tr>
</tbody>
</table>
<ul>
<li>The strength (higher for more asterisks) is based on the impression from the reviewed literature.</li>
</ul>
<hr/>
<h2>References</h2>
<ul>
<li><a href="https://arxiv.org/abs/1312.6199">Intriguing properties of neural networks</a>, C. Szegedy et al., arxiv 2014</li>
<li><a href="https://arxiv.org/abs/1412.6572">Explaining and Harnessing Adversarial Examples</a>, I. Goodfellow et al., ICLR 2015</li>
<li><a href="https://arxiv.org/pdf/1607.02533v3.pdf">Adversarial Examples In The Physical World</a>, A. Kurakin et al., ICLR workshop 2017</li>
<li><a href="https://arxiv.org/abs/1710.06081">Boosting Adversarial Attacks with Momentum</a>, Yinpeng Dong et al., arxiv 2018</li>
<li><a href="https://arxiv.org/abs/1511.07528">The Limitations of Deep Learning in Adversarial Settings</a>, N. Papernot et al., ESSP 2016</li>
<li><a href="https://arxiv.org/abs/1511.04599">DeepFool: a simple and accurate method to fool deep neural networks</a>, S. Moosavi-Dezfooli et al., CVPR 2016</li>
<li><a href="https://arxiv.org/abs/1707.04131">Foolbox: A Python toolbox to benchmark the robustness of machine learning models</a>, Jonas Rauber et al., arxiv 2018</li>
<li><a href="https://github.com/tensorflow/cleverhans#setting-up-cleverhans">CleverHans: An adversarial example library for constructing attacks, building defenses, and benchmarking both</a></li>
<li><a href="https://arxiv.org/abs/1801.00553">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a>, Naveed Akhtar, Ajmal Mian, arxiv 2018</li>
</ul>
{% endverbatim %}