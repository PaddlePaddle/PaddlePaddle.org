{% verbatim %}
<p>运行本目录下的程序示例需要使用PaddlePaddle v0.10.0 版本。如果您的PaddlePaddle安装版本低于此要求，请按照<a href="http://www.paddlepaddle.org/docs/develop/documentation/zh/build_and_install/pip_install_cn.html">安装文档</a>中的说明更新PaddlePaddle安装版本。</p>
<hr/>
<h1>Hsigmoid加速词向量训练</h1>
<h2>背景介绍</h2>
<p>在自然语言处理领域中，传统做法通常使用one-hot向量来表示词，比如词典为['我', '你', '喜欢']，可以用[1,0,0]、[0,1,0]和[0,0,1]这三个向量分别表示'我'、'你'和'喜欢'。这种表示方式比较简洁，但是当词表很大时，容易产生维度爆炸问题；而且任意两个词的向量是正交的，向量包含的信息有限。为了避免或减轻one-hot表示的缺点，目前通常使用词向量来取代one-hot表示，词向量也就是word embedding，即使用一个低维稠密的实向量取代高维稀疏的one-hot向量。训练词向量的方法有很多种，神经网络模型是其中之一，包括CBOW、Skip-gram等，这些模型本质上都是一个分类模型，当词表较大即类别较多时，传统的softmax将非常消耗时间。PaddlePaddle提供了Hsigmoid Layer、NCE Layer，来加速模型的训练过程。本文主要介绍如何使用Hsigmoid Layer来加速训练，词向量相关内容请查阅PaddlePaddle Book中的<a href="https://github.com/PaddlePaddle/book/tree/develop/04.word2vec">词向量章节</a>。</p>
<h2>Hsigmoid Layer</h2>
<p>Hsigmoid Layer引用自论文[<a href="#参考文献">1</a>]，Hsigmoid指Hierarchical-sigmoid，原理是通过构建一个分类二叉树来降低计算复杂度，二叉树中每个叶子节点代表一个类别，每个非叶子节点代表一个二类别分类器。例如我们一共有4个类别分别是0、1、2、3，softmax会分别计算4个类别的得分，然后归一化得到概率。当类别数很多时，计算每个类别的概率非常耗时，Hsigmoid Layer会根据类别数构建一个平衡二叉树，如下：</p>
<p align="center">
<img hspace="10" src="images/binary_tree.png" width="220"/> <img hspace="10" src="images/path_to_1.png" width="220"/> <br/>
图1. （a）为平衡二叉树，（b）为根节点到类别1的路径
</p>
<p>二叉树中每个非叶子节点是一个二类别分类器（sigmoid），如果类别是0，则取左子节点继续分类判断，反之取右子节点，直至达到叶节点。按照这种方式，每个类别均对应一条路径，例如从root到类别1的路径编码为0、1。训练阶段我们按照真实类别对应的路径，依次计算对应分类器的损失，然后综合所有损失得到最终损失。预测阶段，模型会输出各个非叶节点分类器的概率，我们可以根据概率获取路径编码，然后遍历路径编码就可以得到最终预测类别。传统softmax的计算复杂度为N（N为词典大小），Hsigmoid可以将复杂度降至log(N)，详细理论细节可参照论文[<a href="#参考文献">1</a>]。</p>
<h2>数据准备</h2>
<h3>PTB数据</h3>
<p>本文采用Penn Treebank (PTB)数据集（<a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz">Tomas Mikolov预处理版本</a>），共包含train、valid和test三个文件。其中使用train作为训练数据，valid作为测试数据。本文训练的是5-gram模型，即用每条数据的前4个词来预测第5个词。PaddlePaddle提供了对应PTB数据集的python包<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/dataset/imikolov.py">paddle.dataset.imikolov</a>    ，自动做数据的下载与预处理。预处理会把数据集中的每一句话前后加上开始符号&lt;s&gt;以及结束符号&lt;e&gt;，然后依据窗口大小（本文为5），从头到尾每次向右滑动窗口并生成一条数据。如"I have a dream that one day"可以生成&lt;s&gt; I have a dream、I have a dream that、have a dream that one、a dream that one day、dream that one day &lt;e&gt;，PaddlePaddle会把词转换成id数据作为预处理的输出。</p>
<h3>自定义数据</h3>
<p>用户可以使用自己的数据集训练模型，自定义数据集最关键的地方是实现reader接口做数据处理，reader需要产生一个迭代器，迭代器负责解析文件中的每一行数据，返回一个python list，例如[1, 2, 3, 4, 5]，分别是第一个到第四个词在字典中的id，PaddlePaddle会进一步将该list转化成<code>paddle.data_type.inter_value</code>类型作为data layer的输入，一个封装样例如下：</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">reader_creator</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">word_dict</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">reader</span><span class="p">():</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">UNK</span> <span class="o">=</span> <span class="n">word_dict</span><span class="p">[</span><span class="s1">'&lt;unk&gt;'</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">l</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'&lt;s&gt;'</span><span class="p">]</span> <span class="o">+</span> <span class="n">l</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="o">+</span> <span class="p">[</span><span class="s1">'&lt;e&gt;'</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">n</span><span class="p">:</span>
                    <span class="n">l</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">UNK</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                        <span class="k">yield</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">n</span><span class="p">:</span><span class="n">i</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">reader</span>


<span class="k">def</span> <span class="nf">train_data</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">word_dict</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Reader interface for training data.</span>

<span class="sd">    It returns a reader creator, each sample in the reader is a word ID tuple.</span>

<span class="sd">    :param filename: path of data file</span>
<span class="sd">    :type filename: str</span>
<span class="sd">    :param word_dict: word dictionary</span>
<span class="sd">    :type word_dict: dict</span>
<span class="sd">    :param n: sliding window size</span>
<span class="sd">    :type n: int</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">reader_creator</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">word_dict</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
</pre></div>
<h2>网络结构</h2>
<p>本文通过训练N-gram语言模型来获得词向量，具体地使用前4个词来预测当前词。网络输入为词在字典中的id，然后查询词向量词表获取词向量，接着拼接4个词的词向量，然后接入一个全连接隐层，最后是<code>Hsigmoid</code>层。详细网络结构见图2：</p>
<p align="center">
<img align="center" src="images/network_conf.png" width="70%"/><br/>
图2. 网络配置结构
</p>
<p>代码如下：</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ngram_lm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">dict_size</span><span class="p">,</span> <span class="n">gram_num</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">emb_layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">embed_param_attr</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">"_proj"</span><span class="p">,</span> <span class="n">initial_std</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">l2_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gram_num</span><span class="p">):</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"__word</span><span class="si">%02d</span><span class="s2">__"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">),</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">data_type</span><span class="o">.</span><span class="n">integer_value</span><span class="p">(</span><span class="n">dict_size</span><span class="p">))</span>
        <span class="n">emb_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span>
                <span class="nb">input</span><span class="o">=</span><span class="n">word</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">param_attr</span><span class="o">=</span><span class="n">embed_param_attr</span><span class="p">))</span>

    <span class="n">target_word</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">"__target_word__"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">data_type</span><span class="o">.</span><span class="n">integer_value</span><span class="p">(</span><span class="n">dict_size</span><span class="p">))</span>

    <span class="n">embed_context</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">emb_layers</span><span class="p">)</span>

    <span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">embed_context</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
        <span class="n">layer_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Extra</span><span class="p">(</span><span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
        <span class="n">bias_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
        <span class="n">param_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span>
            <span class="n">initial_std</span><span class="o">=</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">embed_size</span> <span class="o">*</span> <span class="mi">8</span><span class="p">),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">hsigmoid</span><span class="p">(</span>
                <span class="nb">input</span><span class="o">=</span><span class="n">hidden_layer</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="n">target_word</span><span class="p">,</span>
                <span class="n">param_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"sigmoid_w"</span><span class="p">),</span>
                <span class="n">bias_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"sigmoid_b"</span><span class="p">))</span>
</pre></div>
<p>需要注意在 PaddlePaddle 中，hsigmoid 层将可学习参数存储为一个 <code>[类别数目 - 1 × 隐层向量宽度]</code> 大小的矩阵。预测时，需要将 hsigmoid 层替换为全连接运算<strong>并固定以 <code>sigmoid</code> 为激活</strong>。预测时输出一个宽度为<code>[batch_size x 类别数目 - 1]</code> 维度的矩阵（<code>batch_size = 1</code>时退化为一个向量）。矩阵行向量的每一维计算了一个输入向量属于一个内部结点的右孩子的概率。<strong>全连接运算在加载 hsigmoid 层学习到的参数矩阵时，需要对参数矩阵进行一次转置</strong>。代码片段如下：</p>
<p></p><div class="highlight"><pre><span></span><span class="k">return</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">mixed</span><span class="p">(</span>
    <span class="n">size</span><span class="o">=</span><span class="n">dict_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
    <span class="nb">input</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">trans_full_matrix_projection</span><span class="p">(</span>
        <span class="n">hidden_layer</span><span class="p">,</span> <span class="n">param_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"sigmoid_w"</span><span class="p">)),</span>
    <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">bias_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">"sigmoid_b"</span><span class="p">))</span>
</pre></div>
上述代码片段中的 <code>paddle.layer.mixed</code> 必须以 PaddlePaddle 中 <code>paddle.layer.×_projection</code> 为输入。<code>paddle.layer.mixed</code> 将多个 <code>projection</code> （输入可以是多个）计算结果求和作为输出。<code>paddle.layer.trans_full_matrix_projection</code> 在计算矩阵乘法时会对参数<span class="markdown-equation" id="equation-0">$W$</span>进行转置。
<h2>训练阶段</h2>
<p>训练比较简单，直接运行<code>python train.py</code>。程序第一次运行会检测用户缓存文件夹中是否包含imikolov数据集，如果未包含，则自动下载。运行过程中，每100个iteration会打印模型训练信息，主要包含训练损失和测试损失，每个pass会保存一次模型。</p>
<h2>预测阶段</h2>
<p>在命令行运行 :
</p><div class="highlight"><pre><span></span>python infer.py <span class="se">\</span>
  --model_path <span class="s2">"models/XX"</span> <span class="se">\</span>
  --batch_size <span class="m">1</span> <span class="se">\</span>
  --use_gpu <span class="nb">false</span> <span class="se">\</span>
  --trainer_count <span class="m">1</span>
</pre></div>
参数含义如下：
- <code>model_path</code>：指定训练好的模型所在的路径。必选。
- <code>batch_size</code>：一次预测并行的样本数目。可选，默认值为 <code>1</code>。
- <code>use_gpu</code>：是否使用 GPU 进行预测。可选，默认值为 <code>False</code>。
- <code>trainer_count</code> : 预测使用的线程数目。可选，默认为 <code>1</code>。<strong>注意：预测使用的线程数目必选大于一次预测并行的样本数目</strong>。
<p>预测阶段根据多个二分类概率得到编码路径，遍历路径获取最终的预测类别，逻辑如下：</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">decode_res</span><span class="p">(</span><span class="n">infer_res</span><span class="p">,</span> <span class="n">dict_size</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Inferring probabilities are orginized as a complete binary tree.</span>
<span class="sd">    The actual labels are leaves (indices are counted from class number).</span>
<span class="sd">    This function travels paths decoded from inferring results.</span>
<span class="sd">    If the probability &gt;0.5 then go to right child, otherwise go to left child.</span>

<span class="sd">    param infer_res: inferring result</span>
<span class="sd">    param dict_size: class number</span>
<span class="sd">    return predict_lbls: actual class</span>
<span class="sd">    """</span>
    <span class="n">predict_lbls</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">infer_res</span> <span class="o">=</span> <span class="n">infer_res</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">probs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">infer_res</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">result</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">while</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">probs</span><span class="p">):</span>
            <span class="n">result</span> <span class="o">&lt;&lt;=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">probs</span><span class="p">[</span><span class="n">idx</span><span class="p">]:</span>
                <span class="n">result</span> <span class="o">|=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">probs</span><span class="p">[</span><span class="n">idx</span><span class="p">]:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span>  <span class="c1"># right child</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># left child</span>

        <span class="n">predict_lbl</span> <span class="o">=</span> <span class="n">result</span> <span class="o">-</span> <span class="n">dict_size</span>
        <span class="n">predict_lbls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predict_lbl</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">predict_lbls</span>
</pre></div>
<p>预测程序的输入数据格式与训练阶段相同，如<code>have a dream that one</code>，程序会根据<code>have a dream that</code>生成一组概率，通过对概率解码生成预测词，<code>one</code>作为真实词，方便评估。解码函数的输入是一个batch样本的预测概率以及词表的大小，里面的循环是对每条样本的输出概率进行解码，解码方式就是按照左0右1的准则，不断遍历路径，直至到达叶子节点。</p>
<h2>参考文献</h2>
<ol>
<li>Morin, F., &amp; Bengio, Y. (2005, January). <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf">Hierarchical Probabilistic Neural Network Language Model</a>. In Aistats (Vol. 5, pp. 246-252).</li>
</ol>
{% endverbatim %}