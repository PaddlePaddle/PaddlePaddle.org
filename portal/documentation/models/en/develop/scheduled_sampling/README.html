{% verbatim %}
<p>运行本目录下的程序示例需要使用PaddlePaddle v0.10.0 版本。如果您的PaddlePaddle安装版本低于此要求，请按照<a href="http://www.paddlepaddle.org/docs/develop/documentation/zh/build_and_install/pip_install_cn.html">安装文档</a>中的说明更新PaddlePaddle安装版本。</p>
<hr/>
<h1>Scheduled Sampling</h1>
<h2>概述</h2>
<p>序列生成任务的生成目标是在给定源输入的条件下，最大化目标序列的概率。训练时该模型将目标序列中的真实元素作为解码器每一步的输入，然后最大化下一个元素的概率。生成时上一步解码得到的元素被用作当前的输入，然后生成下一个元素。可见这种情况下训练阶段和生成阶段的解码器输入数据的概率分布并不一致。</p>
<p>Scheduled Sampling [<a href="#参考文献">1</a>]是一种解决训练和生成时输入数据分布不一致的方法。在训练早期该方法主要使用目标序列中的真实元素作为解码器输入，可以将模型从随机初始化的状态快速引导至一个合理的状态。随着训练的进行，该方法会逐渐更多地使用生成的元素作为解码器输入，以解决数据分布不一致的问题。</p>
<p>标准的序列到序列模型中，如果序列前面生成了错误的元素，后面的输入状态将会收到影响，而该误差会随着生成过程不断向后累积。Scheduled Sampling以一定概率将生成的元素作为解码器输入，这样即使前面生成错误，其训练目标仍然是最大化真实目标序列的概率，模型会朝着正确的方向进行训练。因此这种方式增加了模型的容错能力。</p>
<h2>算法简介</h2>
<p>Scheduled Sampling主要应用在序列到序列模型的训练阶段，而生成阶段则不需要使用。</p>
<p>训练阶段解码器在最大化第<span class="markdown-equation" id="equation-0">$t$</span>个元素概率时，标准序列到序列模型使用上一时刻的真实元素<span class="markdown-equation" id="equation-1">$y_{t-1}$</span>作为输入。设上一时刻生成的元素为<span class="markdown-equation" id="equation-2">$g_{t-1}$</span>，Scheduled Sampling算法会以一定概率使用<span class="markdown-equation" id="equation-2">$g_{t-1}$</span>作为解码器输入。</p>
<p>设当前已经训练到了第<span class="markdown-equation" id="equation-4">$i$</span>个mini-batch，Scheduled Sampling定义了一个概率<span class="markdown-equation" id="equation-5">$\epsilon_i$</span>控制解码器的输入。<span class="markdown-equation" id="equation-5">$\epsilon_i$</span>是一个随着<span class="markdown-equation" id="equation-4">$i$</span>增大而衰减的变量，常见的定义方式有：</p>
<ul>
<li>
<p>线性衰减：<span class="markdown-equation" id="equation-8">$\epsilon_i=max(\epsilon,k-c*i)$</span>，其中<span class="markdown-equation" id="equation-9">$\epsilon$</span>限制<span class="markdown-equation" id="equation-5">$\epsilon_i$</span>的最小值，<span class="markdown-equation" id="equation-11">$k$</span>和<span class="markdown-equation" id="equation-12">$c$</span>控制线性衰减的幅度。</p>
</li>
<li>
<p>指数衰减：<span class="markdown-equation" id="equation-13">$\epsilon_i=k^i$</span>，其中<span class="markdown-equation" id="equation-14">$0&lt;k&lt;1$</span>，<span class="markdown-equation" id="equation-11">$k$</span>控制着指数衰减的幅度。</p>
</li>
<li>
<p>反向Sigmoid衰减：<span class="markdown-equation" id="equation-16">$\epsilon_i=k/(k+exp(i/k))$</span>，其中<span class="markdown-equation" id="equation-17">$k&gt;1$</span>，<span class="markdown-equation" id="equation-11">$k$</span>同样控制衰减的幅度。</p>
</li>
</ul>
<p>图1给出了这三种方式的衰减曲线，</p>
<p align="center">
<img align="center" src="images/decay.jpg" width="50%"/><br/>
图1. 线性衰减、指数衰减和反向Sigmoid衰减的衰减曲线
</p>
<p>如图2所示，在解码器的<span class="markdown-equation" id="equation-0">$t$</span>时刻Scheduled Sampling以概率<span class="markdown-equation" id="equation-5">$\epsilon_i$</span>使用上一时刻的真实元素<span class="markdown-equation" id="equation-1">$y_{t-1}$</span>作为解码器输入，以概率<span class="markdown-equation" id="equation-22">$1-\epsilon_i$</span>使用上一时刻生成的元素<span class="markdown-equation" id="equation-2">$g_{t-1}$</span>作为解码器输入。从图1可知随着<span class="markdown-equation" id="equation-4">$i$</span>的增大<span class="markdown-equation" id="equation-5">$\epsilon_i$</span>会不断减小，解码器将不断倾向于使用生成的元素作为输入，训练阶段和生成阶段的数据分布将变得越来越一致。</p>
<p align="center">
<img align="center" src="images/Scheduled_Sampling.jpg" width="50%"/><br/>
图2. Scheduled Sampling选择不同元素作为解码器输入示意图
</p>
<h2>模型实现</h2>
<p>由于Scheduled Sampling是对序列到序列模型的改进，其整体实现框架与序列到序列模型较为相似。为突出本文重点，这里仅介绍与Scheduled Sampling相关的部分，完整的代码见<code>network_conf.py</code>。</p>
<p>首先导入需要的包，并定义控制衰减概率的类<code>RandomScheduleGenerator</code>，如下：</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">math</span>


<span class="k">class</span> <span class="nc">RandomScheduleGenerator</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    The random sampling rate for scheduled sampling algoithm, which uses devcayed</span>
<span class="sd">    sampling rate.</span>

<span class="sd">    """</span>
    <span class="o">...</span>
</pre></div>
<p>下面将分别定义类<code>RandomScheduleGenerator</code>的<code>__init__</code>、<code>getScheduleRate</code>和<code>processBatch</code>三个方法。</p>
<p><code>__init__</code>方法对类进行初始化，其<code>schedule_type</code>参数指定了使用哪种衰减方式，可选的方式有<code>constant</code>、<code>linear</code>、<code>exponential</code>和<code>inverse_sigmoid</code>。<code>constant</code>指对所有的mini-batch使用固定的<span class="markdown-equation" id="equation-5">$\epsilon_i$</span>，<code>linear</code>指线性衰减方式，<code>exponential</code>表示指数衰减方式，<code>inverse_sigmoid</code>表示反向Sigmoid衰减。<code>__init__</code>方法的参数<code>a</code>和<code>b</code>表示衰减方法的参数，需要在验证集上调优。<code>self.schedule_computers</code>将衰减方式映射为计算<span class="markdown-equation" id="equation-5">$\epsilon_i$</span>的函数。最后一行根据<code>schedule_type</code>将选择的衰减函数赋给<code>self.schedule_computer</code>变量。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schedule_type</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    schduled_type: is the type of the decay. It supports constant, linear,</span>
<span class="sd">    exponential, and inverse_sigmoid right now.</span>
<span class="sd">    a: parameter of the decay (MUST BE DOUBLE)</span>
<span class="sd">    b: parameter of the decay (MUST BE DOUBLE)</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">schedule_type</span> <span class="o">=</span> <span class="n">schedule_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">data_processed_</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">schedule_computers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"constant"</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="n">a</span><span class="p">,</span>
        <span class="s2">"linear"</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">d</span> <span class="o">/</span> <span class="n">b</span><span class="p">),</span>
        <span class="s2">"exponential"</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="nb">pow</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">d</span> <span class="o">/</span> <span class="n">b</span><span class="p">),</span>
        <span class="s2">"inverse_sigmoid"</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="n">b</span> <span class="o">/</span> <span class="p">(</span><span class="n">b</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">d</span> <span class="o">*</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span><span class="p">)),</span>
    <span class="p">}</span>
    <span class="k">assert</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schedule_type</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedule_computers</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">schedule_computer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedule_computers</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">schedule_type</span><span class="p">]</span>
</pre></div>
<p><code>getScheduleRate</code>根据衰减函数和已经处理的数据量计算<span class="markdown-equation" id="equation-5">$\epsilon_i$</span>。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">getScheduleRate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Get the schedule sampling rate. Usually not needed to be called by the users</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedule_computer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_processed_</span><span class="p">)</span>
</pre></div>
<p><code>processBatch</code>方法根据概率值<span class="markdown-equation" id="equation-5">$\epsilon_i$</span>进行采样，得到<code>indexes</code>，<code>indexes</code>中每个元素取值为<code>0</code>的概率为<span class="markdown-equation" id="equation-5">$\epsilon_i$</span>，取值为<code>1</code>的概率为<span class="markdown-equation" id="equation-22">$1-\epsilon_i$</span>。<code>indexes</code>决定了解码器的输入是真实元素还是生成的元素，取值为<code>0</code>表示使用真实元素，取值为<code>1</code>表示使用生成的元素。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">processBatch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Get a batch_size of sampled indexes. These indexes can be passed to a</span>
<span class="sd">    MultiplexLayer to select from the grouth truth and generated samples</span>
<span class="sd">    from the last time step.</span>
<span class="sd">    """</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getScheduleRate</span><span class="p">()</span>
    <span class="n">numbers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">indexes</span> <span class="o">=</span> <span class="p">(</span><span class="n">numbers</span> <span class="o">&gt;=</span> <span class="n">rate</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'int32'</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">data_processed_</span> <span class="o">+=</span> <span class="n">batch_size</span>
    <span class="k">return</span> <span class="n">indexes</span>
</pre></div>
<p>Scheduled Sampling需要在序列到序列模型的基础上增加一个输入<code>true_token_flag</code>，以控制解码器输入。</p>
<div class="highlight"><pre><span></span><span class="n">true_token_flags</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">'true_token_flag'</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">data_type</span><span class="o">.</span><span class="n">integer_value_sequence</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
<p>这里还需要对原始reader进行封装，增加<code>true_token_flag</code>的数据生成器。下面以线性衰减为例说明如何调用上面定义的<code>RandomScheduleGenerator</code>产生<code>true_token_flag</code>的输入数据。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gen_schedule_data</span><span class="p">(</span><span class="n">reader</span><span class="p">,</span>
                      <span class="n">schedule_type</span><span class="o">=</span><span class="s2">"linear"</span><span class="p">,</span>
                      <span class="n">decay_a</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span>
                      <span class="n">decay_b</span><span class="o">=</span><span class="mi">1000000</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Creates a data reader for scheduled sampling.</span>

<span class="sd">    Output from the iterator that created by original reader will be</span>
<span class="sd">    appended with "true_token_flag" to indicate whether to use true token.</span>

<span class="sd">    :param reader: the original reader.</span>
<span class="sd">    :type reader: callable</span>
<span class="sd">    :param schedule_type: the type of sampling rate decay.</span>
<span class="sd">    :type schedule_type: str</span>
<span class="sd">    :param decay_a: the decay parameter a.</span>
<span class="sd">    :type decay_a: float</span>
<span class="sd">    :param decay_b: the decay parameter b.</span>
<span class="sd">    :type decay_b: float</span>

<span class="sd">    :return: the new reader with the field "true_token_flag".</span>
<span class="sd">    :rtype: callable</span>
<span class="sd">    """</span>
    <span class="n">schedule_generator</span> <span class="o">=</span> <span class="n">RandomScheduleGenerator</span><span class="p">(</span><span class="n">schedule_type</span><span class="p">,</span> <span class="n">decay_a</span><span class="p">,</span> <span class="n">decay_b</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">data_reader</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">src_ids</span><span class="p">,</span> <span class="n">trg_ids</span><span class="p">,</span> <span class="n">trg_ids_next</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">src_ids</span><span class="p">,</span> <span class="n">trg_ids</span><span class="p">,</span> <span class="n">trg_ids_next</span><span class="p">,</span> \
                  <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">schedule_generator</span><span class="o">.</span><span class="n">processBatch</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trg_ids</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data_reader</span>
</pre></div>
<p>这段代码在原始输入数据（即源序列元素<code>src_ids</code>、目标序列元素<code>trg_ids</code>和目标序列下一个元素<code>trg_ids_next</code>）后追加了控制解码器输入的数据。由于解码器第一个元素是序列开始符，因此将追加的数据第一个元素设置为<code>0</code>，表示解码器第一步始终使用真实目标序列的第一个元素（即序列开始符）。</p>
<p>训练时<code>recurrent_group</code>每一步调用的解码器函数如下：</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gru_decoder_with_attention_train</span><span class="p">(</span><span class="n">enc_vec</span><span class="p">,</span> <span class="n">enc_proj</span><span class="p">,</span> <span class="n">true_word</span><span class="p">,</span>
                                       <span class="n">true_token_flag</span><span class="p">):</span>
      <span class="sd">"""</span>
<span class="sd">      The decoder step for training.</span>
<span class="sd">      :param enc_vec: the encoder vector for attention</span>
<span class="sd">      :type enc_vec: LayerOutput</span>
<span class="sd">      :param enc_proj: the encoder projection for attention</span>
<span class="sd">      :type enc_proj: LayerOutput</span>
<span class="sd">      :param true_word: the ground-truth target word</span>
<span class="sd">      :type true_word: LayerOutput</span>
<span class="sd">      :param true_token_flag: the flag of using the ground-truth target word</span>
<span class="sd">      :type true_token_flag: LayerOutput</span>
<span class="sd">      :return: the softmax output layer</span>
<span class="sd">      :rtype: LayerOutput</span>
<span class="sd">      """</span>

      <span class="n">decoder_mem</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">memory</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">'gru_decoder'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">decoder_size</span><span class="p">,</span> <span class="n">boot_layer</span><span class="o">=</span><span class="n">decoder_boot</span><span class="p">)</span>

      <span class="n">context</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">simple_attention</span><span class="p">(</span>
          <span class="n">encoded_sequence</span><span class="o">=</span><span class="n">enc_vec</span><span class="p">,</span>
          <span class="n">encoded_proj</span><span class="o">=</span><span class="n">enc_proj</span><span class="p">,</span>
          <span class="n">decoder_state</span><span class="o">=</span><span class="n">decoder_mem</span><span class="p">)</span>

      <span class="n">gru_out_memory</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">memory</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">'gru_out'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">target_dict_dim</span><span class="p">)</span>

      <span class="n">generated_word</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">max_id</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">gru_out_memory</span><span class="p">)</span>

      <span class="n">generated_word_emb</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span>
          <span class="nb">input</span><span class="o">=</span><span class="n">generated_word</span><span class="p">,</span>
          <span class="n">size</span><span class="o">=</span><span class="n">word_vector_dim</span><span class="p">,</span>
          <span class="n">param_attr</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">attr</span><span class="o">.</span><span class="n">ParamAttr</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'_target_language_embedding'</span><span class="p">))</span>

      <span class="n">current_word</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">multiplex</span><span class="p">(</span>
          <span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">true_token_flag</span><span class="p">,</span> <span class="n">true_word</span><span class="p">,</span> <span class="n">generated_word_emb</span><span class="p">])</span>

      <span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span>
          <span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">context</span><span class="p">,</span> <span class="n">current_word</span><span class="p">],</span>
          <span class="n">size</span><span class="o">=</span><span class="n">decoder_size</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span>
          <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Linear</span><span class="p">(),</span>
          <span class="n">bias_attr</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

      <span class="n">gru_step</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">gru_step</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">'gru_decoder'</span><span class="p">,</span>
          <span class="nb">input</span><span class="o">=</span><span class="n">decoder_inputs</span><span class="p">,</span>
          <span class="n">output_mem</span><span class="o">=</span><span class="n">decoder_mem</span><span class="p">,</span>
          <span class="n">size</span><span class="o">=</span><span class="n">decoder_size</span><span class="p">)</span>

      <span class="n">out</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">'gru_out'</span><span class="p">,</span>
          <span class="nb">input</span><span class="o">=</span><span class="n">gru_step</span><span class="p">,</span>
          <span class="n">size</span><span class="o">=</span><span class="n">target_dict_dim</span><span class="p">,</span>
          <span class="n">act</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Softmax</span><span class="p">())</span>
      <span class="k">return</span> <span class="n">out</span>
</pre></div>
<p>该函数使用<code>memory</code>层<code>gru_out_memory</code>记忆上一时刻生成的元素，根据<code>gru_out_memory</code>选择概率最大的词语<code>generated_word</code>作为生成的词语。<code>multiplex</code>层会在真实元素<code>true_word</code>和生成的元素<code>generated_word</code>之间做出选择，并将选择的结果作为解码器输入。<code>multiplex</code>层使用了三个输入，分别为<code>true_token_flag</code>、<code>true_word</code>和<code>generated_word_emb</code>。对于这三个输入中每个元素，若<code>true_token_flag</code>中的值为<code>0</code>，则<code>multiplex</code>层输出<code>true_word</code>中的相应元素；若<code>true_token_flag</code>中的值为<code>1</code>，则<code>multiplex</code>层输出<code>generated_word_emb</code>中的相应元素。</p>
<h2>参考文献</h2>
<p>[1] Bengio S, Vinyals O, Jaitly N, et al. <a href="http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks">Scheduled sampling for sequence prediction with recurrent neural networks</a>//Advances in Neural Information Processing Systems. 2015: 1171-1179.</p>
{% endverbatim %}