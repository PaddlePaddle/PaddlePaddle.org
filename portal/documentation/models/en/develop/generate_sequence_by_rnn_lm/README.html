{% verbatim %}
<p>运行本目录下的程序示例需要使用PaddlePaddle v0.10.0 版本。如果您的PaddlePaddle安装版本低于此要求，请按照<a href="http://www.paddlepaddle.org/docs/develop/documentation/zh/build_and_install/pip_install_cn.html">安装文档</a>中的说明更新PaddlePaddle安装版本。</p>
<hr/>
<h1>使用循环神经网语言模型生成文本</h1>
<p>语言模型(Language Model)是一个概率分布模型，简单来说，就是用来计算一个句子的概率的模型。利用它可以确定哪个词序列的可能性更大，或者给定若干个词，可以预测下一个最可能出现的词。语言模型是自然语言处理领域里一个重要的基础模型。</p>
<h2>应用场景</h2>
<p><strong>语言模型被应用在很多领域</strong>，如：</p>
<ul>
<li><strong>自动写作</strong>：语言模型可以根据上文生成下一个词，递归下去可以生成整个句子、段落、篇章。</li>
<li><strong>QA</strong>：语言模型可以根据Question生成Answer。</li>
<li><strong>机器翻译</strong>：当前主流的机器翻译模型大多基于Encoder-Decoder模式，其中Decoder就是一个待条件的语言模型，用来生成目标语言。</li>
<li><strong>拼写检查</strong>：语言模型可以计算出词序列的概率，一般在拼写错误处序列的概率会骤减，可以用来识别拼写错误并提供改正候选集。</li>
<li><strong>词性标注、句法分析、语音识别......</strong></li>
</ul>
<h2>关于本例</h2>
<p>本例实现基于RNN的语言模型，以及利用语言模型生成文本，本例的目录结构如下：</p>
<div class="highlight"><pre><span></span>.
├── data
│   └── train_data_examples.txt        # 示例数据，可参考示例数据的格式，提供自己的数据
├── config.py    # 配置文件，包括data、train、infer相关配置
├── generate.py  # 预测任务脚本，即生成文本
├── beam_search.py    # beam search 算法实现
├── network_conf.py   # 本例中涉及的各种网络结构均定义在此文件中，希望进一步修改模型结构，请修改此文件
├── reader.py    # 读取数据接口
├── README.md
├── train.py    # 训练任务脚本
└── utils.py    # 定义通用的函数，例如：构建字典、加载字典等
</pre></div>
<h2>RNN 语言模型</h2>
<h3>简介</h3>
<p>RNN是一个序列模型，基本思路是：在时刻<span class="markdown-equation" id="equation-0">$t$</span>，将前一时刻<span class="markdown-equation" id="equation-1">$t-1$</span>的隐藏层输出和<span class="markdown-equation" id="equation-0">$t$</span>时刻的词向量一起输入到隐藏层从而得到时刻<span class="markdown-equation" id="equation-0">$t$</span>的特征表示，然后用这个特征表示得到<span class="markdown-equation" id="equation-0">$t$</span>时刻的预测输出，如此在时间维上递归下去。可以看出RNN善于使用上文信息、历史知识，具有“记忆”功能。理论上RNN能实现“长依赖”（即利用很久之前的知识），但在实际应用中发现效果并不理想，研究提出了LSTM和GRU等变种，通过引入门机制对传统RNN的记忆单元进行了改进，弥补了传统RNN在学习长序列时遇到的难题。本例模型使用了LSTM或GRU，可通过配置进行修改。下图是RNN（广义上包含了LSTM、GRU等）语言模型“循环”思想的示意图：</p>
<p align="center"><img src="images/rnn.png" width="500px"/></p>
<h3>模型实现</h3>
<p>本例中RNN语言模型的实现简介如下：</p>
<ul>
<li><strong>定义模型参数</strong>：<code>config.py</code>中定义了模型的参数变量。</li>
<li><strong>定义模型结构</strong>：<code>network_conf.py</code>中的<code>rnn_lm</code><strong>函数</strong>中定义了模型的<strong>结构</strong>，如下：<ul>
<li>输入层：将输入的词（或字）序列映射成向量，即词向量层： <code>embedding</code>。</li>
<li>中间层：根据配置实现RNN层，将上一步得到的<code>embedding</code>向量序列作为输入。</li>
<li>输出层：使用<code>softmax</code>归一化计算单词的概率。</li>
<li>loss：定义多类交叉熵作为模型的损失函数。</li>
</ul>
</li>
<li>
<p><strong>训练模型</strong>：<code>train.py</code>中的<code>main</code>方法实现了模型的训练，实现流程如下：</p>
<ul>
<li>准备输入数据：建立并保存词典、构建train和test数据的reader。</li>
<li>初始化模型：包括模型的结构、参数。</li>
<li>构建训练器：demo中使用的是Adam优化算法。</li>
<li>定义回调函数：构建<code>event_handler</code>来跟踪训练过程中loss的变化，并在每轮训练结束时保存模型的参数。</li>
<li>训练：使用trainer训练模型。</li>
</ul>
</li>
<li>
<p><strong>生成文本</strong>：<code>generate.py</code> 实现了文本的生成，实现流程如下：</p>
<ul>
<li>加载训练好的模型和词典文件。</li>
<li>读取<code>gen_file</code>文件，每行是一个句子的前缀，用<a href="https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/README.cn.md#柱搜索算法">柱搜索算法(Beam Search)</a>根据前缀生成文本。</li>
<li>将生成的文本及其前缀保存到文件<code>gen_result</code>。</li>
</ul>
</li>
</ul>
<h2>使用说明</h2>
<p>运行本例的方法如下：</p>
<ul>
<li>1，运行<code>python train.py</code>命令，开始train模型（默认使用LSTM），待训练结束。</li>
<li>2，运行<code>python generate.py</code>运行文本生成。（输入的文本默认为<code>data/train_data_examples.txt</code>，生成的文本默认保存到<code>data/gen_result.txt</code>中。）</li>
</ul>
<p><strong>如果需要使用自己的语料、定制模型，需要修改<code>config.py</code>中的配置，细节和适配工作详情如下：</strong></p>
<h3>语料适配</h3>
<ul>
<li>清洗语料：去除原文中空格、tab、乱码，按需去除数字、标点符号、特殊符号等。</li>
<li>内容格式：每个句子占一行；每行中的各词之间使用一个空格符分开。</li>
<li>
<p>按需要配置<code>config.py</code>中的如下参数：</p>
<p></p><div class="highlight"><pre><span></span> <span class="n">train_file</span> <span class="o">=</span> <span class="s2">"data/train_data_examples.txt"</span>
 <span class="n">test_file</span> <span class="o">=</span> <span class="s2">""</span>

 <span class="n">vocab_file</span> <span class="o">=</span> <span class="s2">"data/word_vocab.txt"</span>
 <span class="n">model_save_dir</span> <span class="o">=</span> <span class="s2">"models"</span>
</pre></div>
1. <code>train_file</code>：指定训练数据的路径，<strong>需要预先分词</strong>。
2. <code>test_file</code>：指定测试数据的路径，如果训练数据不为空，将在每个 <code>pass</code> 训练结束对指定的测试数据进行测试。
3. <code>vocab_file</code>：指定字典的路径，如果字典文件不存在，将会对训练语料进行词频统计，构建字典。
4. <code>model_save_dir</code>：指定模型保存的路径，如果指定的文件夹不存在，将会自动创建。
</li>
</ul>
<h3>构建字典的策略</h3>
<ul>
<li>
<p>当指定的字典文件不存在时，将对训练数据进行词频统计，自动构建字典<code>config.py</code> 中有如下两个参数与构建字典有关：</p>
<p></p><div class="highlight"><pre><span></span><span class="n">max_word_num</span> <span class="o">=</span> <span class="mi">51200</span> <span class="o">-</span> <span class="mi">2</span>
<span class="n">cutoff_word_fre</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
1. <code>max_word_num</code>：指定字典中含有多少个词。
2. <code>cutoff_word_fre</code>：字典中词语在训练语料中出现的最低频率。
- 假如指定了 <code>max_word_num = 5000</code>，并且 <code>cutoff_word_fre = 10</code>，词频统计发现训练语料中出现频率高于10次的词语仅有3000个，那么最终会取3000个词构成词典。
- 构建词典时，会自动加入两个特殊符号：
1. <code>&lt;unk&gt;</code>：不出现在字典中的词
2. <code>&lt;e&gt;</code>：句子的结束符
<p><em>注：需要注意的是，词典越大生成的内容越丰富，但训练耗时越久。一般中文分词之后，语料中不同的词能有几万乃至几十万，如果<code>max_word_num</code>取值过小则导致<code>&lt;unk&gt;</code>占比过高，如果<code>max_word_num</code>取值较大，则严重影响训练速度（对精度也有影响）。所以，也有“按字”训练模型的方式，即：把每个汉字当做一个词，常用汉字也就几千个，使得字典的大小不会太大、不会丢失太多信息，但汉语中同一个字在不同词中语义相差很大，有时导致模型效果不理想。建议多试试、根据实际情况选择是“按词训练”还是“按字训练”。</em></p>
</li>
</ul>
<h3>模型适配、训练</h3>
<ul>
<li>
<p>按需调整<code>config.py</code>中如下配置，来修改 rnn 语言模型的网络结果：</p>
<p></p><div class="highlight"><pre><span></span><span class="n">rnn_type</span> <span class="o">=</span> <span class="s2">"lstm"</span>  <span class="c1"># "gru" or "lstm"</span>
<span class="n">emb_dim</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">stacked_rnn_num</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
1. <code>rnn_type</code>：支持 ”gru“ 或者 ”lstm“ 两种参数，选择使用何种 RNN 单元。
2. <code>emb_dim</code>：设置词向量的维度。
3. <code>hidden_size</code>：设置 RNN 单元隐层大小。
4. <code>stacked_rnn_num</code>：设置堆叠 RNN 单元的个数，构成一个更深的模型。
</li>
<li>
<p>运行<code>python train.py</code>命令训练模型，模型将被保存到<code>model_save_dir</code>指定的目录。</p>
</li>
</ul>
<h3>按需生成文本</h3>
<ul>
<li>
<p>按需调整<code>config.py</code>中以下变量，详解如下：</p>
<p></p><div class="highlight"><pre><span></span><span class="n">gen_file</span> <span class="o">=</span> <span class="s2">"data/train_data_examples.txt"</span>
<span class="n">gen_result</span> <span class="o">=</span> <span class="s2">"data/gen_result.txt"</span>
<span class="n">max_gen_len</span> <span class="o">=</span> <span class="mi">25</span>  <span class="c1"># the max number of words to generate</span>
<span class="n">beam_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s2">"models/rnn_lm_pass_00000.tar.gz"</span>
</pre></div>
1. <code>gen_file</code>：指定输入数据文件，每行是一个句子的前缀，<strong>需要预先分词</strong>。
2. <code>gen_result</code>：指定输出文件路径，生成结果将写入此文件。
3. <code>max_gen_len</code>：指定每一句生成的话最长长度，如果模型无法生成出<code>&lt;e&gt;</code>，当生成 <code>max_gen_len</code> 个词语后，生成过程会自动终止。
4. <code>beam_size</code>：Beam Search 算法每一步的展开宽度。
5. <code>model_path</code>：指定训练好的模型的路径。
<p>其中，<code>gen_file</code> 中保存的是待生成的文本前缀，每个前缀占一行，形如：</p>
<p></p><div class="highlight"><pre><span></span>若隐若现 地像 幽灵 , 像 死神
</pre></div>
将需要生成的文本前缀按此格式存入文件即可；
</li>
<li>
<p>运行<code>python generate.py</code>命令运行beam search 算法为输入前缀生成文本，下面是模型生成的结果：</p>
<p></p><div class="highlight"><pre><span></span>81    若隐若现 地像 幽灵 , 像 死神
-12.2542    一样 。 他 是 个 怪物 &lt;e&gt;
-12.6889    一样 。 他 是 个 英雄 &lt;e&gt;
-13.9877    一样 。 他 是 我 的 敌人 &lt;e&gt;
-14.2741    一样 。 他 是 我 的 &lt;e&gt;
-14.6250    一样 。 他 是 我 的 朋友 &lt;e&gt;
</pre></div>
其中：
1. 第一行 <code>81    若隐若现 地像 幽灵 , 像 死神</code>以<code>\t</code>为分隔，共有两列：
    - 第一列是输入前缀在训练样本集中的序号。
    - 第二列是输入的前缀。
2. 第二 ~ <code>beam_size + 1</code> 行是生成结果，同样以 <code>\t</code> 分隔为两列：
    - 第一列是该生成序列的对数概率（log probability）。
    - 第二列是生成的文本序列，正常的生成结果会以符号<code>&lt;e&gt;</code>结尾，如果没有以<code>&lt;e&gt;</code>结尾，意味着超过了最大序列长度，生成强制终止。
</li>
</ul>
{% endverbatim %}