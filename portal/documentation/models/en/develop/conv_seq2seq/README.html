{% verbatim %}
<p>The minimum PaddlePaddle version needed for the code sample in this directory is v0.11.0. If you are on a version of PaddlePaddle earlier than v0.11.0, <a href="http://www.paddlepaddle.org/docs/develop/documentation/en/build_and_install/pip_install_en.html">please update your installation</a>.</p>
<hr/>
<h1>Convolutional Sequence to Sequence Learning</h1>
<p>This model implements the work in the following paper:</p>
<p>Jonas Gehring, Micheal Auli, David Grangier, et al. Convolutional Sequence to Sequence Learning. Association for Computational Linguistics (ACL), 2017</p>
<h1>Data Preparation</h1>
<ul>
<li>
<p>The data used in this tutorial can be downloaded by runing:</p>
<div class="highlight"><pre><span></span>sh download.sh
</pre></div>
</li>
<li>
<p>Each line in the data file contains one sample and each sample consists of a source sentence and a target sentence. And the two sentences are seperated by 't'. So, to use your own data, it should be organized as follows:</p>
<div class="highlight"><pre><span></span>&lt;source sentence&gt;\t&lt;target sentence&gt;
</pre></div>
</li>
</ul>
<h1>Training a Model</h1>
<ul>
<li>Modify the following script if needed and then run:</li>
</ul>
<div class="highlight"><pre><span></span>python train.py <span class="se">\</span>
    --train_data_path ./data/train <span class="se">\</span>
    --test_data_path ./data/test <span class="se">\</span>
    --src_dict_path ./data/src_dict <span class="se">\</span>
    --trg_dict_path ./data/trg_dict <span class="se">\</span>
    --enc_blocks <span class="s2">"[(256, 3)] * 5"</span> <span class="se">\</span>
    --dec_blocks <span class="s2">"[(256, 3)] * 3"</span> <span class="se">\</span>
    --emb_size <span class="m">256</span> <span class="se">\</span>
    --pos_size <span class="m">200</span> <span class="se">\</span>
    --drop_rate <span class="m">0</span>.2 <span class="se">\</span>
    --use_bn False <span class="se">\</span>
    --use_gpu False <span class="se">\</span>
    --trainer_count <span class="m">1</span> <span class="se">\</span>
    --batch_size <span class="m">32</span> <span class="se">\</span>
    --num_passes <span class="m">20</span> <span class="se">\</span>
    &gt;train.log <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>
</pre></div>
<h1>Inferring by a Trained Model</h1>
<ul>
<li>Infer by a trained model by running:</li>
</ul>
<p><code>bash
  python infer.py \
      --infer_data_path ./data/dev \
      --src_dict_path ./data/src_dict \
      --trg_dict_path ./data/trg_dict \
      --enc_blocks "[(256, 3)] * 5" \
      --dec_blocks "[(256, 3)] * 3" \
      --emb_size 256 \
      --pos_size 200 \
      --drop_rate 0.2 \
      --use_bn False \
      --use_gpu False \
      --trainer_count 1 \
      --max_len 100 \
      --batch_size 256 \
      --beam_size 1 \
      --is_show_attention False \
      --model_path ./params.pass-0.tar.gz \
      1&gt;infer_result 2&gt;infer.log</code></p>
<h1>Notes</h1>
<p>Since PaddlePaddle of current version doesn't support weight normalization, we use batch normalization instead to confirm convergence when the network is deep.</p>
{% endverbatim %}