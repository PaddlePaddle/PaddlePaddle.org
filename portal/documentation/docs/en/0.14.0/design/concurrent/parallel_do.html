{% verbatim %}
<h1>Design Doc: Parallel_Do in PaddlePaddle</h1>
<p>In PaddlePaddle, we use parallel_do primitive to represent multithread data parallel processing.</p>
<h2>Design overview</h2>
<p>The definition of a parallel_do op looks like the following</p>
<div class="highlight"><pre><span></span><span class="n">AddInput</span><span class="p">(</span><span class="n">kInputs</span><span class="p">,</span> <span class="s">&quot;Inputs needed to be split onto different devices&quot;</span><span class="p">).</span><span class="n">AsDuplicable</span><span class="p">();</span>
<span class="n">AddInput</span><span class="p">(</span><span class="n">kParameters</span><span class="p">,</span> <span class="s">&quot;Parameters are duplicated over different devices&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">AsDuplicable</span><span class="p">();</span>
<span class="n">AddInput</span><span class="p">(</span><span class="n">kPlaces</span><span class="p">,</span> <span class="s">&quot;Devices used for parallel processing&quot;</span><span class="p">);</span>
<span class="n">AddOutput</span><span class="p">(</span><span class="n">kOutputs</span><span class="p">,</span> <span class="s">&quot;Outputs needed to be merged from different devices&quot;</span><span class="p">).</span><span class="n">AsDuplicable</span><span class="p">();</span>
<span class="n">AddOutput</span><span class="p">(</span><span class="n">kParallelScopes</span><span class="p">,</span>
          <span class="s">&quot;Scopes for all local variables in forward pass. One scope for each device&quot;</span><span class="p">);</span>
<span class="n">AddAttr</span><span class="o">&lt;</span><span class="n">framework</span><span class="o">::</span><span class="n">BlockDesc</span> <span class="o">*&gt;</span><span class="p">(</span><span class="n">kParallelBlock</span><span class="p">,</span>
                                <span class="s">&quot;List of operaters to be executed in parallel&quot;</span><span class="p">);</span>
</pre></div>

<p>A vanilla implementation of parallel_do can be shown as the following (<code>|</code> means single thread and
<code>||||</code> means multiple threads)</p>
<div class="highlight"><pre><span></span>In the forward pass
  |      Split input onto different devices
  |      Copy parameter onto different devices
  ||||   Compute forward pass in parallel
  |      Merge output from different devices

In the backward pass
  |      Split output@grad onto different devices
  ||||   Compute backward pass in parallel
  |      accumulate param@grad from different devices to the first device
  |      Merge input@grad from different devices
  |      Copy param@grad to the place of parallel_do_op
</pre></div>

<p>This implementation allows to write mixed device program like this</p>
<div class="highlight"><pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span> <span class="n">parameter</span><span class="o">=</span><span class="n">true</span><span class="p">)</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span><span class="mi">15</span><span class="p">],</span> <span class="n">parameter</span><span class="o">=</span><span class="n">true</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">data</span><span class="p">()</span>

<span class="n">gpu_places</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">get_place</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># parallel processing on multiple GPUs</span>
<span class="n">pd</span> <span class="o">=</span> <span class="n">ParallelDo</span><span class="p">(</span><span class="n">gpu_places</span><span class="p">)</span>
<span class="k">with</span> <span class="n">pd</span><span class="o">.</span><span class="n">do</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">data</span><span class="p">):</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">fc</span><span class="p">(</span><span class="n">fc</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">W1</span><span class="p">),</span> <span class="n">W2</span><span class="p">))</span>
    <span class="n">write_output</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">pd</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>

<p>And the programDesc are like the following</p>
<div class="highlight"><pre><span></span># start_program will be run by executor(CPUPlace), all w1, w2 will be allocated on CPU
start_program
{
  vars: w1, w2
  ops: init(w1), init(w2)
}

main_program
{
block0 {
  vars: data, places, w1, w2, w1_grad, w2_grad,
  ops: data, get_place, parallel_do(block1),
       parallel_do_grad(block2),
       sgd(w2, w2_grad),
       sgd(w1, w1_grad)
}
block1 { # the forward pass
  parent_block: 0
  vars: data, h1, h2, loss
  ops: fc, fc, softmax
}
block2 { # the backward pass
  parent_block: 1
  vars: data_grad, h1_grad, h2_grad, loss_gard, local_w1_grad, local_w2_grad
  ops: softmax_grad,
       fc_grad
       fc_grad
}
}
</pre></div>

<h2>Performance Imporvement</h2>
<p>There are serial places we can make this parallel_do faster.</p>
<h3>forward: split input onto different devices</h3>
<p>If the input of the parallel_do is independent from any prior opeartors, we can avoid this step by 
prefetching the input onto different devices in a seperate background thread. And the python code
looks like this.
<div class="highlight"><pre><span></span><span class="n">pd</span> <span class="o">=</span> <span class="n">ParallelDo</span><span class="p">(</span><span class="n">gpu_places</span><span class="p">)</span>
<span class="k">with</span> <span class="n">pd</span><span class="o">.</span><span class="n">do</span><span class="p">():</span>
 <span class="err"> </span> <span class="err"> </span><span class="n">feature</span> <span class="o">=</span> <span class="n">get_data_from_prefetch_queue</span><span class="p">(</span><span class="n">gpu_places</span><span class="p">)</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">my_net</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
    <span class="n">write_output</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
</pre></div></p>
<h3>forward: Copy parameter to onto different devices</h3>
<p>We can avoid this step by making each device have a copy of the parameter. This requires:</p>
<ol>
<li><code>fluid.default_start_up_program()</code> to be run on all devices</li>
<li>In the backward, allreduce param@grad at different devices, this requires<ol>
<li><code>backward.py</code> add <code>allreduce</code> operators at parallel_do_grad</li>
<li><code>allreduce</code> operators need to be called in async mode to achieve maximum throughput</li>
</ol>
</li>
<li>apply gradients related op(i.e. cliping, normalization, decay, sgd) on different devices in parallel</li>
</ol>
<p>By doing so, we also avoided "backward: accumulate param@grad from different devices to the first device".
And the ProgramDesc looks like the following</p>
<div class="highlight"><pre><span></span># w1, w2 will be allocated on all GPUs
start_program
{
block0 {
  parallel_do(block1)
}
block1 {
  parent_block: 0
  vars: w1, w2
  ops: init(w1), init(w2)
}
}

main_program
{
block0 {
  vars: data, places, w1, w2
  ops: data, get_place, parallel_do(block1),
       parallel_do_grad(block2),      # append_backward
       parallel_do(block3)            # append_optimization

}
block1 {
  parent_block: 0
  vars: data, h1, h2, loss
  ops: fc, fc, softmax
}
block2 {
  parent_block: 1
  vars: data_grad, h1_grad, h2_grad, loss_gard, w1_grad, w2_grad
  ops: softmax_grad,
       fc_grad, allreduce(places, scopes, w1_grad),
       fc_grad, allreduce(places, scopes, w2_grad)
}
block3 {
  parent_block: 0
  vars: lr
  ops: sgd(w2, w2_grad),
       sgd(w1, w1_grad)
}
}
</pre></div>
{% endverbatim %}