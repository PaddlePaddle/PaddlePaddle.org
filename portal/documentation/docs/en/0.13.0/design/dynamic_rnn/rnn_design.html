{% verbatim %}
<h1>RNN 变长输入设计</h1>
<p>对变长序列的学习，现有主流框架比如 tensorflow, pytorch, caffe2, mxnet 等均使用了padding的方式，
即将一个mini-batch内不同长度的序列补0到固定长度参与计算。</p>
<p>现有Paddle包括 <code>RecurrentLayerGroup</code> 在内的RNN均实现了无padding的变长序列支持，本文也将基于该模块的思路，设计重构后的变长序列支持。</p>
<h2>背景介绍</h2>
<p>由于tensor必须有明确的shape，因此基于tensor 的主流框架在存储变长序列时，
必须用zero-padding的方式将变长序列补全为固定shape的tensor。</p>
<p>由于padding是一种框架实现变长序列的妥协， 从用户角度，在使用RNN类模型时自然会比较介意padding的存在，
因此会有pytorch中对非padding方式变长序列支持长篇的讨论[3]。</p>
<p>由于padding对内存和计算会有额外的消耗，tensorflow和mxnet均使用了bucketing来进行优化[1][2]，
但不管是padding还是bucket，对于用户都是额外的使用负担。</p>
<p>因此，<strong>paddle原生支持变长序列的方式，能直接满足用户对变长序列的最直接的需求，在当前主流平台中可以算是一大优势</strong>。</p>
<p>但对变长序列的支持，需要对目前框架做一些修改，下面讨论如何在最小修改下支持变长序列。</p>
<h2>多层序列数据格式 <code>LODTensor</code></h2>
<p>目前 Paddle 会将一个mini-batch内的数据存储在一维的内存上，
额外使用 <code>Argument.sequenceStartPositions</code> 来存储每个句子的信息。</p>
<p>Paddle里使用 <code>Argument.subSequenceStartPositions</code> 来存储2层的序列信息，更高维度的序列则无法直接支持；</p>
<p>为了支持 <code>N-level</code> 序列的存储，本文将序列信息定义成如下数据结构:</p>
<div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;&gt;</span> <span class="n">lod_start_pos_</span><span class="p">;</span>
</pre></div>

<p>或者更明确的定义</p>
<div class="highlight"><pre><span></span><span class="k">typedef</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">level_t</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">level_t</span><span class="o">&gt;</span> <span class="n">lod_start_pos</span><span class="p">;</span>
</pre></div>

<p>这里的每一个 <code>level_t</code> 存储一个粒度(level)的偏移信息，和paddle目前做法一致。</p>
<p>为了更透明地传递序列信息，我们引入了一种新的tensor 称为 <code>LODTensor</code>[4]，
其关于tensor相关的接口都直接继承自 <code>Tensor</code>，但另外添加了序列相关接口。
如此，在操作一个 <code>LODTensor</code> 时，普通 <code>Op</code> 直接当成 <code>Tensor</code> 使用，
而操作序列的 <code>Op</code> 会额外操作 <code>LODTensor</code> 的变长序列操作的相关接口。</p>
<p><code>LODTensor</code> 具体定义如下：</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LODTensor</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Tensor</span> <span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
  <span class="kt">size_t</span> <span class="n">Levels</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="n">seq_start_positions_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="p">}</span>
  <span class="kt">size_t</span> <span class="n">Elements</span><span class="p">(</span><span class="kt">int</span> <span class="n">level</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">seq_start_positions_</span><span class="p">[</span><span class="n">level</span><span class="p">].</span><span class="n">size</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="c1">// slice of level[elem_begin: elem_end]</span>
  <span class="c1">// NOTE low performance in slice seq_start_positions_.</span>
  <span class="c1">// TODO should call Tensor&#39;s Slice.</span>
  <span class="n">LODTensor</span> <span class="n">LODSlice</span><span class="p">(</span><span class="kt">int</span> <span class="n">level</span><span class="p">,</span> <span class="kt">int</span> <span class="n">elem_begin</span><span class="p">,</span> <span class="kt">int</span> <span class="n">elem_end</span><span class="p">)</span> <span class="k">const</span><span class="p">;</span>

  <span class="c1">// slice with tensor&#39;s data shared with this.</span>
  <span class="n">LODTensor</span> <span class="nf">LODSliceShared</span><span class="p">(</span><span class="kt">int</span> <span class="n">level</span><span class="p">,</span> <span class="kt">int</span> <span class="n">elem_begin</span><span class="p">,</span> <span class="kt">int</span> <span class="n">elem_end</span><span class="p">)</span> <span class="k">const</span><span class="p">;</span>

  <span class="c1">// copy other&#39;s lod_start_pos_, to share LOD info.</span>
  <span class="c1">// NOTE the LOD info sould not be changed.</span>
  <span class="kt">void</span> <span class="nf">ShareConstLODFrom</span><span class="p">(</span><span class="k">const</span> <span class="n">LODTensor</span> <span class="o">&amp;</span><span class="n">other</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">lod_start_pos_</span> <span class="o">=</span> <span class="n">other</span><span class="p">.</span><span class="n">lod_start_pos_</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="c1">// copy other&#39;s lod_start_pos_&#39;s content, free to mutate.</span>
  <span class="kt">void</span> <span class="nf">ShareMutableLODFrom</span><span class="p">(</span><span class="k">const</span> <span class="n">LODTensor</span> <span class="o">&amp;</span><span class="n">other</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">lod_start_pos_</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span> <span class="o">&lt;</span>
                     <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">other</span><span class="p">.</span><span class="n">lod_start_pos_</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span>
                                                   <span class="n">other</span><span class="p">.</span><span class="n">lod_start_pos_</span><span class="p">.</span><span class="n">end</span><span class="p">());</span>
  <span class="p">}</span>

<span class="k">private</span><span class="o">:</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;&gt;</span> <span class="n">lod_start_pos_</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>

<p>其中， <code>lod_start_pos_</code> 使用了 <code>shared_ptr</code> 来减少存储和复制的代价，
可以认为 <code>LODTensor</code> 是 <code>Tensor</code> 的扩展，几乎完全兼容原始 <code>Tensor</code> 的使用。</p>
<h2>框架支持</h2>
<h3>框架现有的 <code>Tensor</code> 调用替换为 <code>LODTensor</code></h3>
<p>为了实现 <code>LODTensor</code> 的传递，框架里很多 <code>Tensor</code> 都需要变成 <code>LODTensor</code>，
简单实现，直接 <strong>把之前所有的<code>Tensor</code> 全部替换成 <code>LODTensor</code>，这里可以直接修改 <code>pybind.cc</code> 里面创建<code>Tensor</code>的接口</strong>。</p>
<p>此外，用户有可能需要感知序列的存在（比如序列的可视化需要解析模型中输出的序列），因此一些序列操作的API也需要暴露到 python 层。</p>
<h3><code>lod_start_pos</code> 随着Op调用链传递</h3>
<p>框架需要支持下列特性，以实现<code>lod_start_pos</code>的传递：</p>
<ol>
<li>
<p>以 <code>shared_ptr</code> 的方式实现传递</p>
<ul>
<li>不修改 <code>lod_start_pos</code> 内容的作为 consumer</li>
<li>修改 <code>lod_start_pos</code> 的作为 producer</li>
<li>约定 consumer 只需要复制传递过来的 <code>shared_ptr</code></li>
<li>producer 需要创建自己的独立的内存，以存储自己独立的修改，并暴露 <code>shared_ptr</code> 给后续 consumer</li>
<li>由于传递过程是以复制<code>shared_ptr</code>的方式实现，因此框架只需要传递一次 <code>lod_start_pos</code></li>
</ul>
</li>
<li>
<p>对于不感知 <code>lod_start_pos</code> 的Op足够透明</p>
</li>
<li>需要修改 <code>lod_start_pos</code> 的producer Op可以在 <code>Run</code> 时更新自己的 <code>lod_start_pos</code> 数据</li>
</ol>
<p>具体的设计分为以下3小节</p>
<h4><code>load_start_pos</code> 的传递</h4>
<ul>
<li>对于不需要修改 <code>lod_start_pos</code> 的情况，调用 LODTensor的 <code>ShareConstLODFrom</code> 接口实现复制</li>
<li>需要修改的，调用<code>ShareMutableLODFrom</code> 接口自己分配内存以存储修改</li>
</ul>
<h4>框架透明</h4>
<p>传递这一步需要加入到网络跑之前的初始化操作中，并且只需要初始化一次，基于当前框架设计的初步方案如下</p>
<ul>
<li>在 Op 的 <code>attrs</code> 中添加一项 <code>do_mutate_lod_info</code> 的属性，默认为 <code>false</code></li>
<li>有需要修改 <code>lod_start_pos</code> 的Op需要在定义 <code>OpProto</code> 时设置为 <code>true</code></li>
<li><code>OperatorBase</code> 的 <code>InferShape</code> 中会读取 <code>do_mutate_lod_info</code> ，并且调用 <code>LODTensor</code> 相关的方法实现 <code>lod_start_pos</code> 的复制。</li>
<li><code>OperatorBase</code> 中添加一个 member <code>is_lod_inited{false}</code> 来保证传递只进行一次</li>
</ul>
<p>一些逻辑如下</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">OperatorBase</span> <span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
  <span class="c1">// ...</span>
  <span class="kt">void</span> <span class="n">InferShape</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">is_load_inited</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">bool</span> <span class="n">do_mutate_lod_info</span> <span class="o">=</span> <span class="n">GetAttr</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;do_mutate_load_info&quot;</span><span class="p">);</span>
      <span class="c1">// find a input having LOD to copy</span>
      <span class="k">auto</span> <span class="n">lod_input</span> <span class="o">=</span> <span class="n">ValidLODInput</span><span class="p">();</span>
      <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">output</span> <span class="p">:</span> <span class="n">outputs</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">do_mutate_load_info</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">output</span><span class="p">.</span><span class="n">ShareMutableLODFrom</span><span class="p">(</span><span class="n">lod_input</span><span class="p">);</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
          <span class="n">output</span><span class="p">.</span><span class="n">ShareConstLODFrom</span><span class="p">(</span><span class="n">load_input</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>
      <span class="n">is_pod_inited</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// call op&#39;s InferShape</span>
    <span class="c1">// ...</span>
  <span class="p">}</span>

<span class="k">private</span><span class="o">:</span>
  <span class="c1">// ...</span>
  <span class="kt">bool</span> <span class="n">is_lod_inited</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>
<span class="p">};</span>
</pre></div>

<p>如此，<code>lod_start_pos</code> 的信息的传递对非OLD的Op的实现是完全透明的。</p>
<h4><code>lod_start_pos</code> 的更新</h4>
<p>上一小节介绍到，对于需要修改 <code>load_start_pos</code> 的Op，<code>OperatorBase</code> 会分配一块自己的内存以存储修改，
Op在 <code>Run</code> 的实现中，操作更新自己的 <code>load_start_pos</code> ，
而所有依赖其 outputs 的 op 会通过共享的指针自动获取到其更新。</p>
<h2>根据长度排序</h2>
<p>按照长度排序后，从前往后的时间步的batch size会自然地递减，可以直接塞入 Net 做batch计算</p>
<p>比如原始的输入：</p>
<div class="highlight"><pre><span></span>origin:
xxxx
xx
xxx

-&gt; sorted:
xxxx
xxx
xx
</pre></div>

<p>经过 <code>SegmentInputs</code> 之后，每个会有4个时间步，每个时间步的输入如下（纵向排列）</p>
<div class="highlight"><pre><span></span>0    1    2    3
x    x    x    x
x    x    x
x    x
</pre></div>

<p>为了追踪排序前后序列的变化，这里用
<div class="highlight"><pre><span></span><span class="k">struct</span> <span class="n">SortedSeqItem</span> <span class="p">{</span>
   <span class="kt">void</span> <span class="o">*</span><span class="n">start</span><span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>
   <span class="kt">void</span> <span class="o">*</span><span class="n">end</span><span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>
<span class="p">};</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">SortedSeqItem</span><span class="o">&gt;</span> <span class="n">sorted_seqs</span><span class="p">;</span>
</pre></div>
来追踪序列排序后的位置，并添加一个新的接口</p>
<div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">SortedSeqItem</span><span class="o">&gt;</span> <span class="n">SortBySeqLen</span><span class="p">(</span><span class="k">const</span> <span class="n">LODTensor</span><span class="o">&amp;</span> <span class="n">tensor</span><span class="p">);</span>
</pre></div>

<p>由于输入序列的顺序变化，以下现有的接口需要针对性地修改：</p>
<ul>
<li>InitMemories, memory需要根据 <code>sorted_seqs</code> 重新排列</li>
<li>SetmentInputs</li>
<li>ConcatOutputs</li>
</ul>
<p>此外，由于 <code>sorted_seqs</code> 需要被 <code>RecurrentGradientOp</code> 复用，因此会变成 <code>RecurrentOp</code> 一个新的output输出，
之后作为 <code>RecurrentGradientOp</code> 的一个输入传入。</p>
<h2>InitMemories</h2>
<p>由于序列顺序的变化，<code>boot_memories</code> 的batch上的element的顺序也需要对应重新排列。</p>
<h2>SegmentInputs</h2>
<p><code>SegmentInputs</code> 会依赖 <code>sorted_seqs</code> 的信息，将原始的序列按照排序后的序列顺序，从横向切割，转为每个step中的inputs。</p>
<p>即下面的转变：
<div class="highlight"><pre><span></span>origin:
xxxx
xx
xxx

   |
   |
  \ /
   !
0    1    2    3
x    x    x    x
x    x    x
x    x
</pre></div></p>
<h2>ConcatOutputs</h2>
<p><code>ConcatOutputs</code> 需要</p>
<ul>
<li>将每个时间步的输出重新还原为原始输入的序列顺序（以防止Infer阶段顺序打乱）</li>
<li>将每个序列concat 为规则的mini-batch表示</li>
</ul>
<h2>参考文献</h2>
<p><a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.training/bucketing">Tensorflow Bucketing</a></p>
<p><a href="http://mxnet.io/how_to/bucketing.html">mxnet Bucketing</a></p>
<p><a href="https://discuss.pytorch.org/t/about-the-variable-length-input-in-rnn-scenario/345/5">variable length input in RNN scenario</a></p>
<p><a href="https://en.wikipedia.org/wiki/Level_of_detail">Level of details</a></p>
{% endverbatim %}