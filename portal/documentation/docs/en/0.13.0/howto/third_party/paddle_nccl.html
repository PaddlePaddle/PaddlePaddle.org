<html><body><p>{% verbatim %}
</p><h1>Design Doc: NCCL support in Paddle Fluid</h1>
<h2>Abstract</h2>
<p>This Design Doc refers to the NCCL feature in  paddle.  We propose an approach to support NCCL library both on a single machine and multiple machines. We wrapper the NCCL primitives <code>Broadcast</code>, <code>Allreduce</code>, <code>Reduce</code> as operators to utilize Multi-GPU powers in one script.</p>
<h2>Motivation</h2>
<p><a href="https://developer.nvidia.com/nccl">NCCL</a> is a NVIDIA library support Multi-GPU communicating and optimized for NVIDIA GPUs, it provides routines such as all-gather, all-reduce, broadcast, reduce, reduce-scatter, that can achieve high bandwidth over PCIe and NVLink high-speed interconnect. With NCCL library, we can easily accelerate the training in parallel. </p>
<ul>
<li>Pros</li>
<li>easily plug-in with <a href="https://developer.nvidia.com/nccl">NCCL2</a> library.</li>
<li>high performance in NVIDIA GPUs.</li>
<li>
<p>MPI like primitives, which have low learning cost for users.</p>
</li>
<li>
<p>Cons</p>
</li>
<li>Only design for NVIDIA GPUs, not a general multi-device solution.</li>
<li>Although NCCL1 is opensourced under BSD license, but NCCL2 is not opensourced anymore.</li>
</ul>
<p>At the beginning of training, the framework needs to distribute the same parameters to every GPU, and merge the gradients at any time user interests.</p>
<p>As a result, during training, we need the operations of peer to peer copy between different GPUs, aggregating gradients/parameters from GPUs, and broadcasting parameters to GPUs. Every GPU only need to run the operator with correct place information.</p>
<p>Besides, it needs interfaces to synchronize model update with each different GPU Cards. </p>
<h2>Implementation</h2>
<p>As mentioned above, we wrap the NCCL routines as several kinds of operators. Need to note that NCCL need to create Communicator between gpu at the beginning, so there is a NCCLInit operator created.</p>
<h3>Transpiler</h3>
<p>To be compatible with <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/design/ops/dist_train.md">parameter server design doc</a>, the transpiler compiles the user defined operation graph into sub-graphs to be executed on different devices.</p>
<ol>
<li>
<p>The user-defined model will be a single device program</p>
</li>
<li>
<p>Broadcast/Reduce operators between GPUs will be inserted into the program, even for the multi-node, may insert the <code>Send</code>, <code>Recv</code> operator.</p>
</li>
</ol>
<p><em>Broadcast, AllReduce in a single machine. And Broadcast, AllReduce, <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/design/ops/dist_train.md#graph-converter">Send, Recv</a> in multiple machines</em></p>
<p><img src="../../../../_images/multigpu_before_convert.png" width="300"/></p>
<p>After compiling, the graph as shows</p>
<p><img src="../../../../_images/multigpu_allreduce.png" width="1000"/></p>
<p>Operators are added to the sub-graphs. Every GPU assigned a role of <code>rank0</code>, <code>rank1</code> etc. </p>
<ul>
<li><strong>Broadcast</strong>. Broadcast operator distribute initialized parameter to all the GPUs from the GPU who owns it. e.g. from<code>rank0</code> GPU.</li>
<li><strong>AllReduce</strong>. AllReduce operator synchronizes parameters/gradients between GPUs. AllReduce implemented in the Ring-Based  communicating method, avoid of the bottle neck in a single GPU.</li>
</ul>
<p>Need to notice that AllReduce operator force GPUs synchronized at that point. The whole training process in asynchronous or synchronous mode depends on the AllReduce point in the graph.</p>
<p>As it shown in the picture, when each GPU compute the gradient of <code>W</code>, followed with a <code>AllReduce</code> operator, accumulate the <code>dW</code> to full batch of data, then run the optimize process individually and apply the gradient to its <code>W</code>.</p>
<ul>
<li><strong>AllReduce</strong>
  Need to note that our AllReduce operator is a ring-base AllReduce implementation. If we use the NCCL2 AllReduce primitive, every GPU optimized full batch of data, wasted (n-1) GPU compute resources. In addition, NCCL2 built-in AllReduce will only utilize the communicating resource during synchronization, then update the gradient will be a subsequent phase. In fact, we can amortize the update gradient time cost into the communicating phase. The process is</li>
<li>Every parameter has its root card. That card will responsible for aggregating the gradients from GPUs.</li>
<li>The whole model's parameter will be hashed to different root card, ensure the load balance between GPUs.</li>
<li>Logically neighberhood card will start send parameter to the next one. After one round, the parameter main card will aggregate the full gradients.</li>
<li>Then the root card will optimize the parameter.</li>
<li>This parameter card will send its optimized result to its neighberhood, then the neighberhood will send parameter to its next one.</li>
<li>Finish the sychronization round.</li>
</ul>
<p>The total time cost will be 2 * (n-1) * per-parameter-send-time, we reach the goal of amortize the upgrade time into communicating phase.</p>
{% endverbatim %}</body></html>