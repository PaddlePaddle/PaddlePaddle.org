{% verbatim %}
<h1>Fluid 分布式版本使用指南</h1>
<p>本篇文章将说明如何在PaddlePaddle Fluid版本下进行分布式训练的配置和执行，以及将单机训练脚本改造成支持集群训练的版本</p>
<h2>准备工作</h2>
<ul>
<li>
<p>可用的集群</p>
<p>包含一个或多个计算节点的集群，每一个节点都能够执行PaddlePaddle的训练任务且拥有唯一的IP地址，集群内的所有计算节点可以通过网络相互通信。
* 安装PaddlePaddle Fluid with Distribution版本</p>
<p>所有的计算节点上均需要按照分布式版本的PaddlePaddle, 在用于GPU等设备的机器上还需要额外安装好相应的驱动程序和CUDA的库。</p>
<p><strong>注意：</strong>当前对外提供的PaddlePaddle版本并不支持分布式，需要通过源码重新编译。编译和安装方法参见<a href="http://www.paddlepaddle.org/docs/develop/documentation/en/getstarted/build_and_install/index_en.html">编译和安装指南</a>。
cmake编译命令中需要将WITH_DISTRIBUTE设置为ON，下面是一个cmake编译指令示例：
<div class="highlight"><pre><span></span>cmake .. -DWITH_DOC<span class="o">=</span>OFF -DWITH_GPU<span class="o">=</span>OFF -DWITH_DISTRIBUTE<span class="o">=</span>ON -DWITH_SWIG_PY<span class="o">=</span>ON -DWITH_PYTHON<span class="o">=</span>ON
</pre></div></p>
</li>
</ul>
<h2>更新训练脚本</h2>
<p>这里，我们以<a href="http://www.paddlepaddle.org/docs/develop/book/01.fit_a_line/index.html">Deep Learing 101</a>课程中的第一章 fit a line 为例，描述如何将单机训练脚本改造成支持集群训练的版本。</p>
<h3>单机训练脚本示例</h3>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">paddle.v2</span> <span class="kn">as</span> <span class="nn">paddle</span>
<span class="kn">import</span> <span class="nn">paddle.fluid</span> <span class="kn">as</span> <span class="nn">fluid</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">13</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">y_predict</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

<span class="n">cost</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">square_error_cost</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">y_predict</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">avg_cost</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">cost</span><span class="p">)</span>

<span class="n">sgd_optimizer</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">sgd_optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">avg_cost</span><span class="p">)</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">train_reader</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span>
    <span class="n">paddle</span><span class="o">.</span><span class="n">reader</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span>
        <span class="n">paddle</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">uci_housing</span><span class="o">.</span><span class="n">train</span><span class="p">(),</span> <span class="n">buf_size</span><span class="o">=</span><span class="mi">500</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>

<span class="n">place</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">CPUPlace</span><span class="p">()</span>
<span class="n">feeder</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">DataFeeder</span><span class="p">(</span><span class="n">place</span><span class="o">=</span><span class="n">place</span><span class="p">,</span> <span class="n">feed_list</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span>
<span class="n">exe</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">Executor</span><span class="p">(</span><span class="n">place</span><span class="p">)</span>

<span class="n">exe</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">fluid</span><span class="o">.</span><span class="n">default_startup_program</span><span class="p">())</span>

<span class="n">PASS_NUM</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">pass_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">PASS_NUM</span><span class="p">):</span>
    <span class="n">fluid</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">save_persistables</span><span class="p">(</span><span class="n">exe</span><span class="p">,</span> <span class="s2">&quot;./fit_a_line.model/&quot;</span><span class="p">)</span>
    <span class="n">fluid</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">load_persistables</span><span class="p">(</span><span class="n">exe</span><span class="p">,</span> <span class="s2">&quot;./fit_a_line.model/&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">train_reader</span><span class="p">():</span>
        <span class="n">avg_loss_value</span><span class="p">,</span> <span class="o">=</span> <span class="n">exe</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">fluid</span><span class="o">.</span><span class="n">default_main_program</span><span class="p">(),</span>
                                  <span class="n">feed</span><span class="o">=</span><span class="n">feeder</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">data</span><span class="p">),</span>
                                  <span class="n">fetch_list</span><span class="o">=</span><span class="p">[</span><span class="n">avg_cost</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">avg_loss_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">10.0</span><span class="p">:</span>
            <span class="nb">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># if avg cost less than 10.0, we think our code is good.</span>
<span class="nb">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

<p>我们创建了一个简单的全连接神经网络程序，并且通过Fluid的Executor执行了100次迭代,现在我们需要将该单机版本的程序更新为分布式版本的程序。</p>
<h3>介绍Parameter Server</h3>
<p>在非分布式版本的训练脚本中，只存在Trainer一种角色，它不仅处理常规的计算任务，也处理参数相关的计算、保存和优化任务。在分布式版本的训练过程中，由于存在多个Trainer节点进行同样的数据计算任务，因此需要有一个中心化的节点来统一处理参数相关的保存和分配。在PaddlePaddle中，我们称这样的节点为<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/dist_train/parameter_server.md">Parameter Server</a></p>
<p><strong>因此，在分布式的Fluid环境中，我们有两个角色需要创建，分别是Parameter Server和Trainer。</strong></p>
<h3>分布式训练</h3>
<p>Fliud专门提供了工具<a href="https://github.com/PaddlePaddle/Paddle/blob/ba65d54d9d3b41cd3c5171b00f476d4e60133ddb/doc/fluid/design/dist_train/distributed_architecture.md#distributed-transpiler">Distributed Transpiler</a>用于将单机版的训练程序转换为分布式版本的训练程序。工具背后的理念是找出程序的优化算子和梯度参数，将他们分隔为两部分，通过send/recv 操作算子进行连接,优化算子和梯度参数可以在优化器的minimize函数的返回值中获取到。
<div class="highlight"><pre><span></span><span class="n">optimize_ops</span><span class="p">,</span> <span class="n">params_grads</span> <span class="o">=</span> <span class="n">sgd_optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">avg_cost</span><span class="p">)</span>
</pre></div>
将Distributed Transpiler、优化算子和梯度函数放在一个代码中如下：
<div class="highlight"><pre><span></span><span class="o">...</span> <span class="c1">#define the program, cost, and create sgd optimizer</span>

<span class="n">optimize_ops</span><span class="p">,</span> <span class="n">params_grads</span> <span class="o">=</span> <span class="n">sgd_optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">avg_cost</span><span class="p">)</span> <span class="c1">#get optimize OPs and gradient parameters</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">DistributeTranspiler</span><span class="p">()</span> <span class="c1"># create the transpiler instance</span>
<span class="c1"># slice the program into 2 pieces with optimizer_ops and gradient parameters list, as well as pserver_endpoints, which is a comma separated list of [IP:PORT] and number of trainers</span>
<span class="n">t</span><span class="o">.</span><span class="n">transpile</span><span class="p">(</span><span class="n">optimize_ops</span><span class="p">,</span> <span class="n">params_grads</span><span class="p">,</span> <span class="n">pservers</span><span class="o">=</span><span class="n">pserver_endpoints</span><span class="p">,</span> <span class="n">trainers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="o">...</span> <span class="c1">#create executor</span>

<span class="c1"># in pserver, run this</span>
<span class="c1">#current_endpoint here means current pserver IP:PORT you wish to run on</span>
<span class="n">pserver_prog</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">get_pserver_program</span><span class="p">(</span><span class="n">current_endpoint</span><span class="p">)</span>
<span class="n">pserver_startup</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">get_startup_program</span><span class="p">(</span><span class="n">current_endpoint</span><span class="p">,</span> <span class="n">pserver_prog</span><span class="p">)</span>
<span class="n">exe</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">pserver_startup</span><span class="p">)</span>
<span class="n">exe</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">pserver_prog</span><span class="p">)</span>

<span class="c1"># in trainer, run this</span>
<span class="o">...</span> <span class="c1"># define data reader</span>
<span class="n">exe</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">fluid</span><span class="o">.</span><span class="n">default_startup_program</span><span class="p">())</span>
<span class="k">for</span> <span class="n">pass_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">train_reader</span><span class="p">():</span>
        <span class="n">exe</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">get_trainer_program</span><span class="p">())</span>
</pre></div></p>
<h3>分布式训练脚本运行说明</h3>
<p>分布式任务的运行需要将表格中说明的多个参数进行赋值:</p>
<table>
<thead>
<tr>
<th>参数名</th>
<th> 值类型</th>
<th>说明</th>
<th> 示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>trainer_id </td>
<td> int</td>
<td> 当前训练节点的ID，训练节点ID编号为0 - n-1， n为trainers的值 </td>
<td> 0/1/2/3  </td>
</tr>
<tr>
<td>pservers </td>
<td> str</td>
<td> parameter server 列表 </td>
<td> 127.0.0.1:6710,127.0.0.1:6711 </td>
</tr>
<tr>
<td>trainers </td>
<td>int </td>
<td> 训练节点的总个数，>0的数字 </td>
<td> 4 </td>
</tr>
<tr>
<td> server_endpoint</td>
<td> str </td>
<td> 当前所起的服务节点的IP:PORT </td>
<td> 127.0.0.1:8789 </td>
</tr>
<tr>
<td> training_role</td>
<td>str </td>
<td> 节点角色， TRAINER/PSERVER </td>
<td> PSERVER </td>
</tr>
</tbody>
</table>

<p><strong>注意：</strong> <code>training_role</code>是用来区分当前所起服务的角色的，用于训练程序中，用户可根据需要自行定义，其他参数为fluid.DistributeTranspiler的transpile函数所需要，需要在调用函数前进行定义，样例如下：</p>
<div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">DistributeTranspiler</span><span class="p">()</span>
<span class="n">t</span><span class="o">.</span><span class="n">transpile</span><span class="p">(</span>
    <span class="n">optimize_ops</span><span class="p">,</span>
    <span class="n">params_grads</span><span class="p">,</span>
    <span class="n">trainer_id</span><span class="p">,</span>
    <span class="n">pservers</span><span class="o">=</span><span class="n">pserver</span><span class="p">,</span>
    <span class="n">trainers</span><span class="o">=</span><span class="n">trainers</span><span class="p">)</span>
<span class="k">if</span> <span class="n">training_role</span> <span class="o">==</span> <span class="s2">&quot;PSERVER&quot;</span><span class="p">:</span>
    <span class="n">pserver_prog</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">get_pserver_program</span><span class="p">(</span><span class="n">server_endpoint</span><span class="p">)</span>
    <span class="n">pserver_startup</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">get_startup_program</span><span class="p">(</span><span class="n">server_endpoint</span><span class="p">,</span> <span class="n">pserver_prog</span><span class="p">)</span>
</pre></div>

<h3>Demo</h3>
<p>完整的demo代码位于Fluid的test目录下的<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/fluid/tests/book/test_fit_a_line.py">book</a>中。</p>
<p>第一步，进入demo代码所在目录：
<div class="highlight"><pre><span></span><span class="nb">cd</span> /paddle/python/paddle/fluid/tests/book
</pre></div></p>
<p>第二步，启动Parameter Server：
<div class="highlight"><pre><span></span><span class="nv">PADDLE_INIT_PORT</span><span class="o">=</span><span class="m">6174</span> <span class="nv">PADDLE_INIT_PSERVERS</span><span class="o">=</span><span class="m">192</span>.168.1.2 <span class="nv">TRAINERS</span><span class="o">=</span><span class="m">2</span> <span class="nv">POD_IP</span><span class="o">=</span><span class="m">192</span>.168.1.2 <span class="nv">PADDLE_INIT_TRAINER_ID</span><span class="o">=</span><span class="m">1</span> <span class="nv">TRAINING_ROLE</span><span class="o">=</span>PSERVER python test_fit_a_line.py
</pre></div>
执行命令后请等待出现提示： <code>Server listening on 192.168.1.2:6174</code>, 表示Paramter Server已经正常启动。</p>
<p>第三步，启动Trainer：
<div class="highlight"><pre><span></span><span class="nv">PADDLE_INIT_PORT</span><span class="o">=</span><span class="m">6174</span> <span class="nv">PADDLE_INIT_PSERVERS</span><span class="o">=</span><span class="m">192</span>.168.1.3 <span class="nv">TRAINERS</span><span class="o">=</span><span class="m">2</span> <span class="nv">POD_IP</span><span class="o">=</span><span class="m">192</span>.168.1.3 <span class="nv">PADDLE_INIT_TRAINER_ID</span><span class="o">=</span><span class="m">1</span> <span class="nv">TRAINING_ROLE</span><span class="o">=</span>TRAINER python test_fit_a_line.py
</pre></div>
由于我们定义的Trainer的数量是2个，因此需要在另外一个计算节点上再启动一个Trainer。</p>
<p>现在我们就启动了一个包含一个Parameter Server和两个Trainer的分布式训练任务。</p>
{% endverbatim %}