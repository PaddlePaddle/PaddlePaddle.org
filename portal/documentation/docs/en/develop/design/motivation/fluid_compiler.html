<html><body><p>{% verbatim %}
</p><h1>PaddlePaddle Fluid: Towards a Compiled Programming Language</h1>
<p>As described in <a href="fluid.md">fluid.md</a>, when a Fluid application program
runs, it generates a <code>ProgramDesc</code> protobuf message as an intermediate
representation of itself.  The C++ class <code>Executor</code> can run this
protobuf message as an interpreter.  This article describes the Fluid
compiler.</p>
<p><img alt="" src="../../../../_images/fluid-compiler.png"/></p>
<h2>ProgramDesc</h2>
<p>Before we go deeper into the idea of compiled language, let us take a
look at a simple example Fluid application.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="s2">"fluid"</span>

<span class="n">func</span> <span class="n">paddlepaddle</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">X</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="n">W</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="n">fluid</span><span class="o">.</span><span class="n">mult</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
<p>This program consists of a <a href="../concepts/block.md">block</a> of three operators --
<code>read</code>, <code>assign</code>, and <code>mult</code>.  Its <code>ProgramDesc</code> message looks like
the following</p>
<div class="highlight"><pre><span></span><span class="kd">message</span> <span class="nc">ProgramDesc</span> <span class="p">{</span>
  <span class="n">block</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Block</span> <span class="p">{</span>
    <span class="na">vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">Y</span><span class="p">],</span>
    <span class="na">ops</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">read</span><span class="p">(</span><span class="na">output</span> <span class="o">=</span> <span class="n">X</span><span class="p">)</span>
      <span class="n">assign</span><span class="p">(</span><span class="na">input</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span> <span class="na">output</span> <span class="o">=</span> <span class="n">W</span><span class="p">)</span>
      <span class="n">mult</span><span class="p">(</span><span class="na">input</span> <span class="o">=</span> <span class="p">{</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">},</span> <span class="na">output</span> <span class="o">=</span> <span class="n">Y</span><span class="p">)</span>
    <span class="p">],</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<h2>Transpilers</h2>
<p>We can write a transpiler program that takes a <code>ProgramDesc</code>, e.g.,
the above one, and outputs another <code>ProgramDesc</code>.  Let us take some
examples:</p>
<ol>
<li>
<p><em>Memory optimization transpiler</em>: We can write a transpiler that
   inserts some <code>FreeMemoryOp</code>s in the above example <code>ProgramDesc</code> so
   to free memory early, before the end of an iteration, so to keep a
   small memory footprint.</p>
</li>
<li>
<p><em>Distributed training transpiler</em>: We can write a transpiler that
   converts a<code>ProgramDesc</code> into its distributed version of two
   <code>ProgramDesc</code>s -- one for running by the trainer processes and the
   other for the parameter server.</p>
</li>
</ol>
<p>In the rest of this article, we talk about a special kind of
transpiler, <em>Native code generator</em>, which takes a <code>ProgramDesc</code> and
generates a <code>.cu</code> (or <code>.cc</code>) file, which could be built by C++
compilers (gcc, nvcc, icc) into binaries.</p>
<h2>Native Code Generator</h2>
<p>For the above example, the native code generator transpiler, say, the
CUDA code generator, should generate a <code>main</code> function:</p>
<div class="highlight"><pre><span></span><span class="kt">void</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="k">auto</span> <span class="n">X</span> <span class="o">=</span> <span class="n">fluid_cuda_read</span><span class="p">(...);</span>
  <span class="k">auto</span> <span class="n">W</span> <span class="o">=</span> <span class="n">fluid_cuda_create_tensor</span><span class="p">(...);</span>
  <span class="k">auto</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">fluid_cuda_mult</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
<p>and the definitions of functions <code>fluid_cuda_read</code>,
<code>fluid_cuda_create_tensor</code>, and <code>fluid_cuda_mult</code>.  Please be aware
that each function could just define a C++ instance of an operator and
run it.  For example</p>
<div class="highlight"><pre><span></span><span class="n">paddle</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">fluid_cuda_read</span><span class="p">(...)</span> <span class="p">{</span>
  <span class="n">paddle</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">t</span><span class="p">;</span>
  <span class="n">paddle</span><span class="o">::</span><span class="k">operator</span><span class="o">::</span><span class="n">Read</span> <span class="n">r</span><span class="p">(</span><span class="o">&amp;</span><span class="n">t</span><span class="p">,</span> <span class="p">...);</span>
  <span class="n">r</span><span class="p">.</span><span class="n">Run</span><span class="p">();</span>
  <span class="k">return</span> <span class="n">t</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
<p>For computational operators that have multiple <em>kernels</em>, each for a
specific hardware platform, for example, the <code>mult</code> operator, the
generated code should call its CUDA kernel:</p>
<div class="highlight"><pre><span></span><span class="n">paddle</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">fluid_cuda_mult</span><span class="p">(</span><span class="k">const</span> <span class="n">paddle</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">a</span><span class="p">,</span>
                               <span class="k">const</span> <span class="n">paddle</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">b</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">paddle</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">t</span><span class="p">;</span>
  <span class="n">paddle</span><span class="o">::</span><span class="k">operator</span><span class="o">::</span><span class="n">Mult</span> <span class="n">m</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="p">...);</span>
  <span class="n">Mult</span><span class="p">.</span><span class="n">Run</span><span class="p">(</span><span class="n">cuda_context</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
<p>where <code>cuda_context</code> could be a global variable of type
<code>paddle::CUDADeviceContext</code>.</p>
<h2>Multi-Block Code Generation</h2>
<p>Most Fluid application programs may have more than one blocks.  To
execute them, we need to trace <a href="../concepts/scope.md">scopes</a>.</p>
{% endverbatim %}</body></html>