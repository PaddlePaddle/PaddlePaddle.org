{% verbatim %}
<h1>Design Doc: Supporting new Device/Library</h1>
<h2>Background</h2>
<p>Deep learning has a high demand for computing resources. New high-performance devices and computing libraries are appearing very frequently. Deep learning frameworks have to integrate these high-performance devices and computing libraries in a flexible and efficient manner.</p>
<p>On one hand, hardware and computing libraries usually do not have a one-to-one correspondence. For example, Intel CPUs support Eigen and MKL computing libraries while Nvidia GPUs support Eigen and cuDNN computing libraries. We have to implement operator specific kernels for each computing library.</p>
<p>On the other hand, users usually do not want to care about the low-level hardware and computing libraries when writing a neural network configuration. In Fluid, <code>Layer</code> is exposed in <code>Python</code>, and <code>Operator</code> is exposed in <code>C++</code>. Both <code>Layer</code> and <code>Operator</code> are hardware independent.</p>
<p>So, how to support a new Device/Library in Fluid becomes a challenge.</p>
<h2>Basic: Integrate A New Device/Library</h2>
<p>For a general overview of fluid, please refer to the <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/read_source.md">overview doc</a>.</p>
<p>There are mainly three parts that we have to consider while integrating a new device/library:</p>
<ul>
<li>
<p>Place and DeviceContext: indicate the device id and manage hardware resources</p>
</li>
<li>
<p>Memory and Tensor: malloc/free data on certain device</p>
</li>
<li>
<p>Math Functor and OpKernel: implement computing unit on certain devices/libraries</p>
</li>
</ul>
<h3>Place and DeviceContext</h3>
<p>Please note that device and computing library are not one-to-one corresponding. A device can have a lot of computing libraries and a computing library can also support several devices.</p>
<h4>Place</h4>
<p>Fluid uses class <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/platform/place.h#L55">Place</a> to represent the device memory where data is located. If we add another device, we have to add the corresponding <code>DevicePlace</code>.</p>
<div class="highlight"><pre><span></span>        |   CPUPlace
Place --|   CUDAPlace
        |   FPGAPlace
</pre></div>

<p>And <code>Place</code> is defined as follows:</p>
<div class="highlight"><pre><span></span>typedef boost::variant&lt;CUDAPlace, CPUPlace, FPGAPlace&gt; Place;
</pre></div>

<h4>DeviceContext</h4>
<p>Fluid uses class <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/fluid/paddle/platform/device_context.h#L30">DeviceContext</a> to manage the resources in different libraries, such as CUDA stream in <code>CDUADeviceContext</code>. There are also inheritance relationships between different kinds of <code>DeviceContext</code>.</p>
<div class="highlight"><pre><span></span>                /-&gt;  CPUDeviceContext   
DeviceContext ----&gt;  CUDADeviceContext  
                \-&gt;  FPGADeviceContext
</pre></div>

<p>An example of Nvidia GPU is as follows:</p>
<ul>
<li>DeviceContext</li>
</ul>
<div class="highlight"><pre><span></span>class DeviceContext {
  virtual Place GetPlace() const = 0;
};  
</pre></div>

<ul>
<li>CUDADeviceContext</li>
</ul>
<div class="highlight"><pre><span></span>class CUDADeviceContext : public DeviceContext {
  Place GetPlace() const override { return place_; }
private:
  CUDAPlace place_;
  cudaStream_t stream_;
  cublasHandle_t cublas_handle_;
  std::unique_ptr&lt;Eigen::GpuDevice&gt; eigen_device_;  // binds with stream_
};
</pre></div>

<h3>Memory and Tensor</h3>
<h4>memory module</h4>
<p>Fluid provides the following <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/memory/memory.h#L36">memory interfaces</a>:</p>
<div class="highlight"><pre><span></span>template &lt;typename Place&gt;
void* Alloc(Place place, size_t size);

template &lt;typename Place&gt;
void Free(Place place, void* ptr);

template &lt;typename Place&gt;
size_t Used(Place place);
</pre></div>

<p>To implement these interfaces, we have to implement MemoryAllocator for different Devices.</p>
<h4>Tensor</h4>
<p><a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/tensor.h#L36">Tensor</a> holds data with some shape in a specific Place.</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Tensor</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
  <span class="cm">/*! Return a pointer to mutable memory block. */</span>
  <span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">T</span><span class="o">&gt;</span>
  <span class="kr">inline</span> <span class="n">T</span><span class="o">*</span> <span class="n">data</span><span class="p">();</span>

  <span class="cm">/**</span>
<span class="cm">   * @brief   Return a pointer to mutable memory block.</span>
<span class="cm">   * @note    If not exist, then allocation.</span>
<span class="cm">   */</span>
  <span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">T</span><span class="o">&gt;</span>
  <span class="kr">inline</span> <span class="n">T</span><span class="o">*</span> <span class="n">mutable_data</span><span class="p">(</span><span class="n">platform</span><span class="o">::</span><span class="n">Place</span> <span class="n">place</span><span class="p">);</span>

  <span class="cm">/**</span>
<span class="cm">   * @brief     Return a pointer to mutable memory block.</span>
<span class="cm">   *</span>
<span class="cm">   * @param[in] dims    The dimensions of the memory block.</span>
<span class="cm">   * @param[in] place   The place of the memory block.</span>
<span class="cm">   *</span>
<span class="cm">   * @note      If not exist, then allocation.</span>
<span class="cm">   */</span>
  <span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">T</span><span class="o">&gt;</span>
  <span class="kr">inline</span> <span class="n">T</span><span class="o">*</span> <span class="n">mutable_data</span><span class="p">(</span><span class="n">DDim</span> <span class="n">dims</span><span class="p">,</span> <span class="n">platform</span><span class="o">::</span><span class="n">Place</span> <span class="n">place</span><span class="p">);</span>

  <span class="cm">/*! Resize the dimensions of the memory block. */</span>
  <span class="kr">inline</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">Resize</span><span class="p">(</span><span class="k">const</span> <span class="n">DDim</span><span class="o">&amp;</span> <span class="n">dims</span><span class="p">);</span>

  <span class="cm">/*! Return the dimensions of the memory block. */</span>
  <span class="kr">inline</span> <span class="k">const</span> <span class="n">DDim</span><span class="o">&amp;</span> <span class="n">dims</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span>

 <span class="k">private</span><span class="o">:</span>
  <span class="cm">/*! holds the memory block if allocated. */</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Placeholder</span><span class="o">&gt;</span> <span class="n">holder_</span><span class="p">;</span>

  <span class="cm">/*! points to dimensions of memory block. */</span>
  <span class="n">DDim</span> <span class="n">dim_</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>

<p><code>Placeholder</code> is used to delay memory allocation; that is, we can first define a tensor, using <code>Resize</code> to configurate its shape, and then call <code>mutuable_data</code> to allocate the actual memory.</p>
<div class="highlight"><pre><span></span><span class="n">paddle</span><span class="o">::</span><span class="n">framework</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">t</span><span class="p">;</span>
<span class="n">paddle</span><span class="o">::</span><span class="n">platform</span><span class="o">::</span><span class="n">CPUPlace</span> <span class="n">place</span><span class="p">;</span>
<span class="c1">// set size first</span>
<span class="n">t</span><span class="p">.</span><span class="n">Resize</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">});</span>
<span class="c1">// allocate memory on CPU later</span>
<span class="n">t</span><span class="p">.</span><span class="n">mutable_data</span><span class="p">(</span><span class="n">place</span><span class="p">);</span>
</pre></div>

<h3>Math Functor and OpKernel</h3>
<p>Fluid implements computing units based on different DeviceContexts. Some computing units are shared between operators. This common part will be put in operators/math directory as basic Functors.</p>
<p>Let's take <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/operators/math/maxouting.h#L27">MaxOutFunctor</a> as an example:</p>
<p>The interface is defined in the header file.</p>
<div class="highlight"><pre><span></span>template &lt;typename DeviceContext, typename T&gt;
class MaxOutFunctor {
 public:
  void operator()(const DeviceContext&amp; context, const framework::Tensor&amp; input,
                  framework::Tensor* output, int groups);
};
</pre></div>

<p>CPU implementation is in .cc file</p>
<div class="highlight"><pre><span></span>template &lt;typename T&gt;
class MaxOutFunctor&lt;platform::CPUDeviceContext, T&gt; {
  public:
  void operator()(const platform::CPUDeviceContext&amp; context,
                  const framework::Tensor&amp; input, framework::Tensor* output,
                  int groups) {
                  ...
                  }
};
</pre></div>

<p>CUDA implementation is in .cu file</p>
<div class="highlight"><pre><span></span>template &lt;typename T&gt;
class MaxOutFunctor&lt;platform::CUDADeviceContext, T&gt; {
 public:
  void operator()(const platform::CUDADeviceContext&amp; context,
                  const framework::Tensor&amp; input, framework::Tensor* output,
                  int groups) {
                  ...
                  }
};                  
</pre></div>

<p>We first obtain the computing handle from a concrete DeviceContext and then compute on tensors.</p>
<p>The implementation of <code>OpKernel</code> is similar to math functors, the extra thing we need to do is to register the OpKernel in a global map.</p>
<p>Fluid provides different register interfaces in op_registry.h</p>
<p>Let's take <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/operators/crop_op.cc#L134">Crop</a> operator as an example:</p>
<p>In .cc file:</p>
<div class="highlight"><pre><span></span>REGISTER_OP_CPU_KERNEL(crop, ops::CropKernel&lt;float&gt;);
REGISTER_OP_CPU_KERNEL(
    crop_grad, ops::CropGradKernel&lt;paddle::platform::CPUDeviceContext, float&gt;);
</pre></div>

<p>In .cu file:</p>
<div class="highlight"><pre><span></span>REGISTER_OP_CUDA_KERNEL(crop, ops::CropKernel&lt;float&gt;);
REGISTER_OP_CUDA_KERNEL(
    crop_grad, ops::CropGradKernel&lt;paddle::platform::CUDADeviceContext, float&gt;);
</pre></div>

<h2>Advanced topics: How to switch between different Device/Library</h2>
<p>Generally, we will implement OpKernel for all Device/Library of an Operator. We can easily train a Convolutional Neural Network in GPU. However, some OpKernel is not suitable on a specific Device. For example, crf operator can only run on CPU, whereas most other operators can run on GPU. To achieve high performance in such circumstance, we have to switch between different Device/Library.</p>
<p>For more details, please refer to following docs:</p>
<ul>
<li>operator kernel type <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/multi_devices/operator_kernel_type.md">doc</a></li>
<li>switch kernel <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/execution/switch.md">doc</a></li>
</ul>
{% endverbatim %}