{% verbatim %}
<h1>Design Doc: Refactorization Overview</h1>
<p>The goals of refactoring include:</p>
<ol>
<li>Making it easy for external contributors to write new elementary computation operations.</li>
<li>Making the codebase clean and readable.</li>
<li>Designing a new computation representation -- a computation graph of operators and variables.</li>
<li>Implementing auto-scalability and auto fault recoverable distributed computing with the help of computation graphs.</li>
</ol>
<h2>Computation Graphs</h2>
<ol>
<li>
<p>PaddlePaddle represents the computation, training and inference of Deep Learning models, by computation graphs.</p>
</li>
<li>
<p>Please refer to <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/others/graph.md">computation graphs</a> for a concrete example.</p>
</li>
<li>
<p>Users write Python programs to describe the graphs and run them (locally or remotely).</p>
</li>
<li>
<p>A graph is composed of <em>variables</em> and <em>operators</em>.</p>
</li>
<li>
<p>The description of graphs must be serializable/deserializable, so that:</p>
</li>
<li>
<p>It can be sent to the cloud for distributed execution, and</p>
</li>
<li>
<p>It can be sent to clients for mobile or enterprise deployment.</p>
</li>
<li>
<p>The Python program does two things</p>
</li>
<li>
<p><em>Compilation</em> runs a Python program to generate a protobuf message representation of the graph and send it to</p>
<ol>
<li>the C++ library <code>libpaddle.so</code> for local execution,</li>
<li>the master process of a distributed training job for training, or</li>
<li>the server process of a Kubernetes serving job for distributed serving.</li>
</ol>
</li>
<li><em>Execution</em> executes the graph by constructing instances of class <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/variable.h#L24"><code>Variable</code></a> and <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/operator.h#L70"><code>OperatorBase</code></a>, according to the protobuf message.</li>
</ol>
<h2>Description and Realization of Computation Graph</h2>
<p>At compile time, the Python program generates a protobuf message representation of the graph, or a description of the graph.</p>
<p>At runtime, the C++ program realizes the graph and runs it.</p>
<table>
<thead>
<tr>
<th></th>
<th>Representation (protobuf messages)</th>
<th>Realization (C++ class objects) </th>
</tr>
</thead>
<tbody>
<tr>
<td>Data</td>
<td>
<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/framework.proto#L107">VarDesc</a></td>
<td>
<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/variable.h#L24">Variable</a></td>
</tr>
<tr>
<td>Operation </td>
<td>
<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/framework.proto#L35">OpDesc</a></td>
<td>
<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/operator.h#L64">Operator</a></td>
</tr>
<tr>
<td>Block </td>
<td>BlockDesc </td>
<td>Block </td>

</tbody>
</table>

<p>The word <em>graph</em> is interchangeable with <em>block</em> in this document.  A graph consists of computation steps and local variables similar to a C++/Java program block, or a pair of parentheses(<code>{</code> and <code>}</code>).</p>
<h2>Compilation and Execution</h2>
<ol>
<li>
<p>Run a Python program to describe the graph.  In particular, the Python application program does the following:</p>
</li>
<li>
<p>Create <code>VarDesc</code> to represent local/intermediate variables,</p>
</li>
<li>Create operators and set attributes,</li>
<li>Validate attribute values,</li>
<li>Infer the type and the shape of variables,</li>
<li>Plan memory-reuse for variables,</li>
<li>Generate the backward graph</li>
<li>Add optimization operators to the computation graph.</li>
<li>
<p>Optionally, split the graph for distributed training.</p>
</li>
<li>
<p>The invocation of <code>train</code> or <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/inference.py#L108"><code>infer</code></a> methods in the Python program does the following:</p>
</li>
<li>
<p>Create a new Scope instance in the <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/concepts/scope.md">scope hierarchy</a> for each run of a block,</p>
<ol>
<li>realize local variables defined in the BlockDesc message in the new scope,</li>
<li>a scope is similar to the stack frame in programming languages,</li>
</ol>
</li>
<li>
<p>Create an instance of class <code>Block</code>, in which,</p>
<ol>
<li>realize operators in the BlockDesc message,</li>
</ol>
</li>
<li>
<p>Run the Block by calling</p>
<ol>
<li><code>Block::Eval(vector&lt;Variable&gt;* targets)</code> for forward and backward computations, or</li>
<li><code>Block::Eval(vector&lt;Operator&gt;* targets)</code> for optimization.</li>
</ol>
</li>
</ol>
<h2>Intermediate Representation (IR)</h2>
<div class="highlight"><pre><span></span>Compile Time -&gt; IR -&gt; Runtime
</pre></div>

<h3>Benefits of IR</h3>
<ul>
<li>Optimization
  <div class="highlight"><pre><span></span>Compile Time -&gt; IR -&gt; Optimized IR -&gt; Runtime
</pre></div></li>
<li>Automatically send partitioned IR to different nodes.</li>
<li>Automatic Data Parallelism
    <div class="highlight"><pre><span></span>Compile Time
|-&gt; Single GPU IR
    |-&gt; [trainer-IR-0, trainer-IR-1, pserver-IR]
        |-&gt; Node-0 (runs trainer-IR-0)
        |-&gt; Node-1 (runs trainer-IR-1)
        |-&gt; Node-2 (runs pserver-IR)
</pre></div></li>
<li>Automatic Model Parallelism (planned for future)</li>
</ul>
<hr />
<h2>Operator/OpWithKernel/OpKernel</h2>
<p><img alt="class_diagram" src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/op_op_with_kern_class_diagram.dot" /></p>
<hr />
<h2>Operator</h2>
<p><img alt="class_diagram" src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/op.dot" /></p>
<ul>
<li><code>Operator</code> is the fundamental building block of the user interface.<ul>
<li>Operator stores input/output variable names and attributes.</li>
<li>The <code>InferShape</code> interface is used to infer the shape of the output variables based on the shapes of the input variables.</li>
<li>Use <code>Run</code> to compute the <code>output</code> variables from the <code>input</code> variables.</li>
</ul>
</li>
</ul>
<hr />
<h2>OpWithKernel/Kernel</h2>
<p><img alt="class_diagram" src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/op_with_kernel.dot" /></p>
<ul>
<li><code>OpWithKernel</code> inherits <code>Operator</code>.</li>
<li><code>OpWithKernel</code> contains a Kernel map.<ul>
<li><code>OpWithKernel::Run</code> get device's kernel, and invoke <code>OpKernel::Compute</code>.</li>
<li><code>OpKernelKey</code> is the map key. Only device place now, but may be data type later.</li>
</ul>
</li>
</ul>
<hr />
<h2>Why separate Kernel and Operator</h2>
<ul>
<li>Separate GPU and CPU code.<ul>
<li>Make Paddle capable of running without GPU.</li>
</ul>
</li>
<li>Make one operator (which is a user interface) and create many implementations.<ul>
<li>For example, same multiplication op can have different implementations kernels such as FP16 kernel, FP32 kernel, MKL, eigen kernel.</li>
</ul>
</li>
</ul>
<hr />
<h2>Libraries for Kernel development</h2>
<ul>
<li><code>Eigen::Tensor</code> contains basic math and element-wise functions.<ul>
<li>Note that <code>Eigen::Tensor</code> has broadcast implementation.</li>
<li>Limit the number of <code>tensor.device(dev) =</code> in your code.</li>
</ul>
</li>
<li><code>thrust::transform</code> and <code>std::transform</code>.<ul>
<li><code>thrust</code> has the same API as C++ standard library. Using <code>transform</code>, one can quickly implement customized element-wise kernels.</li>
<li><code>thrust</code>, in addition, supports more complex APIs, like <code>scan</code>, <code>reduce</code>, <code>reduce_by_key</code>.</li>
</ul>
</li>
<li>Hand-writing <code>GPUKernel</code> and <code>CPU</code> code<ul>
<li>Do not write in header (<code>.h</code>) files. CPU Kernel should be in cpp source (<code>.cc</code>) and GPU kernels should be in cuda (<code>.cu</code>) files. (GCC cannot compile GPU code.)</li>
</ul>
</li>
</ul>
<hr />
<h2>Operator Registration</h2>
<h3>Why is registration necessary?</h3>
<p>We need a method to build mappings between Op type names and Op classes.</p>
<h3>How is registration implemented?</h3>
<p>Maintaining a map, whose key is the type name and the value is the corresponding Op constructor.</p>
<hr />
<h2>The Registry Map</h2>
<h3><code>OpInfoMap</code></h3>
<p><code>op_type(string)</code> -&gt; <code>OpInfo</code></p>
<p><code>OpInfo</code>:</p>
<ul>
<li><strong><code>creator</code></strong>: The Op constructor.</li>
<li><strong><code>grad_op_type</code></strong>: The type of the gradient Op.</li>
<li><strong><code>proto</code></strong>: The Op's Protobuf, including inputs, outputs and required attributes.</li>
<li><strong><code>checker</code></strong>: Used to check attributes.</li>
</ul>
<hr />
<h2>Related Concepts</h2>
<h3>Op_Maker</h3>
<p>It's constructor takes <code>proto</code> and <code>checker</code>. They are completed during Op_Maker's construction. (<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/operators/scale_op.cc#L37">ScaleOpMaker</a>)</p>
<h3>Register Macros</h3>
<div class="highlight"><pre><span></span><span class="n">REGISTER_OP</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">op_class</span><span class="p">,</span> <span class="n">op_maker_class</span><span class="p">,</span> <span class="n">grad_op_type</span><span class="p">,</span> <span class="n">grad_op_class</span><span class="p">)</span>
<span class="n">REGISTER_OP_WITHOUT_GRADIENT</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">op_class</span><span class="p">,</span> <span class="n">op_maker_class</span><span class="p">)</span>
</pre></div>

<hr />
<h2>Registration Process</h2>
<ol>
<li>Write an Op class and its gradient Op class, if required.</li>
<li>Write an Op maker class. In the constructor of this class, describe the inputs, outputs and attributes of the operator.</li>
<li>Invoke the macro <code>REGISTER_OP</code>. This macro will<ol>
<li>Call maker class to complete <code>proto</code> and <code>checker</code></li>
<li>Using the completed <code>proto</code> and <code>checker</code>, it will add a new key-value pair to the <code>OpInfoMap</code></li>
</ol>
</li>
</ol>
<hr />
<h2>Backward Module (1/2)</h2>
<h3>Create Backward Operator</h3>
<ul>
<li>Mapping from forward Op to backward Op
<img alt="backward" src="https://gist.githubusercontent.com/dzhwinter/a6fbd4623ee76c459f7f94591fd1abf0/raw/61026ab6e518e66bde66a889bc42557a1fccff33/backward.png" /></li>
</ul>
<hr />
<h2>Backward Module (2/2)</h2>
<h3>Build Backward Network</h3>
<ul>
<li><strong>Input</strong>: a graph of forward operators</li>
<li><strong>Output</strong>: a graph of backward operators</li>
<li><strong>Corner cases in construction</strong><ul>
<li>Shared Variables =&gt; insert an <code>Add</code> operator to combine gradients</li>
<li>No Gradient =&gt; insert a <code>fill_zero_grad</code> operator</li>
<li>Recursive NetOp =&gt; call <code>Backward</code> recursively</li>
<li>RNN Op =&gt; recursively call <code>Backward</code> on stepnet</li>
<li>RNN Op =&gt; recursively call <code>Backward</code> on stepnet</li>
</ul>
</li>
</ul>
<hr />
<h2>Scope, Variable, Tensor</h2>
<ul>
<li><code>Tensor</code> is an n-dimension array with type.<ul>
<li>Only dims and data pointers are stored in <code>Tensor</code>.</li>
<li>All operations on <code>Tensor</code> are written in <code>Operator</code> or global functions.</li>
<li>Variable length Tensor design <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/design/concepts/lod_tensor.md">LoDTensor</a></li>
</ul>
</li>
<li><code>Variable</code> instances are the inputs and the outputs of an operator, not just <code>Tensor</code>.<ul>
<li><code>step_scopes</code> in RNN is a variable and not a tensor.</li>
</ul>
</li>
<li><code>Scope</code> is where variables are stored.<ul>
<li>map<string <code>var name</code>, Variable></li>
<li><code>Scope</code> has a hierarchical structure. The local scope can get variables from its parent scope.</li>
</ul>
</li>
</ul>
<hr />
<h2>Block (in design)</h2>
<h3>the difference between original RNNOp and Block</h3>
<ul>
<li>As an operator is more intuitive than <code>RNNOp</code>,</li>
<li>Offers a new interface <code>Eval(targets)</code> to deduce the minimal block to <code>Run</code>,</li>
<li>Fits the compile-time/ runtime separation design paradigm.</li>
<li>During the compilation, <code>SymbolTable</code> stores <code>VarDesc</code>s and <code>OpDesc</code>s and serialize to a <code>BlockDesc</code></li>
<li>When graph executes, a Block with <code>BlockDesc</code> is passed. It then creates <code>Op</code> and <code>Var</code> instances and then invokes <code>Run</code>.</li>
</ul>
<hr />
<h2>Milestone</h2>
<ul>
<li>Take Paddle/books as the main line, the requirement of the models motivates framework refactoring,</li>
<li>Model migration</li>
<li>Framework development gives <strong>priority support</strong> to model migration, for example,<ul>
<li>the MNIST demo needs a Python interface,</li>
<li>the RNN models require the framework to support <code>LoDTensor</code>.</li>
</ul>
</li>
<li>Determine some timelines,</li>
<li>Frequently used Ops need to be migrated first,</li>
<li>Different models can be migrated in parallel.</li>
<li>Improve the framework at the same time</li>
<li>Accept imperfection, concentrate on solving the specific problem at the right price.</li>
</ul>
<hr />
<h2>Control the migration quality</h2>
<ul>
<li>Compare the performance of migrated models with old ones.</li>
<li>Follow the google C++ style guide.</li>
<li>Build the automatic workflow of generating Python/C++ documentations.</li>
<li>The documentation of layers and ops should be written inside the code.</li>
<li>Take the documentation quality into account when submitting pull requests.</li>
<li>Preview the documentations, read and improve them from a user's perspective.</li>
</ul>
{% endverbatim %}