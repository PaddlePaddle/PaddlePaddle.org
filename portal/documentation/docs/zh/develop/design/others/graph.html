<html><body><p>{% verbatim %}
</p><h1>Design Doc: Computations as a Graph</h1>
<p>A primary goal of the refactorization of PaddlePaddle is a more flexible representation of deep learning computation, in particular, a graph of operators and variables, instead of sequences of layers as before.</p>
<p>This document explains that the construction of a graph as three steps:</p>
<ul>
<li>construct the forward part</li>
<li>construct the backward part</li>
<li>construct the optimization part</li>
</ul>
<h2>The Construction of a Graph</h2>
<p>Let us take the problem of image classification as a simple example.  The application program that trains the model looks like:</p>
<div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="s2">"images"</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="s2">"label"</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="n">optimize</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
<span class="n">train</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">reader</span><span class="o">=</span><span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="p">())</span>
</pre></div>
<h3>Forward Part</h3>
<p>The first four lines of above program build the forward part of the graph.</p>
<p><img alt="" src="../../../../_images/graph_construction_example_forward_only.png"/></p>
<p>In particular, the first line <code>x = layer.data("images")</code> creates variable x and a Feed operator that copies a column from the minibatch to x.  <code>y = layer.fc(x)</code> creates not only the FC operator and output variable y, but also two parameters, W and b, and the initialization operators.</p>
<p>Initialization operators are kind of "run-once" operators -- the <code>Run</code> method increments a class data member counter so to run at most once.  By doing so, a parameter wouldn't be initialized repeatedly, say, in every minibatch.</p>
<p>In this example, all operators are created as <code>OpDesc</code> protobuf messages, and all variables are <code>VarDesc</code>.  These protobuf messages are saved in a <code>BlockDesc</code> protobuf message.</p>
<h3>Backward Part</h3>
<p>The fifth line <code>optimize(cost)</code> calls two functions, <code>ConstructBackwardGraph</code> and <code>ConstructOptimizationGraph</code>.</p>
<p><code>ConstructBackwardGraph</code> traverses the forward graph in the <code>BlockDesc</code> protobuf message and builds the backward part.</p>
<p><img alt="" src="../../../../_images/graph_construction_example_forward_backward.png"/></p>
<p>According to the chain rule of gradient computation, <code>ConstructBackwardGraph</code> would</p>
<ol>
<li>create a gradient operator G for each operator F,</li>
<li>make all inputs, outputs, and outputs' gradient of F as inputs of G,</li>
<li>create gradients for all inputs of F, except for those who don't have gradients, like x and l, and</li>
<li>make all these gradients as outputs of G.</li>
</ol>
<h3>Optimization Part</h3>
<p>For each parameter, like W and b created by <code>layer.fc</code>, marked as double circles in above graphs, <code>ConstructOptimizationGraph</code> creates an optimization operator to apply its gradient.  Here results in the complete graph:</p>
<p><img alt="" src="../../../../_images/graph_construction_example_all.png"/></p>
<h2>Block and Graph</h2>
<p>The word block and graph are interchangable in the desgin of PaddlePaddle.  A <a href="https://github.com/PaddlePaddle/Paddle/pull/3708">Block</a> is a metaphore of the code and local variables in a pair of curly braces in programming languages, where operators are like statements or instructions.  A graph of operators and variables is a representation of the block.</p>
<p>A Block keeps operators in an array <code>BlockDesc::ops</code></p>
<div class="highlight"><pre><span></span><span class="kd">message</span> <span class="nc">BlockDesc</span> <span class="p">{</span>
  <span class="k">repeated</span> <span class="n">OpDesc</span> <span class="na">ops</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="k">repeated</span> <span class="n">VarDesc</span> <span class="na">vars</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
<p>in the order that they appear in user programs, like the Python program at the beginning of this article.  We can imagine that in <code>ops</code>,  we have some forward operators, followed by some gradient operators, and then some optimization operators.</p>
{% endverbatim %}</body></html>