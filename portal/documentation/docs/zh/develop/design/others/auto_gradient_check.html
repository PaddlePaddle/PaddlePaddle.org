{% verbatim %}
<h2>Auto Gradient Check Design</h2>
<h2>Backgroundï¼š</h2>
<ul>
<li>Generally, it is easy to check whether the forward computation of an Operator is correct or not. However, backpropagation is a notoriously difficult algorithm to debug and get right because of the following challenges:</li>
<li>The formula for backpropagation formula should be correct according to the forward computation.</li>
<li>The Implementation of the above shoule be correct in CPP.</li>
<li>
<p>It is difficult to prepare an unbiased test data.</p>
</li>
<li>
<p>Auto gradient checking gets a numerical gradient using forward Operator and uses it as a reference for the backward Operator's result. It has several advantages:</p>
</li>
<li>Numerical gradient checker only needs the forward operator.</li>
<li>The user only needs to prepare the input data for forward Operator and not worry about the backward Operator.</li>
</ul>
<h2>Mathematical Theory</h2>
<p>The following documents from Stanford have a detailed explanation of how to compute the numerical gradient and why it is useful.</p>
<ul>
<li><a href="http://deeplearning.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization">Gradient checking and advanced optimization(en)</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E4%B8%8E%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96">Gradient checking and advanced optimization(cn)</a></li>
</ul>
<h2>Numerical Gradient Implementation</h2>
<h3>Python Interface</h3>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_numerical_gradient</span><span class="p">(</span><span class="n">op</span><span class="p">,</span>
                         <span class="n">input_values</span><span class="p">,</span>
                         <span class="n">output_name</span><span class="p">,</span>
                         <span class="n">input_to_check</span><span class="p">,</span>
                         <span class="n">delta</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span>
                         <span class="n">local_scope</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get Numerical Gradient for the input of an operator.</span>

<span class="sd">    :param op: C++ operator instance, could be an network.</span>
<span class="sd">    :param input_values: The input variables. Should be an dictionary, whose key is</span>
<span class="sd">    variable name, and value is a numpy array.</span>
<span class="sd">    :param output_name: The final output variable name.</span>
<span class="sd">    :param input_to_check: The input variable with respect to which the gradient has to be computed.</span>
<span class="sd">    :param delta: The perturbation value for numerical gradient method. The</span>
<span class="sd">    smaller the delta, the more accurate the result. But if the delta is too</span>
<span class="sd">    small, it will suffer from the numerical stability problem.</span>
<span class="sd">    :param local_scope: The local scope used for get_numeric_gradient.</span>
<span class="sd">    :return: The gradient array in numpy format.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>

<h3>Explanation:</h3>
<ul>
<li>Why do we need an <code>output_name</code></li>
<li>
<p>An Operator may have multiple Outputs, one can compute an independent gradient from each Output. So the caller should specify the name of the output variable.</p>
</li>
<li>
<p>Why do we need <code>input_to_check</code></p>
</li>
<li>One operator can have multiple inputs. Gradient Op can calculate the gradient of these inputs at the same time. But Numerical Gradient needs to calculate them one by one. So <code>get_numeric_gradient</code> is designed to calculate the gradient for one input. If you need to compute multiple inputs, you can call <code>get_numeric_gradient</code> multiple times each with a different input.</li>
</ul>
<h3>Core Algorithm Implementation</h3>
<div class="highlight"><pre><span></span>    <span class="c1"># we only compute the gradient of one element a time.</span>
    <span class="c1"># we use a for loop to compute the gradient of each element.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">tensor_size</span><span class="p">):</span>
        <span class="c1"># get one input element using the index i.</span>
        <span class="n">original</span> <span class="o">=</span> <span class="n">tensor_to_check</span><span class="o">.</span><span class="n">get_float_element</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

        <span class="c1"># add delta to it, run the forward op and then</span>
        <span class="c1"># get the new value of the result tensor.</span>
        <span class="n">x_pos</span> <span class="o">=</span> <span class="n">original</span> <span class="o">+</span> <span class="n">delta</span>
        <span class="n">tensor_to_check</span><span class="o">.</span><span class="n">set_float_element</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x_pos</span><span class="p">)</span>
        <span class="n">y_pos</span> <span class="o">=</span> <span class="n">get_output</span><span class="p">()</span>

        <span class="c1"># Subtract delta from this element, run the op again</span>
        <span class="c1"># and get the new value of the result tensor.</span>
        <span class="n">x_neg</span> <span class="o">=</span> <span class="n">original</span> <span class="o">-</span> <span class="n">delta</span>
        <span class="n">tensor_to_check</span><span class="o">.</span><span class="n">set_float_element</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x_neg</span><span class="p">)</span>
        <span class="n">y_neg</span> <span class="o">=</span> <span class="n">get_output</span><span class="p">()</span>

        <span class="c1"># restore old value</span>
        <span class="n">tensor_to_check</span><span class="o">.</span><span class="n">set_float_element</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">original</span><span class="p">)</span>

        <span class="c1"># compute the gradient of this element and store</span>
        <span class="c1"># it into a numpy array.</span>
        <span class="n">gradient_flat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pos</span> <span class="o">-</span> <span class="n">y_neg</span><span class="p">)</span> <span class="o">/</span> <span class="n">delta</span> <span class="o">/</span> <span class="mi">2</span>

    <span class="c1"># reshape the gradient result to the shape of the source tensor.</span>
    <span class="k">return</span> <span class="n">gradient_flat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor_to_check</span><span class="o">.</span><span class="n">get_dims</span><span class="p">())</span>
</pre></div>

<h2>Auto Gradient Check Framework</h2>
<p>Each Operator Kernel has three kinds of Gradient:</p>
<ol>
<li>Numerical gradient</li>
<li>CPU kernel gradient</li>
<li>GPU kernel gradient (if supported by the device)</li>
</ol>
<p>The numerical gradient only relies on the forward Operator, so we use the numerical gradient as the reference value. The gradient checking is performed in the following three steps:</p>
<ol>
<li>Calculate the numerical gradient</li>
<li>Calculate CPU kernel gradient with the backward Operator and compare it with the numerical gradient.</li>
<li>Calculate GPU kernel gradient with the backward Operator and compare it with the numeric gradient. (if supported)</li>
</ol>
<h4>Python Interface</h4>
<div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">check_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                   <span class="n">forward_op</span><span class="p">,</span>
                   <span class="n">input_vars</span><span class="p">,</span>
                   <span class="n">inputs_to_check</span><span class="p">,</span>
                   <span class="n">output_name</span><span class="p">,</span>
                   <span class="n">no_grad_set</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                   <span class="n">only_cpu</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                   <span class="n">max_relative_error</span><span class="o">=</span><span class="mf">0.005</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param forward_op: used to create backward_op</span>
<span class="sd">        :param input_vars: numpy value of input variable. The following</span>
<span class="sd">          computation will use these variables.</span>
<span class="sd">        :param inputs_to_check: the input variable with respect to which the</span>
<span class="sd">          gradient will be computed.</span>
<span class="sd">        :param output_name: The final output variable name.</span>
<span class="sd">        :param max_relative_error: The relative tolerance parameter.</span>
<span class="sd">        :param no_grad_set: used to create backward ops</span>
<span class="sd">        :param only_cpu: only compute and check gradient on cpu kernel.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
</pre></div>

<h3>How to check if two numpy arrays are close enough?</h3>
<p>if <code>abs_numerical_grad</code> is nearly zero, then use absolute error for numerical_grad.</p>
<div class="highlight"><pre><span></span><span class="n">numerical_grad</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">operator_grad</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">scope</span><span class="o">.</span><span class="n">find_var</span><span class="p">(</span><span class="n">grad_var_name</span><span class="p">(</span><span class="n">name</span><span class="p">))</span><span class="o">.</span><span class="n">get_tensor</span><span class="p">())</span>

<span class="n">abs_numerical_grad</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">numerical_grad</span><span class="p">)</span>
<span class="c1"># if abs_numerical_grad is nearly zero, then use abs error for</span>
<span class="c1"># numeric_grad, instead of relative error.</span>
<span class="n">abs_numerical_grad</span><span class="p">[</span><span class="n">abs_numerical_grad</span> <span class="o">&lt;</span> <span class="mf">1e-3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">diff_mat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">abs_numerical_grad</span> <span class="o">-</span> <span class="n">operator_grad</span><span class="p">)</span> <span class="o">/</span> <span class="n">abs_numerical_grad</span>
<span class="n">max_diff</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">diff_mat</span><span class="p">)</span>
</pre></div>

<h4>Notesï¼š</h4>
<p>The Input data for auto gradient checker should be reasonable to avoid numerical stability problem.</p>
<h4>References:</h4>
<ul>
<li><a href="http://deeplearning.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization">Gradient checking and advanced optimization(en)</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E4%B8%8E%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96">Gradient checking and advanced optimization(cn)</a></li>
</ul>
{% endverbatim %}