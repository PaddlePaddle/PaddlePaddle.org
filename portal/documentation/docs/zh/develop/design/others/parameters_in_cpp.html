{% verbatim %}
<h1>Design Doc: The C++ Class <code>Parameters</code></h1>
<p><code>Parameters</code> is a concept we designed in PaddlePaddle V2 API. <code>Parameters</code> is a container of parameters, which makes PaddlePaddle capable of  sharing parameter between topologies. We described usages of <code>Parameter</code> in <a href="./api.md">api.md</a>.</p>
<p>We used Python to implement Parameters when designing V2 API before. There are several defects for the current implementation:
<em> We just use <code>memcpy</code> to share Parameters between topologies, but this is very inefficient. 
</em> We did not support sharing Parameters while training. We just trigger <code>memcpy</code> when start training.</p>
<p>It is necessary that we implement Parameters in CPP side. However, it could result a code refactoring for PaddlePaddle, because PaddlePaddle was designed for training only one topology before, i.e., each GradientMachine contains its Parameter as a data member. In current PaddlePaddle implementation, there are three concepts associated with <code>Parameters</code>:</p>
<ol>
<li>
<p><code>paddle::Parameter</code>. A <code>Parameters</code> is a container for <code>paddle::Parameter</code>.
It is evident that we should use <code>paddle::Parameter</code> when developing <code>Parameters</code>.
However, the <code>Parameter</code> class contains many functions and does not have a clear interface.
It contains <code>create/store Parameter</code>, <code>serialize/deserialize</code>, <code>optimize(i.e SGD)</code>, <code>randomize/zero</code>.
When we developing <code>Parameters</code>, we only use <code>create/store Parameter</code> functionality.
We should extract functionalities of Parameter into many classes to clean PaddlePaddle CPP implementation.</p>
</li>
<li>
<p><code>paddle::GradientMachine</code> and its sub-classes, e.g., <code>paddle::MultiGradientMachine</code>, <code>paddle::NeuralNetwork</code>.
We should pass <code>Parameters</code> to <code>paddle::GradientMachine</code> when <code>forward/backward</code> to avoid <code>memcpy</code> between topologies.
Also, we should handle multi-GPU/CPU training, because <code>forward</code> and <code>backward</code> would perform on multi-GPUs and multi-CPUs.
<code>Parameters</code> should dispatch the parameter value to each device, and gather the parameter gradient from each device.</p>
</li>
<li>
<p><code>paddle::ParameterUpdater</code>. The ParameterUpdater is used to update parameters in Paddle. 
So <code>Parameters</code> should be used by <code>paddle::ParameterUpdater</code>, and <code>paddle::ParameterUpdater</code> should optimize <code>Parameters</code> (by SGD).</p>
</li>
</ol>
<p>The step by step approach for implementation Parameters in PaddlePaddle C++ core is listed below. Each step should be a PR and could be merged into PaddlePaddle one by one.</p>
<ol>
<li>
<p>Clean <code>paddle::Parameter</code> interface. Extract the functionalities of <code>paddle::Parameter</code> to prepare for the implementation of Parameters.</p>
</li>
<li>
<p>Implementation a <code>Parameters</code> class. It just stores the <code>paddle::Parameter</code> inside. Make <code>GradientMachine</code> uses <code>Parameters</code> as a class member.</p>
</li>
<li>
<p>Make <code>Parameters</code> support Multi-CPU and Multi-GPU training to prepare for sharing <code>Parameter</code> between topologies.
Because we need share <code>Parameters</code> between topologies, it is <code>Parameters</code>'s response to exchange Parameters between GPUs.
<code>GradientMachine</code> should not handle how to exchange Parameters because <code>GradientMachine</code> only used to train one topology and we need to support train many topologies in Paddle, i.e., there could be many GradientMachines use one <code>Parameters</code>.</p>
</li>
<li>We should use a global function to exchange Parameters between GPUs, not a member function in <code>Parameters</code>. The <code>MultiGradientMachine</code> invoke this function, which uses <code>Parameters</code> as this function inputs.</li>
<li>
<p>The MultiGradientMachine contains many functionalities. Extracting the Parameters exchanging logic could make MultiGradientMachine clearer and simpler.</p>
</li>
<li>
<p>Make <code>Parameters</code> as an argument for <code>forward/backward</code> function, not a data member for <code>GradientMachine</code>. For example, <code>forward</code> could be <code>forward(const Parameters&amp; params, ...)</code> and <code>backward</code> could be <code>backward(Parameters* params, ...)</code>. After this step, Paddle could share <code>Parameters</code> between topologies.</p>
</li>
<li>
<p><code>ParameterUpdater</code> is invoked by <code>GradientMachine</code> and <code>Trainer</code>, but it updates <code>Parameters</code>. In the end of this code refactoring, we could change <code>ParameterUpdater</code> directly uses <code>Parameters</code> to make <code>ParameterUpdater</code>'s implementation clear.</p>
</li>
</ol>
{% endverbatim %}