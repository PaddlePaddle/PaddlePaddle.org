{% verbatim %}
<h1>Design Doc: Parameter Server</h1>
<h2>Abstract</h2>
<p>We propose an approach to implement the parameter server. In this
approach, there is no fundamental difference between the trainer and
the parameter server: they both run subgraphs, but subgraphs of
different purposes.</p>
<h2>Background</h2>
<p>The previous implementations of the parameter server do not run a
fluid sub-program. Parameter initialization, optimizer computation, network
communication and checkpointing are implemented twice on both the
trainer as well as the parameter server.</p>
<p>It would be great if we can write code once and use them on both: the
trainer and the parameter server, since this reduces code duplication and
improves extensibility. Given that after the current refactoring, we are
representing everything as a computation graph on the
trainer. Representing everything as a computation graph on the parameter
server becomes a natural extension.</p>
<h2>Design</h2>
<h3>Distributed Transpiler</h3>
<p>The <em>Distributed Transpiler</em> converts the user-defined fluid program
into sub-programs to be scheduled on different nodes with the following
steps:</p>
<ol>
<li>OP placement: the OPs will be placed on different nodes according
   to a heuristic that minimizes the estimated total computation
   time. Currently we will use a simple heuristic that puts parameter
   variable on parameter server workers and everything else on trainer
   workers.</li>
<li>Add communication OPs to enable the communication between nodes.</li>
</ol>
<p>We will need these OPs: <em>Send</em>, <em>Recv</em>, <em>Enqueue</em>, <em>Dequeue</em>.</p>
<p>Below is an example of converting the user defined graph to the
subgraphs for the trainer and the parameter server:</p>
<p><img src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/local-graph.png" width="300"/></p>
<p>After converting:</p>
<p><img src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/dist-graph.png" width="700"/></p>
<ol>
<li>The parameter variable W and its optimizer program are placed on the parameter server.</li>
<li>Operators are added to the program.</li>
<li><em>Send</em> sends data to the connected <em>Recv</em> operator.  The
     scheduler on the receive node will only schedule <em>Recv</em> operator
     to run when the <em>Send</em> operator has ran (the <em>Send</em> OP will mark
     the <em>Recv</em> OP runnable automatically).</li>
<li><em>Enqueue</em> enqueues the input variable, it can block until space
     become available in the queue.</li>
<li><em>Dequeue</em> outputs configurable numbers of tensors from the
     queue. It will block until the queue has the required number of
     tensors.</li>
</ol>
<h3>Sparse Update</h3>
<p>For embedding layers, the gradient may have many rows containing only 0 when training,
if the gradient uses a dense tensor to do parameter optimization,
it could spend unnecessary memory, slow down the calculations and waste
the bandwidth while doing distributed training.
In Fluid, we introduce <a href="../modules/selected_rows.md">SelectedRows</a> to represent a list of rows containing
non-zero gradient data. So when we do parameter optimization both locally and remotely,
we only need to send those non-zero rows to the optimizer operators:</p>
<p><img src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/sparse_update.png" width="700" /></p>
<h3>Benefits</h3>
<ul>
<li>Model parallelism becomes easier to implement: it is an extension to
  the trainer - parameter server approach. We can have several "Transpilers"
  to achieve different goals.</li>
<li>User-defined optimizer is easier to add - user can now express it as
  a sub-program.</li>
<li>No more duplication logic inside the trainer and the parameter
  server mentioned in the background section.</li>
</ul>
<h3>Challenges</h3>
<ul>
<li>It is important to balance the parameter shards on multiple
  parameter servers. If a single parameter is very big (for example: some
  word-embedding, fully connected, softmax layer), we need to
  automatically partition the single parameter onto different
  parameter servers when possible (only element-wise optimizer depends
  on the parameter variable).</li>
<li>In the "Async SGD" figure, the "W" variable on the parameter server
  could be read and written concurrently. See
  <a href="https://github.com/PaddlePaddle/Paddle/pull/6394">here</a> for more
  details about concurrent program in Fluid.</li>
</ul>
<h3>Discussion</h3>
<ul>
<li>Can the Enqueue OP be implemented under our current tensor design
  (put the input tensor into the queue tensor)?</li>
<li><em>Dequeue</em> OP will have variable numbers of output (depending on the
  <code>min_count</code> attribute), does our current design support it? (similar
  question for the <em>Add</em> OP)</li>
</ul>
<h3>References</h3>
<p>[1] <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45166.pdf">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</a></p>
{% endverbatim %}