{% verbatim %}
<h1>How to write a new operator</h1>
<ul>
<li><a href="#background">Background</a></li>
<li><a href="#implementing-c-types">Implementing C++ Types</a></li>
<li><a href="#defining-protomaker">Defining ProtoMaker</a></li>
<li><a href="#defining-operator">Defining Operator</a></li>
<li><a href="#defining-opkernel">Defining OpKernel</a></li>
<li><a href="#registering-operator-and-opkernel">Registering Operator and OpKernel</a></li>
<li><a href="#compilation">Compilation</a></li>
<li><a href="#python-binding">Python Binding</a></li>
<li><a href="#unit-tests">Unit Tests</a></li>
<li><a href="#testing-forward-operators">Testing Forward Operators</a></li>
<li><a href="#testing-backward-operators">Testing Backward Operators</a></li>
<li><a href="#compiling-and-running">Compiling and Running</a></li>
<li><a href="#remarks">Remarks</a></li>
</ul>
<h2>Background</h2>
<p>Here are the base types needed. For details, please refer to the design docs.</p>
<ul>
<li><code>class OpProtoAndCheckerMaker</code>: Describes an Operator's input, output, attributes and description, mainly used to interface with Python API.</li>
<li><code>framework::OperatorBase</code>: Operator (Op)base class.</li>
<li><code>framework::OpKernel</code>: Base class for Op computation kernel.</li>
<li><code>framework::OperatorWithKernel</code>: Inherited from OperatorBase, describing an operator with computation kernels.</li>
</ul>
<p>Operators can be categorized into two groups: operator with kernel(s) and operator without kernel(s). An operator with kernel(s) inherits from <code>OperatorWithKernel</code> while the one without kernel(s) inherits from <code>OperatorBase</code>. This tutorial focuses on implementing operators with kernels. In short, an operator includes the following information:</p>
<table>
<thead>
<tr>
<th>Information</th>
<th> Where is it defined</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpProtoMake definition </td>
<td> `.cc`files, Backward Op does not need an OpProtoMake interface. </td>
</tr>
<tr>
<td>Op definition  </td>
<td> `.cc` files</td>
</tr>
<tr>
<td>Kernel implementation  </td>
<td> The kernel methods shared between CPU and CUDA are defined in `.h` files. CPU-specific kernels live in `.cc` files, while CUDA-specific kernels are implemented in `.cu`files.</td>
</tr>
<tr>
<td>Registering the Op  </td>
<td> Ops are registered in `.cc` files; For Kernel registration, `.cc` files contain the CPU implementation, while `.cu` files contain the CUDA implementation.</td>
</tr>
</tbody>
</table>

<p>New Operator implementations are added to the list <a href="https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/operators">paddle/operators</a>, with file names in the format <code>*_op.h</code> (if applicable), <code>*_op.cc</code>, <code>*_op.cu</code> (if applicable).<strong> The system will use the naming scheme to automatically build operators and their corresponding Python extensions.</strong></p>
<p>Let's take matrix multiplication operator, <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/operators/mul_op.cc">MulOp</a>, as an example to introduce the writing of an Operator with Kernel.</p>
<h2>Implementing C++ Types</h2>
<h3>Defining ProtoMaker</h3>
<p>Matrix Multiplication can be written as $Out = X * Y$, meaning that the operation consists of two inputs and pne output.</p>
<p>First, define <code>ProtoMaker</code> to describe the Operator's input, output, and additional comments:</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MulOpMaker</span> <span class="o">:</span> <span class="k">public</span> <span class="n">framework</span><span class="o">::</span><span class="n">OpProtoAndCheckerMaker</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
  <span class="n">MulOpMaker</span><span class="p">(</span><span class="n">OpProto</span> <span class="o">*</span><span class="n">proto</span><span class="p">,</span> <span class="n">OpAttrChecker</span> <span class="o">*</span><span class="n">op_checker</span><span class="p">)</span>
      <span class="o">:</span> <span class="n">OpProtoAndCheckerMaker</span><span class="p">(</span><span class="n">proto</span><span class="p">,</span> <span class="n">op_checker</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">AddInput</span><span class="p">(</span><span class="s">&quot;X&quot;</span><span class="p">,</span> <span class="s">&quot;(Tensor), 2D tensor of size (M x K)&quot;</span><span class="p">);</span>
    <span class="n">AddInput</span><span class="p">(</span><span class="s">&quot;Y&quot;</span><span class="p">,</span> <span class="s">&quot;(Tensor), 2D tensor of size (K x N)&quot;</span><span class="p">);</span>
    <span class="n">AddOutput</span><span class="p">(</span><span class="s">&quot;Out&quot;</span><span class="p">,</span> <span class="s">&quot;(Tensor), 2D tensor of size (M x N)&quot;</span><span class="p">);</span>
    <span class="n">AddComment</span><span class="p">(</span><span class="sa">R</span><span class="s">&quot;</span><span class="dl">DOC(</span><span class="s"></span>
<span class="s">Two Element Mul Operator.</span>
<span class="s">The equation is: Out = X * Y</span>
<span class="dl">)DOC</span><span class="s">&quot;</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">};</span>
</pre></div>

<p><a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/operators/mul_op.cc#L76-L127"><code>MulOpMaker</code></a>is inherited from<code>framework::OpProtoAndCheckerMaker</code>, consisting of 2 variables in the constructorï¼š</p>
<ul>
<li><code>framework::OpProto</code> stores Operator input and variable attribute, used for generating Python API interfaces.</li>
<li><code>framework::OpAttrChecker</code> is used to validate variable attributes.</li>
</ul>
<p>The constructor utilizes <code>AddInput</code>, <code>AddOutput</code>, and <code>AddComment</code>, so that the corresponding information will be added to <code>OpProto</code>.</p>
<p>The code above adds two inputs <code>X</code> and <code>Y</code> to <code>MulOp</code>, an output <code>Out</code>, and their corresponding descriptions, in accordance to Paddle's <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/dev/name_convention.md">naming convention</a>.</p>
<p>An additional example <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/operators/scale_op.cc#L38-L55"><code>ScaleOp</code></a> is implemented as follows:</p>
<div class="highlight"><pre><span></span><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">AttrType</span><span class="o">&gt;</span>
<span class="k">class</span> <span class="nc">ScaleOpMaker</span> <span class="o">:</span> <span class="k">public</span> <span class="n">framework</span><span class="o">::</span><span class="n">OpProtoAndCheckerMaker</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
  <span class="n">ScaleOpMaker</span><span class="p">(</span><span class="n">OpProto</span> <span class="o">*</span><span class="n">proto</span><span class="p">,</span> <span class="n">OpAttrChecker</span> <span class="o">*</span><span class="n">op_checker</span><span class="p">)</span>
      <span class="o">:</span> <span class="n">OpProtoAndCheckerMaker</span><span class="p">(</span><span class="n">proto</span><span class="p">,</span> <span class="n">op_checker</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">AddInput</span><span class="p">(</span><span class="s">&quot;X&quot;</span><span class="p">,</span> <span class="s">&quot;The input tensor of scale operator.&quot;</span><span class="p">).</span><span class="n">NotInGradient</span><span class="p">();</span>
    <span class="n">AddOutput</span><span class="p">(</span><span class="s">&quot;Out&quot;</span><span class="p">,</span> <span class="s">&quot;The output tensor of scale operator.&quot;</span><span class="p">).</span><span class="n">NotInGradient</span><span class="p">();</span>
    <span class="n">AddComment</span><span class="p">(</span><span class="sa">R</span><span class="s">&quot;</span><span class="dl">DOC(</span><span class="s">Scale operator</span>
<span class="s">The equation is: Out = scale*X</span>
<span class="dl">)DOC</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="n">AddAttr</span><span class="o">&lt;</span><span class="n">AttrType</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;scale&quot;</span><span class="p">,</span> <span class="s">&quot;scale of scale operator.&quot;</span><span class="p">).</span><span class="n">SetDefault</span><span class="p">(</span><span class="mf">1.0</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">};</span>
</pre></div>

<p>Note <code>AddAttr&lt;AttrType&gt;("scale", "...").SetDefault(1.0);</code> adds <code>scale</code>constant as an attribute, and sets the default value to 1.0.</p>
<h3>Defining Operator</h3>
<p>The following code defines the interface for MulOp:</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MulOp</span> <span class="o">:</span> <span class="k">public</span> <span class="n">framework</span><span class="o">::</span><span class="n">OperatorWithKernel</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
  <span class="k">using</span> <span class="n">framework</span><span class="o">::</span><span class="n">OperatorWithKernel</span><span class="o">::</span><span class="n">OperatorWithKernel</span><span class="p">;</span>

 <span class="k">protected</span><span class="o">:</span>
  <span class="kt">void</span> <span class="n">InferShape</span><span class="p">(</span><span class="k">const</span> <span class="n">framework</span><span class="o">::</span><span class="n">InferShapeContext</span> <span class="o">&amp;</span><span class="n">ctx</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">dim0</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">Input</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;X&quot;</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">dims</span><span class="p">();</span>
    <span class="k">auto</span> <span class="n">dim1</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">Input</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;Y&quot;</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">dims</span><span class="p">();</span>
    <span class="n">PADDLE_ENFORCE_EQ</span><span class="p">(</span><span class="n">dim0</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="mi">2</span><span class="p">,</span>
                      <span class="s">&quot;input X(%s) should be a tensor with 2 dims, a matrix&quot;</span><span class="p">,</span>
                      <span class="n">ctx</span><span class="p">.</span><span class="n">op_</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="s">&quot;X&quot;</span><span class="p">));</span>
    <span class="n">PADDLE_ENFORCE_EQ</span><span class="p">(</span><span class="n">dim1</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="mi">2</span><span class="p">,</span>
                      <span class="s">&quot;input Y(%s) should be a tensor with 2 dims, a matrix&quot;</span><span class="p">,</span>
                      <span class="n">ctx</span><span class="p">.</span><span class="n">op_</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="s">&quot;Y&quot;</span><span class="p">));</span>
    <span class="n">PADDLE_ENFORCE_EQ</span><span class="p">(</span>
        <span class="n">dim0</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="s">&quot;First matrix&#39;s width must be equal with second matrix&#39;s height.&quot;</span><span class="p">);</span>
    <span class="n">ctx</span><span class="p">.</span><span class="n">Output</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;Out&quot;</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">Resize</span><span class="p">({</span><span class="n">dim0</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dim1</span><span class="p">[</span><span class="mi">1</span><span class="p">]});</span>
  <span class="p">}</span>
<span class="p">};</span>
</pre></div>

<p><a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/operators/mul_op.cc#L24"><code>MulOp</code></a> is inherited from <code>OperatorWithKernel</code>. Its <code>public</code> member</p>
<div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">framework</span><span class="o">::</span><span class="n">OperatorWithKernel</span><span class="o">::</span><span class="n">OperatorWithKernel</span><span class="p">;</span>
</pre></div>

<p>expresses an operator constructor using base class <code>OperatorWithKernel</code>, alternatively written as</p>
<div class="highlight"><pre><span></span><span class="n">MulOp</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="o">&amp;</span><span class="n">type</span><span class="p">,</span> <span class="k">const</span> <span class="n">framework</span><span class="o">::</span><span class="n">VariableNameMap</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">framework</span><span class="o">::</span><span class="n">VariableNameMap</span> <span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">framework</span><span class="o">::</span><span class="n">AttributeMap</span> <span class="o">&amp;</span><span class="n">attrs</span><span class="p">)</span>
  <span class="o">:</span> <span class="n">OperatorWithKernel</span><span class="p">(</span><span class="n">type</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">attrs</span><span class="p">)</span> <span class="p">{}</span>
</pre></div>

<p><code>InferShape</code> interface needs to be re-written.<code>InferShape</code> is a constant method and cannot modify Op's member variables, its constant member <code>const framework::InferShapeContext &amp;ctx</code> can be used to extract input, output, and attributes. It functions to</p>
<ul>
<li>1). validate and error out early: it checks input data dimensions and types.</li>
<li>2). configures the tensor shape in the output.</li>
</ul>
<p>Usually <code>OpProtoMaker</code> and <code>Op</code>'s type definitions are written in <code>.cc</code> files, which also include the registration methods introduced later.</p>
<h3>Defining OpKernel</h3>
<p><code>MulKernel</code> inherits <code>framework::OpKernel</code>, which includes the following templates:</p>
<ul>
<li>
<p><code>typename  DeviceContext</code> denotes device context type. When different devices, namely the CPUDeviceContext and the CUDADeviceContext, share the same kernel, this template needs to be added. If they don't share kernels, this must not be added. An example of a non-sharing kernel is <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/operators/cross_entropy_op.h#L43"><code>OnehotCrossEntropyOpKernel</code></a>.</p>
</li>
<li>
<p><code>typename T</code> denotes data type, such as <code>float</code> or <code>double</code>.</p>
</li>
</ul>
<p><code>MulKernel</code> types need to rewrite the interface for <code>Compute</code>.</p>
<ul>
<li><code>Compute</code> takes one input parameter: <code>const framework::ExecutionContext&amp; context</code>.</li>
<li>Compared with <code>InferShapeContext</code>, <code>ExecutionContext</code> includes device types, and can similarly extract input, output, and attribute variables.</li>
<li><code>Compute</code> implements the computation logics of an <code>OpKernel</code>.</li>
</ul>
<p><code>MulKernel</code>'s implementation of <code>Compute</code> is as follows:</p>
<div class="highlight"><pre><span></span><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">DeviceContext</span><span class="p">,</span> <span class="k">typename</span> <span class="n">T</span><span class="o">&gt;</span>
<span class="k">class</span> <span class="nc">MulKernel</span> <span class="o">:</span> <span class="k">public</span> <span class="n">framework</span><span class="o">::</span><span class="n">OpKernel</span> <span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
<span class="kt">void</span> <span class="n">Compute</span><span class="p">(</span><span class="k">const</span> <span class="n">framework</span><span class="o">::</span><span class="n">ExecutionContext</span><span class="o">&amp;</span> <span class="n">context</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
  <span class="k">auto</span><span class="o">*</span> <span class="n">X</span> <span class="o">=</span> <span class="n">context</span><span class="p">.</span><span class="n">Input</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;X&quot;</span><span class="p">);</span>
  <span class="k">auto</span><span class="o">*</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">context</span><span class="p">.</span><span class="n">Input</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;Y&quot;</span><span class="p">);</span>
  <span class="k">auto</span><span class="o">*</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">context</span><span class="p">.</span><span class="n">Output</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;Out&quot;</span><span class="p">);</span>
  <span class="n">Z</span><span class="o">-&gt;</span><span class="n">mutable_data</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">.</span><span class="n">GetPlace</span><span class="p">());</span>
  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">device_context</span> <span class="o">=</span> <span class="n">context</span><span class="p">.</span><span class="k">template</span> <span class="n">device_context</span><span class="o">&lt;</span><span class="n">DeviceContext</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="n">math</span><span class="o">::</span><span class="n">matmul</span><span class="o">&lt;</span><span class="n">DeviceContext</span><span class="p">,</span> <span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="o">*</span><span class="n">Y</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">device_context</span><span class="p">);</span>
<span class="p">}</span>
<span class="p">};</span>
</pre></div>

<p>Note that <strong>different devices (CPU, CUDA)share one Op definition; whether or not they share the same <code>OpKernel</code> depends on whether <code>Compute</code> calls functions can support both devices.</strong></p>
<p><code>MulOp</code>'s CPU and CUDA share the same <code>Kernel</code>. A non-sharing  <code>OpKernel</code> example can be seen in <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/operators/cross_entropy_op.cc"><code>OnehotCrossEntropyOpKernel</code></a>.</p>
<p>To ease the writing of <code>OpKernel</code> compute, and for reusing code cross-device, <a href="https://bitbucket.org/eigen/eigen/src/default/unsupported/Eigen/CXX11/src/Tensor/README.md?fileviewer=file-view-default"><code>Eigen-unsupported Tensor</code></a> module is used to implement <code>Compute</code> interface. To learn about how the Eigen library is used in PaddlePaddle, please see <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/doc/fluid/dev/use_eigen_en.md">usage document</a>.</p>
<p>This concludes the forward implementation of an operator. Next its operation and kernel need to be registered in a <code>.cc</code> file.</p>
<p>The definition of its corresponding backward operator, if applicable, is similar to that of an forward operator. <strong>Note that a backward operator does not include a <code>ProtoMaker</code></strong>.</p>
<h3>Registering Operator and OpKernel</h3>
<ul>
<li>
<p>In <code>.cc</code> files, register forward and backward operator classes and the CPU kernel.</p>
<div class="highlight"><pre><span></span><span class="k">namespace</span> <span class="n">ops</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">::</span><span class="n">operators</span><span class="p">;</span>
<span class="n">REGISTER_OPERATOR</span><span class="p">(</span><span class="n">mul</span><span class="p">,</span> <span class="n">ops</span><span class="o">::</span><span class="n">MulOp</span><span class="p">,</span> <span class="n">ops</span><span class="o">::</span><span class="n">MulOpMaker</span><span class="p">,</span>
              <span class="n">paddle</span><span class="o">::</span><span class="n">framework</span><span class="o">::</span><span class="n">DefaultGradOpDescMaker</span><span class="o">&lt;</span><span class="nb">true</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">REGISTER_OPERATOR</span><span class="p">(</span><span class="n">mul_grad</span><span class="p">,</span> <span class="n">ops</span><span class="o">::</span><span class="n">MulGradOp</span><span class="p">)</span>

<span class="n">REGISTER_OP_CPU_KERNEL</span><span class="p">(</span><span class="n">mul</span><span class="p">,</span> <span class="n">ops</span><span class="o">::</span><span class="n">MulKernel</span><span class="o">&lt;</span><span class="n">paddle</span><span class="o">::</span><span class="n">platform</span><span class="o">::</span><span class="n">CPUDeviceContext</span><span class="p">,</span> <span class="kt">float</span><span class="o">&gt;</span><span class="p">);</span>
<span class="n">REGISTER_OP_CPU_KERNEL</span><span class="p">(</span><span class="n">mul_grad</span><span class="p">,</span>
              <span class="n">ops</span><span class="o">::</span><span class="n">MulGradKernel</span><span class="o">&lt;</span><span class="n">paddle</span><span class="o">::</span><span class="n">platform</span><span class="o">::</span><span class="n">CPUDeviceContext</span><span class="p">,</span> <span class="kt">float</span><span class="o">&gt;</span><span class="p">);</span>
</pre></div>

</li>
</ul>
<p>In that code block,</p>
<pre><code>- `REGISTER_OPERATOR` registers the `ops::MulOp` class, type named `mul`, its type `ProtoMaker` is `ops::MulOpMaker`, registering `ops::MulOpGrad` as `mul_grad`.
- `REGISTER_OP_WITHOUT_GRADIENT` registers an operator without gradient.
- `REGISTER_OP_CPU_KERNEL` registers `ops::MulKernel` class and specialized template types `paddle::platform::CPUPlace` and `float`, which also registers `ops::MulGradKernel`.
</code></pre>
<ul>
<li>
<p>Registering CUDA Kernel in <code>.cu</code> files</p>
<ul>
<li>Note that if CUDA Kernel is implemented using the <code>Eigen unsupported</code> module, then on top of <code>.cu</code>, a macro definition <code>#define EIGEN_USE_GPU</code> is needed, such as</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1">// if use Eigen unsupported module before include head files</span>
<span class="cp">#define EIGEN_USE_GPU</span>

<span class="k">namespace</span> <span class="n">ops</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">::</span><span class="n">operators</span><span class="p">;</span>
<span class="n">REGISTER_OP_CUDA_KERNEL</span><span class="p">(</span><span class="n">mul</span><span class="p">,</span> <span class="n">ops</span><span class="o">::</span><span class="n">MulKernel</span><span class="o">&lt;</span><span class="n">paddle</span><span class="o">::</span><span class="n">platform</span><span class="o">::</span><span class="n">CUDADeviceContext</span><span class="p">,</span> <span class="kt">float</span><span class="o">&gt;</span><span class="p">);</span>
<span class="n">REGISTER_OP_CUDA_KERNEL</span><span class="p">(</span><span class="n">mul_grad</span><span class="p">,</span>
                       <span class="n">ops</span><span class="o">::</span><span class="n">MulGradKernel</span><span class="o">&lt;</span><span class="n">paddle</span><span class="o">::</span><span class="n">platform</span><span class="o">::</span><span class="n">CUDADeviceContext</span><span class="p">,</span> <span class="kt">float</span><span class="o">&gt;</span><span class="p">);</span>
</pre></div>

</li>
</ul>
<h3>Compilation</h3>
<p>Run the following commands to compile.</p>
<div class="highlight"><pre><span></span># maybe you need to rerun cmake
make mul_op
</pre></div>

<h2>Python Binding</h2>
<p>The system will automatically bind to Python and link it to a generated library.</p>
<h2>Unit Tests</h2>
<p>Unit tests for an operator include</p>
<ol>
<li>
<p>comparing a forward operator's implementations on different devices,</p>
</li>
<li>
<p>comparing a backward operator's implementation on different devices, and</p>
</li>
<li>
<p>a scaling test for the backward operator.</p>
</li>
</ol>
<p>Here, we introduce the <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/fluid/tests/unittests/test_mul_op.py">unit tests for <code>MulOp</code></a>.</p>
<h3>Testing Forward Operators</h3>
<p>A forward operator unit test inherits <code>unittest.TestCase</code> and defines metaclass <code>__metaclass__ = OpTestMeta</code>. More concrete tests are performed in <code>OpTestMeta</code>. Testing a forward operator requires the following:</p>
<ol>
<li>
<p>Defining input, output and relevant attributes in <code>setUp</code> method.</p>
</li>
<li>
<p>Generating random input data.</p>
</li>
<li>
<p>Implementing the same computation logic in a Python script.</p>
</li>
<li>
<p>Call check gradient function to check the backward operator.</p>
</li>
</ol>
<p><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">unittest</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">op_test</span> <span class="kn">import</span> <span class="n">OpTest</span>


<span class="k">class</span> <span class="nc">TestMulOp</span><span class="p">(</span><span class="n">OpTest</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">setUp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op_type</span> <span class="o">=</span> <span class="s2">&quot;mul&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;X&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">84</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">),</span>
            <span class="s1">&#39;Y&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">84</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Out&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">])}</span>

    <span class="k">def</span> <span class="nf">test_check_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_output</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">test_check_grad_normal</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_grad</span><span class="p">([</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="s1">&#39;Y&#39;</span><span class="p">],</span> <span class="s1">&#39;Out&#39;</span><span class="p">,</span> <span class="n">max_relative_error</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_check_grad_ingore_x</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_grad</span><span class="p">(</span>
            <span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">],</span> <span class="s1">&#39;Out&#39;</span><span class="p">,</span> <span class="n">max_relative_error</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">no_grad_set</span><span class="o">=</span><span class="nb">set</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">test_check_grad_ingore_y</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_grad</span><span class="p">(</span>
            <span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">],</span> <span class="s1">&#39;Out&#39;</span><span class="p">,</span> <span class="n">max_relative_error</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">no_grad_set</span><span class="o">=</span><span class="nb">set</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">))</span>
</pre></div>
Get its output, and compare it with the forward operator's own output.</p>
<p>The code above first loads required packages. In addition, we have</p>
<ul>
<li><code>self.op_type = "mul"</code> defines the type that is identical to what the operator's registered type.</li>
<li><code>self.inputs</code> defines input, with type <code>numpy.array</code> and initializes it.</li>
<li><code>self.outputs</code> defines output and completes the same operator computation in the Python script, and returns its result from the Python script.</li>
</ul>
<h3>Testing Backward Operators</h3>
<p>Some key points in checking gradient above include:</p>
<ul>
<li><code>test_normal</code> calls <code>check_grad</code> to validate scaling tests' correctness and stability through numeric methods.</li>
<li>The first variable <code>["X", "Y"]</code> appoints <code>X</code> and <code>Y</code> to be scale tested.</li>
<li>The second variable <code>"Out"</code> points to the network's final output target <code>Out</code>.</li>
<li>The third variable <code>max_relative_error</code> points to the maximum relative tolerance error during scaling tests.</li>
<li><code>test_check_grad_ingore_x</code> and <code>test_check_grad_ingore_y</code>branches test the cases where there is only one scaling input.</li>
</ul>
<h3>Compiling and Running</h3>
<p>Any new unit testing file of the format <code>test_*.py</code>  added to the director <code>python/paddle/fluid/tests/unittests/</code> is automatically added to the project to compile.</p>
<p>Note that <strong>unlike the compile test for Ops, running unit tests requires compiling the entire project</strong> and requires compiling with flag <code>WITH_TESTING</code> on i.e. <code>cmake paddle_dir -DWITH_TESTING=ON</code>.</p>
<p>After successfully compiling the project, run the following command to run unit tests:</p>
<div class="highlight"><pre><span></span>make <span class="nb">test</span> <span class="nv">ARGS</span><span class="o">=</span><span class="s2">&quot;-R test_mul_op -V&quot;</span>
</pre></div>

<p>Or,</p>
<div class="highlight"><pre><span></span>ctest -R test_mul_op
</pre></div>

<h2>Remarks</h2>
<ul>
<li>The type with which an operator is registered needs to be identical to the Op's name. Registering <code>REGISTER_OPERATOR(B, ...)</code> in <code>A_op.cc</code> will cause unit testing failures.</li>
<li>If the operator does not implement a CUDA kernel, please refrain from creating an empty <code>*_op.cu</code> file, or else unit tests will fail.</li>
<li>If multiple operators rely on some shared methods, a file NOT named <code>*_op.*</code> can be created to store them, such as <code>gather.h</code>.</li>
</ul>
{% endverbatim %}