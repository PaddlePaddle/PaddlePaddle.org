{% verbatim %}
<h1>Design Doc: Concurrent Programming with Fluid</h1>
<p>With PaddlePaddle Fluid, users describe a program other than a model.  The program is a <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/framework.proto"><code>ProgramDesc</code></a> protobuf message. TensorFlow/MxNet/Caffe2 applications generate protobuf messages too, but their protobuf messages represent the model, a graph of operators, but not the program that trains/uses the model.   </p>
<p>Many know that when we program TensorFlow, we can specify the device on which each operator runs.  This allows us to create a concurrent/parallel AI application.   An interesting questions is <strong>how does a <code>ProgramDesc</code> represents a concurrent program?</strong>  </p>
<p>The answer relies on the fact that a <code>ProgramDesc</code> is similar to an abstract syntax tree (AST) that describes a program.  So users just program a concurrent program that they do with any concurrent programming language, e.g., <a href="https://golang.org">Go</a>.</p>
<h2>An Analogy</h2>
<p>The following table compares concepts in Fluid and Go</p>
<table>
<thead>
<tr>
<th></th>
<th>Go</th>
<th>Fluid</th>
</tr>
</thead>
<tbody>
<tr>
<td>user-defined functions </td>
<td>
<a href="https://github.com/PaddlePaddle/Paddle/tree/develop/python/paddle/fluid">layers</a></td>
<td></td>
</tr>
<tr>
<td>control-flow and built-in functions </td>
<td>
<a href="https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/operators">intrinsics/operators</a></td>
<td></td>
</tr>
<tr>
<td>goroutines, channels </td>
<td>
<a href="https://github.com/PaddlePaddle/Paddle/tree/develop/paddle/fluid/framework/thread_pool.h">class ThreadPool</a></td>
<td></td>
</tr>
<tr>
<td>runtime </td>
<td>
<a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/executor.h">class Executor</a></td>
<td></td>
</tr>
</tbody>
</table>

<h2>An Example Concurrent Program</h2>
<p>To review all above concepts in an example, let us take a simple program and writes its distributed version.</p>
<p>Suppose that we want to parallelize a naive Fluid program (written in Go and calling Fluid's Go binding) that multiplies two tensors.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="s">&quot;fluid&quot;</span>

<span class="kd">func</span> <span class="nx">paddlepaddle</span><span class="p">()</span> <span class="p">{</span>
  <span class="nx">X</span> <span class="p">=</span> <span class="nx">fluid</span><span class="p">.</span><span class="nx">read</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="nx">W</span> <span class="p">=</span> <span class="nx">fluid</span><span class="p">.</span><span class="nx">Tensor</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="nx">Y</span> <span class="p">=</span> <span class="nx">fluid</span><span class="p">.</span><span class="nx">mult</span><span class="p">(</span><span class="nx">X</span><span class="p">,</span> <span class="nx">W</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>

<p>Please be aware that the Fluid's Go binding provides the default <code>main</code> function, which calls the <code>paddlepaddle</code> function, which, in this case, is defined in above program and creates the following <code>ProgramDesc</code> message.</p>
<div class="highlight"><pre><span></span><span class="kd">message</span> <span class="nc">ProgramDesc</span> <span class="p">{</span>
  <span class="n">block</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Block</span> <span class="p">{</span>
    <span class="na">vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">Y</span><span class="p">],</span>
    <span class="na">ops</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">read</span><span class="p">(</span><span class="na">output</span> <span class="o">=</span> <span class="n">X</span><span class="p">)</span>
      <span class="n">assign</span><span class="p">(</span><span class="na">input</span> <span class="o">=</span> <span class="o">...</span><span class="p">,</span> <span class="na">output</span> <span class="o">=</span> <span class="n">W</span><span class="p">)</span>
      <span class="n">mult</span><span class="p">(</span><span class="na">input</span> <span class="o">=</span> <span class="p">{</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">},</span> <span class="na">output</span> <span class="o">=</span> <span class="n">Y</span><span class="p">)</span>
    <span class="p">],</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>

<p>Then, the default <code>main</code> function calls <code>fluid.run()</code>, which creates an instance of the <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/executor.h"><code>class Executor</code></a> and calls <code>Executor.Run(block[0])</code>, where <code>block[0]</code> is the first and only block defined in above <code>ProgramDesc</code> message.</p>
<p>The default <code>main</code> function is defined as follows:</p>
<div class="highlight"><pre><span></span><span class="kd">func</span> <span class="nx">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="nx">paddlepaddle</span><span class="p">()</span>
  <span class="nx">fluid</span><span class="p">.</span><span class="nx">run</span><span class="p">()</span>
<span class="p">}</span>
</pre></div>

<h2>The Concurrent Version</h2>
<p>By parallelizing the above program, we could support very big tensor X by splitting into small pieces {x_1, x_2, ...} and sent each piece to worker process/node for parallel multiplication.</p>
<p>In this case, we can write a transpiler that takes a <code>ProgramDesc</code> message that represents the above example program and outputs two <code>ProgramDesc</code> messages, one for running on the master process/node, and the other one for worker processes/nodes.</p>
<h3>The Master Program</h3>
<p>The master program could look like the following:</p>
<div class="highlight"><pre><span></span><span class="kd">message</span> <span class="nc">ProgramDesc</span> <span class="p">{</span>
  <span class="n">block</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Block</span> <span class="p">{</span>
    <span class="na">vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">Y</span><span class="p">],</span>
    <span class="na">ops</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">read</span><span class="p">(</span><span class="na">output</span> <span class="o">=</span> <span class="n">X</span><span class="p">)</span>
      <span class="n">kube_get_workers_addrs</span><span class="p">(</span><span class="na">output</span> <span class="o">=</span> <span class="n">L</span><span class="p">)</span>
      <span class="na">Y</span> <span class="o">=</span> <span class="n">tensor_array</span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">L</span><span class="p">))</span>
      <span class="n">parallel_for</span><span class="p">(</span><span class="na">input</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="na">output</span> <span class="o">=</span> <span class="n">Y</span><span class="p">,</span>
                   <span class="na">attrs</span> <span class="o">=</span> <span class="p">{</span><span class="n">L</span><span class="p">,</span> <span class="n">block_id</span><span class="p">(</span><span class="mi">1</span><span class="p">)})</span> <span class="err">#</span> <span class="n">referring</span> <span class="k">to</span> <span class="n">block</span> <span class="mi">1</span>
    <span class="p">]</span>
  <span class="p">}</span>

  <span class="n">block</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Block</span> <span class="p">{</span>
    <span class="na">parent</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="na">vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">index</span><span class="p">],</span>
    <span class="na">ops</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">slice</span><span class="p">(</span><span class="na">input</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">index</span><span class="p">],</span> <span class="na">output</span> <span class="o">=</span> <span class="n">x</span><span class="p">)</span> <span class="err">#</span> <span class="n">index</span> <span class="n">is</span> <span class="n">initialized</span> <span class="n">by</span> <span class="n">parallel_for</span>
      <span class="n">send</span><span class="p">(</span><span class="na">input</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="na">attrs</span> <span class="o">=</span> <span class="n">L</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
      <span class="n">recv</span><span class="p">(</span><span class="na">outputs</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="na">attrs</span> <span class="o">=</span> <span class="n">L</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
      <span class="n">assign</span><span class="p">(</span><span class="na">input</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="na">output</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
    <span class="p">]</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>

<p>The equivalent Fluid program (calling the Go binding) is:</p>
<div class="highlight"><pre><span></span><span class="kd">func</span> <span class="nx">main</span><span class="p">()</span> <span class="p">{</span>  <span class="c1">//// block 0</span>
  <span class="nx">X</span> <span class="p">=</span> <span class="nx">fluid</span><span class="p">.</span><span class="nx">read</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="nx">L</span> <span class="p">=</span> <span class="nx">fluid</span><span class="p">.</span><span class="nx">k8s</span><span class="p">.</span><span class="nx">get_worker_addrs</span><span class="p">()</span>
  <span class="nx">Y</span> <span class="p">=</span> <span class="nx">fluid</span><span class="p">.</span><span class="nx">tensor_array</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nx">L</span><span class="p">))</span>
  <span class="nx">fluid</span><span class="p">.</span><span class="nx">parallel_for</span><span class="p">(</span><span class="nx">X</span><span class="p">,</span> <span class="nx">L</span><span class="p">,</span>
                     <span class="kd">func</span><span class="p">(</span><span class="nx">index</span> <span class="kt">int</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">//// block 1</span>
                       <span class="nx">x</span> <span class="p">=</span> <span class="nx">X</span><span class="p">[</span><span class="nx">index</span><span class="p">]</span>
                       <span class="nx">fluid</span><span class="p">.</span><span class="nx">send</span><span class="p">(</span><span class="nx">L</span><span class="p">[</span><span class="nx">index</span><span class="p">],</span> <span class="nx">x</span><span class="p">)</span>
                       <span class="nx">y</span> <span class="p">=</span> <span class="nx">fluid</span><span class="p">.</span><span class="nx">recv</span><span class="p">(</span><span class="nx">L</span><span class="p">[</span><span class="nx">index</span><span class="p">])</span>
                       <span class="nx">Y</span><span class="p">[</span><span class="nx">index</span><span class="p">]</span> <span class="p">=</span> <span class="nx">y</span>
                     <span class="p">})</span>
<span class="p">}</span>
</pre></div>

<p>An explanation of the above program:</p>
<ul>
<li><code>fluid.k8s</code> is a package that provides access to Kubernetes API.  </li>
<li><code>fluid.k8s.get_worker_addrs</code> returns the list of IP and ports of all pods of the current job except for the current one (the master pod).  </li>
<li>
<p><code>fluid.tensor_array</code> creates a <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/lod_tensor_array.h">tensor array</a>.  <code>fluid.parallel_for</code> creates a <code>ParallelFor</code> intrinsic, which, when executed,</p>
</li>
<li>
<p>creates <code>len(L)</code> scopes, each for the concurrent running of the sub-block (block 1 in this case), and initializes a variable named "index" in the scope to an integer value in the range <code>[0, len(L)-1]</code>, and</p>
</li>
<li>creates <code>len(L)</code> threads by calling into the <code>ThreadPool</code> singleton, each thread  <ol>
<li>creates an Executor instance, and</li>
<li>calls <code>Executor.Run(block)</code>, where <code>block</code> is block 1 as explained above.</li>
</ol>
</li>
<li>Please be aware that block 1 is a sub-block of block 0, so ops in block 1 could refer to variables defined in block 0.</li>
</ul>
<h3>The Worker Program</h3>
<p>The worker program looks like</p>
<div class="highlight"><pre><span></span><span class="kd">func</span> <span class="nx">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="nx">W</span> <span class="p">=</span> <span class="nx">Tensor</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="nx">x</span> <span class="p">=</span> <span class="nx">fluid</span><span class="p">.</span><span class="nx">listen_and_do</span><span class="p">(</span>
        <span class="nx">fluid</span><span class="p">.</span><span class="nx">k8s</span><span class="p">.</span><span class="nx">self_addr</span><span class="p">(),</span>
        <span class="kd">func</span><span class="p">(</span><span class="nx">input</span> <span class="nx">Tensor</span><span class="p">)</span> <span class="p">{</span>
          <span class="nx">output</span> <span class="p">=</span> <span class="nx">fluid</span><span class="p">.</span><span class="nx">mult</span><span class="p">(</span><span class="nx">input</span><span class="p">,</span> <span class="nx">W</span><span class="p">)</span>
        <span class="p">})</span>
<span class="p">}</span>
</pre></div>

<p>where</p>
<ul>
<li><code>fluid.listen_and_do</code> creates a <code>ListenAndDo</code> intrinsic, which, when executed,</li>
<li>listens on the current pod's IP address, as returned by <code>fliud.k8s.self_addr()</code>,</li>
<li>once a connection is established,<ol>
<li>creates a scope of two parameters, "input" and "output",</li>
<li>reads a <a href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/framework/variable.h">Fluid variable</a> and saves it into "input",</li>
<li>creates an Executor instance and calls <code>Executor.Run(block)</code>, where the block is generated by running the lambda specified as the second parameter of <code>fluid.listen_and_do</code>.</li>
</ol>
</li>
</ul>
<h2>Summarization</h2>
<p>From the above example, we see that:</p>
<ol>
<li>Fluid enables the imperative programming paradigm by:</li>
<li>letting users describe a program, but not a model (a sequence of layers, or a graph of operators), and</li>
<li>call the <code>fluid.run</code> function that runs the program implicitly.</li>
<li>The program is described as a <code>ProgramDesc</code> protobuf message.</li>
<li>Function <code>Executor.Run</code> takes a block, instead of a <code>ProgramDesc</code>, as its parameter.</li>
<li><code>fluid.run</code> calls <code>Executor.Run</code> to run the first block in the <code>ProgramDesc</code> message.</li>
<li><code>Executor.Run</code>'s implementation is extremely simple -- it doesn't plan the execution nor create threads; instead, it runs on the current thread and execute intrinsics/operators' <code>Run</code> method sequentially as they appear in the <code>Block.ops</code> array.</li>
<li>Intrinsics/operators' <code>Run</code> method might create threads.  For example, the <code>ListenAndDo</code> operator creates a thread to handle each incoming request.</li>
<li>Threads are not necessarily OS thread; instead, they could be <a href="https://en.wikipedia.org/wiki/Green_threads">green threads</a> managed by ThreadPool.  Multiple green threads might run on the same OS thread.  An example green threads is Go's <a href="https://tour.golang.org/concurrency/1">goroutines</a>.</li>
</ol>
{% endverbatim %}