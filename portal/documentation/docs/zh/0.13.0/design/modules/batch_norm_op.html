{% verbatim %}
<h1>Batch Normalization</h1>
<h2>What is batch normalization</h2>
<p>Batch normalization is a frequently-used method in deep network training. It adjusts the mean and variance of a layer's output, and make the data distribution easier for next layer's training.</p>
<p>The principle of batch normalization can be summarized into a simple function:</p>
<div class="highlight"><pre><span></span>y = (x - E[x]) / STD[x]) * scale + bias
</pre></div>

<p><code>x</code> is a batch of output data of a certain layer. <code>E[x]</code> and <code>STD[x]</code> is the mean and standard deviation of <code>x</code>, respectivelyã€‚ <code>scale</code> and <code>bias</code> are two trainable parameters. The training of batch normalization layer equals to the learning of best values of <code>scale</code> and <code>bias</code>.</p>
<p>In our design, we use a single operator(<code>batch_norm_op</code>) to implement the whole batch normalization in C++, and wrap it as a layer in Python.</p>
<h2>Differences with normal operators</h2>
<p><code>batch_norm_op</code> is a single operator. However, there are a few differences between <code>BatchNormOp</code> and normal operators, which we shall take into consideration in our design.</p>
<ol>
<li>
<p><code>batch_norm_op</code> shall behave differently in training and inferencing. For example, during inferencing, there is no batch data and it's impossible to compute <code>E[x]</code> and <code>STD[x]</code>, so we have to use an <code>estimated_mean</code> and an <code>estimated_variance</code> instead of them. These require our framework to be able to inform operators current running type (training/inferencing), then operators can switch their behaviors.</p>
</li>
<li>
<p><code>batch_norm_op</code> shall have the ability to maintain <code>estimated_mean</code> and <code>estimated_variance</code> across mini-batch. In each mini-batch, <code>estimated_mean</code> is iterated by the following equations:</p>
</li>
</ol>
<div class="highlight"><pre><span></span>if batch_id == 0
  estimated_mean = E[x]
else
  estimated_mean = estimated_mean * momentum + (1.0 - momentum_) * E[x]
</pre></div>

<p>The iterating of <code>estimated_variance</code> is similar. <code>momentum</code> is an attribute, which controls estimated_mean updating speed.</p>
<h2>Implementation</h2>
<p>Batch normalization is designed as a single operator is C++, and then wrapped as a layer in Python.</p>
<h3>C++</h3>
<p>As most C++ operators do, <code>batch_norm_op</code> is defined by inputs, outputs, attributes and compute kernels.</p>
<h4>Inputs</h4>
<ul>
<li><code>x</code>: The inputs data, which is generated by the previous layer.</li>
<li><code>estimated_mean</code>: The estimated mean of all previous data batches. It is updated in each forward propagation and will be used in inferencing to take the role of <code>E[x]</code>.</li>
<li><code>estimated_var</code>: The estimated standard deviation of all previous data batches. It is updated in each forward propagation and will be used in inferencing to take the role of <code>STD[x]</code>.</li>
<li><code>scale</code>: trainable parameter 'scale'</li>
<li><code>bias</code>: trainable parameter 'bias'</li>
</ul>
<h4>Outputs</h4>
<ul>
<li><code>y</code>: The output data.</li>
<li><code>batch_mean</code>: The mean value of batch data.</li>
<li><code>batch_var</code>: The standard deviation value of batch data.</li>
<li><code>saved_mean</code>: Updated <code>estimated_mean</code> with current batch data. It's supposed to share the memory with input <code>estimated_mean</code>.</li>
<li><code>saved_var</code>: Updated <code>estimated_var</code> with current batch data. It's supposed to share the memory with input <code>estimated_var</code>.</li>
</ul>
<h4>Attributes</h4>
<ul>
<li><code>is_infer</code>: <em>bool</em>. If true, run <code>batch_norm_op</code> in inferencing mode.</li>
<li><code>use_global_est</code>: <em>bool</em>. If true, use <code>saved_mean</code> and <code>saved_var</code> instead of <code>E[x]</code> and <code>STD[x]</code> in trainning.</li>
<li><code>epsilon</code>: <em>float</em>. The epsilon value to avoid division by zero.</li>
<li><code>momentum</code>: <em>float</em>. Factor used in <code>estimated_mean</code> and <code>estimated_var</code> updating. The usage is shown above.</li>
</ul>
<h4>Kernels</h4>
<p>The following graph showes the training computational process of <code>batch_norm_op</code>:</p>
<p><img src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/batch_norm_op_kernel.png" width="800"/></p>
<p>cudnn provides APIs to finish the whole series of computation, we can use them in our GPU kernel.</p>
<h3>Python</h3>
<p><code>batch_norm_op</code> is warpped as a layer in Python:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">batch_norm_layer</span><span class="p">(</span><span class="n">net</span><span class="p">,</span>
                     <span class="nb">input</span><span class="p">,</span>
                     <span class="n">output</span><span class="p">,</span>
                     <span class="n">scale</span><span class="p">,</span>
                     <span class="n">bias</span><span class="p">,</span>
                     <span class="n">use_global_est</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
                     <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
                     <span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">):</span>
    <span class="n">mean_cache</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">new_var</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;estimated_mean&#39;</span><span class="p">,</span> <span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
    <span class="n">var_cache</span> <span class="o">=</span> <span class="n">scop</span><span class="o">.</span><span class="n">new_var</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;estimated_var&#39;</span><span class="p">,</span> <span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
    <span class="n">batch_mean</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">new_var</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;batch_mean&#39;</span><span class="p">)</span>
    <span class="n">batch_var</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">new_var</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;batch_var&#39;</span><span class="p">)</span>
    <span class="n">batch_norm_op</span> <span class="o">=</span> <span class="n">Operator</span><span class="p">(</span><span class="s1">&#39;batch_norm_op&#39;</span><span class="p">,</span>
                             <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="p">,</span>
                             <span class="n">estimated_mean</span> <span class="o">=</span> <span class="n">mean_cache</span><span class="p">,</span>
                             <span class="n">estimated_mean</span> <span class="o">=</span> <span class="n">var_cache</span><span class="p">,</span>
                             <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="p">,</span>
                             <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="p">,</span>
                             <span class="n">y</span> <span class="o">=</span> <span class="n">output</span><span class="p">,</span>
                             <span class="n">batch_mean</span> <span class="o">=</span> <span class="n">batch_mean</span><span class="p">,</span>
                             <span class="n">batch_var</span> <span class="o">=</span> <span class="n">batch_var</span><span class="p">,</span>
                             <span class="n">saved_mean</span> <span class="o">=</span> <span class="n">mean_cache</span><span class="p">,</span>
                             <span class="n">saved_var</span> <span class="o">=</span> <span class="n">var_cache</span><span class="p">,</span>
                             <span class="n">is_infer</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
                             <span class="n">use_global_est</span> <span class="o">=</span> <span class="n">use_global_est</span><span class="p">,</span>
                             <span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span><span class="p">,</span>
                             <span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">append_op</span><span class="p">(</span><span class="n">batch_norm_op</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>

<p>Because Python API has not been finally decided, the code above can be regarded as pseudo code. There are a few key points we shall note:</p>
<ol>
<li>
<p><code>estimated_mean</code> and <code>estimated_var</code> are assigned the same variables with <code>saved_mean</code> and <code>saved_var</code> respectively. So they share same the memories. The output mean and variance values(<code>saved_mean</code> and <code>saved_var</code>) of a certain batch will be the inputs(<code>estimated_mean</code> and <code>estimated_var</code>) of the next batch.</p>
</li>
<li>
<p><code>is_infer</code> decided whether <code>batch_norm_op</code> will run in training mode or inferencing mode. However, a network may contains both training and inferencing parts. And user may switch <code>batch_norm_op</code>'s running mode in Python <code>for</code> loop like this:</p>
</li>
</ol>
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">pass_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">PASS_NUM</span><span class="p">):</span>
    <span class="c1"># ...</span>
    <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># run training model</span>
    <span class="k">if</span> <span class="n">pass_id</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">net</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">test_image</span><span class="p">)</span>    <span class="c1"># run inferencing model</span>
    <span class="c1"># ...</span>
</pre></div>

<p><code>is_infer</code> is an attribute. Once an operator is created, its attributes can not be changed. It suggests us that we shall maintain two <code>batch_norm_op</code> in the model, one's <code>is_infer</code> is <code>True</code>(we call it <code>infer_batch_norm_op</code>) and the other one's is <code>False</code>(we call it <code>train_batch_norm_op</code>). They share all parameters and variables, but be placed in two different branches. That is to say, if a network contains a <code>batch_norm_op</code>, it will fork into two branches, one go through <code>train_batch_norm_op</code> and the other one go through <code>infer_batch_norm_op</code>:</p>
<div align=center>
<img src="https://raw.githubusercontent.com/PaddlePaddle/Paddle/develop/doc/fluid/images/batch_norm_fork.png" width="500"/>
</div>

<p>Just like what is shown in the above graph, the net forks before <code>batch_norm_op</code> and will never merge again. All the operators after <code>batch_norm_op</code> will duplicate.</p>
<p>When the net runs in training mode, the end of the left branch will be set as the running target, so the dependency tracking process will ignore right branch automatically. When the net runs in inferencing mode, the process is reversed.</p>
<p>How to set a target is related to Python API design, so I will leave it here waiting for more discussions.</p>
{% endverbatim %}